{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Lab 8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9SNdHSrdp9F7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NmtModel(object):\n",
        "  def __init__(self,source_dict,target_dict,use_attention):\n",
        "    self.num_layers = 2\n",
        "    self.hidden_size = 200\n",
        "    self.embedding_size = 100\n",
        "    self.hidden_dropout_rate=0.2\n",
        "    self.embedding_dropout_rate = 0.2\n",
        "    self.max_target_step = 30\n",
        "    self.vocab_target_size = len(target_dict.vocab)\n",
        "    self.vocab_source_size = len(source_dict.vocab)\n",
        "    self.target_dict = target_dict\n",
        "    self.source_dict = source_dict\n",
        "    self.SOS = target_dict.word2ids['<start>']\n",
        "    self.EOS = target_dict.word2ids['<end>']\n",
        "    self.use_attention = use_attention\n",
        "\n",
        "    print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    self.source_words = tf.placeholder(tf.int32,[None,None],\"source_words\")\n",
        "    self.target_words = tf.placeholder(tf.int32,[None,None],\"target_words\")\n",
        "    self.source_sent_lens = tf.placeholder(tf.int32,[None],\"source_sent_lens\")\n",
        "    self.target_sent_lens = tf.placeholder(tf.int32,[None],\"target_sent_lens\")\n",
        "    self.is_training = tf.placeholder(tf.bool,[],\"is_training\")\n",
        "\n",
        "    self.predictions,self.loss = self.get_predictions_and_loss(self.source_words,self.target_words,self.source_sent_lens,self.target_sent_lens,self.is_training)\n",
        "\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
        "    self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params))\n",
        "    self.sess = tf.Session()\n",
        "    self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "  def get_predictions_and_loss(self, source_words,target_words, source_sent_lens,target_sent_lens,is_training):\n",
        "    self.embeddings_target = tf.get_variable(\"embeddings_target\", [self.vocab_target_size, self.embedding_size], dtype=tf.float32)\n",
        "    self.embeddings_source = tf.get_variable(\"embeddings_source\", [self.vocab_source_size, self.embedding_size], dtype=tf.float32)\n",
        "\n",
        "    batch_size = shape(target_words, 0)\n",
        "    max_target_sent_len = shape(target_words, 1)\n",
        "\n",
        "    embedding_keep_prob = 1 - (tf.to_float(is_training) * self.embedding_dropout_rate)\n",
        "    hidden_keep_prob = 1 - (tf.to_float(is_training) * self.hidden_dropout_rate)\n",
        "\n",
        "    source_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_source,source_words),embedding_keep_prob)\n",
        "    target_embs = tf.nn.dropout(tf.nn.embedding_lookup(self.embeddings_target,target_words),embedding_keep_prob)\n",
        "\n",
        "\n",
        "    encoder_outputs, encode_final_states = self.encoder(source_embs,source_sent_lens,hidden_keep_prob)\n",
        "\n",
        "    time_major_target_embs = tf.transpose(target_embs,[1,0,2])\n",
        "\n",
        "\n",
        "    def _decoder_scan(pre,inputs):\n",
        "      pre_logits, pre_pred, pre_states = pre\n",
        "      step_embeddings = inputs\n",
        "\n",
        "      pred_embeddings = tf.nn.embedding_lookup(self.embeddings_target,pre_pred)\n",
        "\n",
        "      step_embeddings = tf.cond(is_training,lambda :step_embeddings,lambda :pred_embeddings)\n",
        "      curr_logits, curr_states = self.step_decoder(step_embeddings,encoder_outputs,pre_states,hidden_keep_prob)\n",
        "      curr_pred = tf.argmax(curr_logits,1,output_type=tf.int32)\n",
        "\n",
        "      return curr_logits, curr_pred, curr_states\n",
        "\n",
        "    init_logits = tf.zeros([batch_size,self.vocab_target_size])\n",
        "    init_pred = tf.ones([batch_size],tf.int32) * self.SOS\n",
        "\n",
        "    time_major_logits, time_major_preds, _ = tf.scan(_decoder_scan,time_major_target_embs,initializer=(init_logits, init_pred,encode_final_states))\n",
        "    time_major_logits, time_major_preds = tf.stack(time_major_logits),tf.stack(time_major_preds)\n",
        "\n",
        "    logits = tf.transpose(time_major_logits,[1,0,2])\n",
        "    predictions = tf.transpose(time_major_preds,[1,0])\n",
        "\n",
        "    logits_mask = tf.sequence_mask(target_sent_lens-1,max_target_sent_len)\n",
        "    flatten_logits_mask = tf.reshape(logits_mask,[batch_size*max_target_sent_len])\n",
        "    flatten_logits = tf.boolean_mask(tf.reshape(logits,[batch_size*max_target_sent_len,self.vocab_target_size]),flatten_logits_mask)\n",
        "\n",
        "    gold_labels_mask = tf.concat([tf.zeros([batch_size,1],dtype=tf.bool),tf.sequence_mask(target_sent_lens-1,max_target_sent_len-1)],1)\n",
        "    flatten_gold_labels_mask = tf.reshape(gold_labels_mask,[batch_size*max_target_sent_len])\n",
        "    flatten_gold_labels = tf.boolean_mask(tf.reshape(target_words,[batch_size*max_target_sent_len]),flatten_gold_labels_mask)\n",
        "\n",
        "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=flatten_gold_labels,logits=flatten_logits))\n",
        "\n",
        "    return predictions, loss\n",
        "\n",
        "\n",
        "\n",
        "  def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "      \"\"\"\n",
        "      Task 1 encoder\n",
        "      \n",
        "      Start\n",
        "      \"\"\"\n",
        "\n",
        "      # Create an LSTM as the basis of the first layer of the encoder\n",
        "      cell1 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      \n",
        "      # Add Dropout (implemented as a Wrapper around the LSTM Cell in Tensorflow)\n",
        "      # Dropout is basically a regularisation technique to prevent overfitting\n",
        "      drop = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = hidden_keep_prob)\n",
        "      \n",
        "      # Add the second layer, which has the same structure as the first layer above\n",
        "      cell2 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = hidden_keep_prob)\n",
        "      \n",
        "      # Use MultiRNNCell to stack the two cells above so that they behave as a \n",
        "      # single 2-layer cell called 'encoder_cell'\n",
        "      encoder_cell = tf.nn.rnn_cell.MultiRNNCell([drop, drop2])\n",
        "      \n",
        "      # Use dynamic_rnn to take the encoder_cell (above) and the embeddings as \n",
        "      # input and produce the tuple of the encoder output and final state\n",
        "      encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
        "      encoder_cell, embeddings,\n",
        "      dtype=tf.float32 \n",
        "       )\n",
        "      \n",
        "      # The encoder function returns the encoder output and final state\n",
        "      return encoder_outputs, encoder_final_state\n",
        "   \n",
        "      \"\"\"\n",
        "      End Task 1\n",
        "      \"\"\"\n",
        "\n",
        "    return encoder_outputs, encoder_final_states\n",
        "\n",
        "\n",
        "  def step_decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "      \"\"\"\n",
        "      Task 2 decoder without attention\n",
        "      \n",
        "      Start\n",
        "      \"\"\"\n",
        "      # Create an LSTM as the basis of the first layer of the decoder\n",
        "      cell1 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      \n",
        "      # Add Dropout (implemented as a Wrapper around the LSTM Cell in Tensorflow)\n",
        "      # Dropout is basically a regularisation technique to prevent overfitting\n",
        "      drop = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = hidden_keep_prob, dtype=tf.float32)\n",
        "      \n",
        "      # Add the second layer, which has the same structure as the first layer above\n",
        "      cell2 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = hidden_keep_prob, dtype=tf.float32)\n",
        "      \n",
        "      # Use MultiRNNCell to stack the two cells above so that they behave as a \n",
        "      # single 2-layer cell called 'decoder_cell'            \n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([drop, drop2])\n",
        "\n",
        "      # Directly run the decoder_cell using the step embeddings and pre_states, which\n",
        "      # are both passed in to the decoder function      \n",
        "      step_decoder_output, curr_states = decoder_cell(step_embeddings,pre_states)\n",
        "   \n",
        "      \n",
        "      # Add attention to improve results:\n",
        "      if not self.use_attention:\n",
        "          w = tf.get_variable(\"weights\", shape=[shape(step_decoder_output,1),self.vocab_target_size],\n",
        "                    initializer=tf.glorot_uniform_initializer())\n",
        "          b = tf.get_variable(\"bias\", shape= self.vocab_target_size,\n",
        "                    initializer=tf.constant_initializer(0.1))\n",
        "          logits = tf.nn.xw_plus_b(step_decoder_output,w,b)\n",
        "      else:\n",
        "          # The part of the code which will actually run:\n",
        "          \n",
        "          # Encoder_outputs is a tensor with shape [batch_size, max_step, emb].\n",
        "          # Step_decoder_output is a matrix with shape [batch_size, emb]\n",
        "          # Use tf.expand_dims on step_decoder to enable the use of tf.matmul\n",
        "          # for matrix multiplication\n",
        "          \n",
        "          reshape_step = tf.expand_dims(step_decoder_output,2)\n",
        "          \n",
        "          # Order important: encoder_outputs first to give raw score required\n",
        "          # shape of [batch_size, max_step, 1]\n",
        "          \n",
        "          raw_score = tf.matmul(encoder_outputs,reshape_step)\n",
        "          \n",
        "          #Add Softmax to give multiple probability scores\n",
        "          soft = tf.nn.softmax(raw_score, axis= 1)  \n",
        "          \n",
        "          # Weighted score for encoder_outputs: multiply by softmax scores\n",
        "          encoder_vector = tf.reduce_sum(soft * encoder_outputs,1)\n",
        "          \n",
        "          # As per lecture slides on attention: concatenate the step_encoder_output\n",
        "          # and the encoder_vector\n",
        "          concat = tf.concat([step_decoder_output,encoder_vector], axis=1)\n",
        "            \n",
        "          # Final Layer: \n",
        "          # Feed Forward Network to output the logits from the step decoder output\n",
        "          w = tf.get_variable(\"weights\", shape=[shape(concat,1),self.vocab_target_size],\n",
        "                        initializer=tf.glorot_uniform_initializer())\n",
        "          b = tf.get_variable(\"bias\", shape=self.vocab_target_size,\n",
        "                        initializer=tf.constant_initializer(0.1))\n",
        "          logits = tf.nn.xw_plus_b(concat,w,b)\n",
        "          \n",
        "      \n",
        "      \"\"\"\n",
        "      Ends Task 3\n",
        "      \"\"\"\n",
        "\n",
        "      return logits, curr_states\n",
        "\n",
        "\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time-start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)\n",
        "\n",
        "  def train(self,train_data,dev_data,test_data, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "      losses = []\n",
        "      source_train,target_train = train_data\n",
        "      for i, (source,target) in enumerate(zip(source_train,target_train)):\n",
        "        source_words,source_sent_lens = source\n",
        "        target_words,target_sent_lens = target\n",
        "        fd = {self.source_words:source_words,self.target_words:target_words,\n",
        "              self.source_sent_lens:source_sent_lens,self.target_sent_lens:target_sent_lens,\n",
        "              self.is_training:True}\n",
        "\n",
        "        _, loss= self.sess.run([self.train_op, self.loss], feed_dict=fd)\n",
        "\n",
        "        losses.append(loss)\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(\"[{}]: loss:{:.2f}\".format(i+1, sum(losses[i + 1 - 100:]) / 100.0))\n",
        "      print(\"Average epoch loss:{}\".format(sum(losses) / len(losses)))\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_data)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_data)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "\n",
        "\n",
        "  def get_target_sentences(self, sents,vocab,reference=False,isnumpy=False):\n",
        "    str_sents = []\n",
        "    for sent in sents:\n",
        "      str_sent = []\n",
        "      for t in sent:\n",
        "        if isnumpy:\n",
        "          t = t.item()\n",
        "        if t == self.SOS:\n",
        "          continue\n",
        "        if t == self.EOS:\n",
        "          break\n",
        "\n",
        "        str_sent.append(vocab[t])\n",
        "      if reference:\n",
        "        str_sents.append([str_sent])\n",
        "      else:\n",
        "        str_sents.append(str_sent)\n",
        "    return str_sents\n",
        "\n",
        "\n",
        "  def eval(self, dataset):\n",
        "    source_batches, target_batches = dataset\n",
        "    references = []\n",
        "    candidates = []\n",
        "    vocab = self.target_dict.vocab\n",
        "    PAD = self.target_dict.PAD\n",
        "\n",
        "    for i, (source, target) in enumerate(zip(source_batches, target_batches)):\n",
        "      source_words, source_sent_lens = source\n",
        "      target_words, target_sent_lens = target\n",
        "      infer_target_words = [[PAD for i in range(self.max_target_step)] for b in target_words]\n",
        "\n",
        "      fd = {self.source_words: source_words, self.target_words: infer_target_words,\n",
        "            self.source_sent_lens: source_sent_lens,\n",
        "            self.is_training: False}\n",
        "      predictions = self.sess.run(self.predictions,feed_dict=fd)\n",
        "\n",
        "      references.extend(self.get_target_sentences(target_words,vocab,reference=True))\n",
        "      candidates.extend(self.get_target_sentences(predictions,vocab,isnumpy=True))\n",
        "\n",
        "    score = corpus_bleu(references,candidates)\n",
        "    print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "    # Remove to reduce file size\n",
        "    #print(references)\n",
        "    #print(candidates)\n",
        "    #print(predictions)\n",
        "\n",
        "\n",
        "def shape(x, n):\n",
        "  return x.get_shape()[n].value or tf.shape(x)[n]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWCuSGE-wgH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "toL7_FTEp0hh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import collections\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "tf.reset_default_graph()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7SFwCJtOqbOS",
        "colab_type": "code",
        "outputId": "4d2218ff-602c-40b7-f9a8-281a4716f890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1597
        }
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "class LanguageDict():\n",
        "  def __init__(self, sents):\n",
        "    word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "    self.vocab = [t for t,c in word_counter.items() if c > 10]\n",
        "    self.vocab.append('<pad>')\n",
        "    self.vocab.append('<unk>')\n",
        "    self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "    self.UNK = self.word2ids['<unk>']\n",
        "    self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "def load_dataset(path, max_num_examples=30000,batch_size=100,add_start_end = False):\n",
        "  lines = [line for line in open(path,'r')]\n",
        "  if max_num_examples > 0:\n",
        "    max_num_examples = min(len(lines), max_num_examples)\n",
        "    lines = lines[:max_num_examples]\n",
        "\n",
        "  sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in lines]\n",
        "  if add_start_end:\n",
        "    for sent in sents:\n",
        "      sent.append('<end>')\n",
        "      sent.insert(0,'<start>')\n",
        "\n",
        "  lang_dict = LanguageDict(sents)\n",
        "\n",
        "  sents = [[lang_dict.word2ids.get(tok,lang_dict.UNK) for tok in sent] for sent in sents]\n",
        "\n",
        "  batches = []\n",
        "  for i in range(len(sents) // batch_size):\n",
        "    batch = sents[i * batch_size:(i + 1) * batch_size]\n",
        "    batch_len = [len(sent) for sent in batch]\n",
        "    max_batch_len = max(batch_len)\n",
        "    for sent in batch:\n",
        "      if len(sent) < max_batch_len:\n",
        "        sent.extend([lang_dict.PAD for _ in range(max_batch_len - len(sent))])\n",
        "    batches.append((batch, batch_len))\n",
        "\n",
        "\n",
        "  unit = len(batches)//10\n",
        "  train_batches = batches[:8*unit]\n",
        "  dev_batches = batches[8*unit:9*unit]\n",
        "  test_batches = batches[9*unit:]\n",
        "\n",
        "  return train_batches,dev_batches,test_batches,lang_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  batch_size = 100\n",
        "  max_example = 30000\n",
        "  use_attention = True\n",
        "  source_train, source_dev, source_test, source_dict = load_dataset(\"data.30.vi\",max_num_examples=max_example,batch_size=batch_size)\n",
        "  target_train, target_dev, target_test, target_dict = load_dataset(\"data.30.en\", max_num_examples=max_example,batch_size=batch_size, add_start_end=True)\n",
        "  print(\"read %d/%d/%d train/dev/test batches\" % (len(source_train),len(source_dev), len(source_test)))\n",
        "\n",
        "  train_data = (source_train,target_train)\n",
        "  dev_data = (source_dev,target_dev)\n",
        "  test_data = (source_test,target_test)\n",
        "\n",
        "  model = NmtModel(source_dict,target_dict,use_attention)\n",
        "  model.build()\n",
        "  model.train(train_data,dev_data,test_data,10)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 240/30/30 train/dev/test batches\n",
            "source vocab: 2034, target vocab:2506\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training epoch 1/10\n",
            "[100]: loss:4.98\n",
            "[200]: loss:4.36\n",
            "Average epoch loss:4.577295217911402\n",
            "Time used for epoch 1: 0 m 57 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 1.93\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 2/10\n",
            "[100]: loss:4.01\n",
            "[200]: loss:3.85\n",
            "Average epoch loss:3.8875531564156214\n",
            "Time used for epoch 2: 0 m 55 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 4.87\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 3/10\n",
            "[100]: loss:3.60\n",
            "[200]: loss:3.46\n",
            "Average epoch loss:3.4872287650903067\n",
            "Time used for epoch 3: 0 m 56 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 7.62\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 4/10\n",
            "[100]: loss:3.27\n",
            "[200]: loss:3.17\n",
            "Average epoch loss:3.1766197184721627\n",
            "Time used for epoch 4: 0 m 55 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 9.70\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 5/10\n",
            "[100]: loss:3.02\n",
            "[200]: loss:2.95\n",
            "Average epoch loss:2.9445862025022507\n",
            "Time used for epoch 5: 0 m 55 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 10.65\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 6/10\n",
            "[100]: loss:2.83\n",
            "[200]: loss:2.78\n",
            "Average epoch loss:2.765901549657186\n",
            "Time used for epoch 6: 0 m 56 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 11.49\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 7/10\n",
            "[100]: loss:2.68\n",
            "[200]: loss:2.64\n",
            "Average epoch loss:2.6262858361005783\n",
            "Time used for epoch 7: 0 m 56 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 11.93\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 8/10\n",
            "[100]: loss:2.56\n",
            "[200]: loss:2.52\n",
            "Average epoch loss:2.5051740323503813\n",
            "Time used for epoch 8: 0 m 55 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 12.06\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 9/10\n",
            "[100]: loss:2.45\n",
            "[200]: loss:2.42\n",
            "Average epoch loss:2.4038231606284777\n",
            "Time used for epoch 9: 0 m 55 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 12.88\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Starting training epoch 10/10\n",
            "[100]: loss:2.36\n",
            "[200]: loss:2.34\n",
            "Average epoch loss:2.316838069756826\n",
            "Time used for epoch 10: 0 m 55 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 13.05\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 9 m 54 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 13.36\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}