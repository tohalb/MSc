{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 10 Dialogue Act Tagging orig.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9_ZORURKg-fp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lab 10: Dialogue Act Tagging\n",
        "\n",
        "Dialogue act (DA) tagging is an important step in the process of developing dialog systems. DA tagging is a problem usually solved by supervised machine learning approaches that all require large amounts of hand labeled data. A wide range of techniques have been investigated for DA tagging. In this lab, we explore two models for DA classification. We are using the Switchboard Dialog Act Corpus for training.\n",
        "Corpus can be downloaded from http://compprag.christopherpotts.net/swda.html.\n"
      ]
    },
    {
      "metadata": {
        "id": "ziKyA9R4gyw9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The downloaded dataset should be kept in a data folder in the same directory as this file. "
      ]
    },
    {
      "metadata": {
        "id": "jmTpKt_uefe5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import tarfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "quKjrce33c7q",
        "colab_type": "code",
        "outputId": "764399b8-5062-4320-eff8-3572da00e870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "# For use with Colab - Check on GPU\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.4 GB  | Proc size: 144.7 MB\n",
            "GPU RAM Free: 10139MB | Used: 1302MB | Util  11% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HsmACOydBoYn",
        "colab_type": "code",
        "outputId": "f3310d54-aede-4e0b-95c2-8166d48b01fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://compprag.christopherpotts.net/code-data/swda.zip\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-28 01:19:48--  http://compprag.christopherpotts.net/code-data/swda.zip\n",
            "Resolving compprag.christopherpotts.net (compprag.christopherpotts.net)... 64.90.36.20\n",
            "Connecting to compprag.christopherpotts.net (compprag.christopherpotts.net)|64.90.36.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14040987 (13M) [application/zip]\n",
            "Saving to: ‘swda.zip’\n",
            "\n",
            "swda.zip            100%[===================>]  13.39M  9.04MB/s    in 1.5s    \n",
            "\n",
            "2019-03-28 01:19:50 (9.04 MB/s) - ‘swda.zip’ saved [14040987/14040987]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uy_8y5847kEv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_to_zip_file = 'swda.zip'\n",
        "directory_to_extract_to = os.getcwd()  \n",
        "\n",
        "zip_ref = zipfile.ZipFile(path_to_zip_file, 'r')\n",
        "zip_ref.extractall(directory_to_extract_to)\n",
        "zip_ref.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7okyvB6WDPKu",
        "colab_type": "code",
        "outputId": "42a798f9-f7ce-4473-9bb6-c2b86c5d3271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "os.listdir()  # Check files are all there\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " '__MACOSX',\n",
              " 'swda',\n",
              " 'data.30.en',\n",
              " 'swda.zip',\n",
              " 'data.30.vi',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "6E8axaw1hAbM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = glob.glob(\"swda/sw*/sw*.csv\") #csv??\n",
        "\n",
        "#f = glob.glob(\"sw*/sw*.utt\")\n",
        "frames = []\n",
        "for i in range(0, len(f)):\n",
        "    frames.append(pd.read_csv(f[i]))  \n",
        "    #frames.append(pd.read_csv(f[i], error_bad_lines=False))\n",
        "result = pd.concat(frames, ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7hKGF7EhM4s",
        "colab_type": "code",
        "outputId": "4b452bdb-18f6-4648-92fb-bf88b299e9ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of converations in the dataset:\",len(result))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of converations in the dataset: 223606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ttyB2lQhc7B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset has many different features, we are only using act_tag and text for this training.\n"
      ]
    },
    {
      "metadata": {
        "id": "-jUifIdshhD0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reduced_df = result[['act_tag','text']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0SB4oj99hiJy",
        "colab_type": "code",
        "outputId": "b4a2e840-9708-4aa0-92ae-cb61c1b884a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "reduced_df.head()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>act_tag</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>o</td>\n",
              "      <td>Okay, {F uh. } /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qy</td>\n",
              "      <td>Do you have annual family reunions  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>%</td>\n",
              "      <td>{C or, } -/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ny</td>\n",
              "      <td>{F Uh, } yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>%</td>\n",
              "      <td>{F uh, }  our, - /</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  act_tag                                   text\n",
              "0       o                       Okay, {F uh. } /\n",
              "1      qy  Do you have annual family reunions  /\n",
              "2       %                            {C or, } -/\n",
              "3      ny                      {F Uh, } yeah,  /\n",
              "4       %                     {F uh, }  our, - /"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "0UNy0vvhhqpD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Theere are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
      ]
    },
    {
      "metadata": {
        "id": "9dR1rKmkh9QG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can check the frequency of tags."
      ]
    },
    {
      "metadata": {
        "id": "x2DzS0iUh-bU",
        "colab_type": "code",
        "outputId": "d4f8623f-2ee7-4fc0-ef0f-31f192276318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1104
        }
      },
      "cell_type": "code",
      "source": [
        "reduced_df['act_tag'].value_counts()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sd            70464\n",
              "b             36180\n",
              "sv            25696\n",
              "+             17813\n",
              "%             15547\n",
              "aa            10136\n",
              "ba             4523\n",
              "qy             3785\n",
              "x              3628\n",
              "ny             2826\n",
              "fc             2404\n",
              "b^r            2102\n",
              "sd^e           1939\n",
              "qw             1890\n",
              "sd(^q)         1341\n",
              "bk             1254\n",
              "nn             1230\n",
              "h              1218\n",
              "qy^d           1217\n",
              "bh             1044\n",
              "^q              972\n",
              "bf              934\n",
              "sd^t            929\n",
              "aa^r            916\n",
              "+@              867\n",
              "o               801\n",
              "na              764\n",
              "^2              714\n",
              "b^m             688\n",
              "ad              666\n",
              "              ...  \n",
              "b^2               1\n",
              "sd^r@             1\n",
              "sd^m@             1\n",
              "aap^r             1\n",
              "qo^d^c            1\n",
              "qw^d@             1\n",
              "t1^t              1\n",
              "^2^t              1\n",
              "h,sd              1\n",
              "fw*               1\n",
              "^2^r              1\n",
              "bh,sd,o@          1\n",
              "b^m,sd,o@         1\n",
              "no@               1\n",
              "sv^e^r            1\n",
              "qo^r              1\n",
              "sd;sv             1\n",
              "bd@               1\n",
              "qh*               1\n",
              "sd(^q)^r          1\n",
              "sd^e(^q)^r        1\n",
              "ng^t              1\n",
              "%@*               1\n",
              "b^r@              1\n",
              "qh(^q)            1\n",
              "h^m               1\n",
              "qr^d*             1\n",
              "ft^m              1\n",
              "sd^t*             1\n",
              "sv;sd             1\n",
              "Name: act_tag, Length: 303, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "6G4zBbtB5Ipz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline BiLSTM Model"
      ]
    },
    {
      "metadata": {
        "id": "9biiyP8UiGDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To get unique tags. "
      ]
    },
    {
      "metadata": {
        "id": "xxn2s_4jiKkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unique_tags = set()\n",
        "for tag in reduced_df['act_tag']:\n",
        "    unique_tags.add(tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMOX5KwgiPmu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_hot_encoding_dic = pd.get_dummies(list(unique_tags))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZPHPCxE3iPby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tags_encoding = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVI8QyVzjqWh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The tags are one hot encoded."
      ]
    },
    {
      "metadata": {
        "id": "SQJTiffPjUtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To create sentence embeddings:"
      ]
    },
    {
      "metadata": {
        "id": "PmkyD1TfjWGO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for i in range(0, len(reduced_df)):\n",
        "    sentences.append(reduced_df['text'].iloc[i].split(\" \"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MlD6L6e3jV-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wordvectors = {}\n",
        "index = 1\n",
        "for s in sentences:\n",
        "    for w in s:\n",
        "        if w not in wordvectors:\n",
        "            wordvectors[w] = index\n",
        "            index += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7_cjDHrjV1c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(sentences, key=len))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LX6DidEvjVWs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_embeddings = []\n",
        "for s in sentences:\n",
        "    sentence_emb = []\n",
        "    for w in s:\n",
        "        sentence_emb.append(wordvectors[w])\n",
        "    sentence_embeddings.append(sentence_emb)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nr4iEyNTjmlu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then we split the dataset into test and train."
      ]
    },
    {
      "metadata": {
        "id": "GiNZ-iI_jnOF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentence_embeddings, np.array(tags_encoding))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJpwQy1e9z-o",
        "colab_type": "code",
        "outputId": "65ee6d08-7c18-4f41-ca95-7ddb9493a88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "tot = len(X_train) + len(X_test)\n",
        "print(len(X_train)/tot)\n",
        "print(len(X_test))\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7499977639240449\n",
            "55902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_RqMeWe_jron",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And pad the sentences with zero to make all sentences of equal length.\n"
      ]
    },
    {
      "metadata": {
        "id": "hxiet3Q1wv0x",
        "colab_type": "code",
        "outputId": "8b60c1c4-c32e-4525-a75b-227cf3084f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167704, 303)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Ai9cwv82jufe",
        "colab_type": "code",
        "outputId": "1636a02c-d765-497e-ec1a-0229bb664487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        " \n",
        "train_sentences_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "# train_sentences_y = pad_sequences(y_train, maxlen=MAX_LENGTH, padding='post')\n",
        "# test_sentences_y = pad_sequences(y_test, maxlen=MAX_LENGTH, padding='post')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "FItlHC1Fjz6y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " The model architecture is as follows: Embedding Layer (to generate word embeddings) Next layer Bidirectional LSTM. Feed forward layer with number of neurons = number of tags. Softmax activation to get probabilities."
      ]
    },
    {
      "metadata": {
        "id": "yZ3EsrzJ067f",
        "colab_type": "code",
        "outputId": "f61e81db-b974-4797-f9dd-031f121d3a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "train_sentences_X.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(167704, 137)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "LCaX-ptaj8G2",
        "colab_type": "code",
        "outputId": "b422a3da-23f8-416d-bf17-211f02c8dfd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
        "from keras.optimizers import Adam\n",
        "#Building the network\n",
        "vocab_size = len(wordvectors)\n",
        "num_tags = len(unique_tags)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "# Set the input dimension to the vocabulary size as counted in the word vectors\n",
        "# Set Output dimension to the maximum sequence length\n",
        "model.add(Embedding(vocab_size,MAX_LENGTH)) \n",
        "\n",
        "# Bidirectional LSTM\n",
        "# Output dimension of the embedding layer is passed to this layer\n",
        "model.add(Bidirectional(LSTM(MAX_LENGTH)))\n",
        "\n",
        "# Feed forward layer with number of neurons = number of tags. \n",
        "# Softmax activation to get multiple probabilities.\n",
        "model.add(Dense(num_tags))\n",
        "model.add(Activation('softmax'))          \n",
        "          \n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 137)         5991147   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 274)               301400    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 303)               83325     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 303)               0         \n",
            "=================================================================\n",
            "Total params: 6,375,872\n",
            "Trainable params: 6,375,872\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hmq3AuBtE0yZ",
        "colab_type": "code",
        "outputId": "1f36e2e9-5c03-4796-f788-0403557b2262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "cell_type": "code",
      "source": [
        "# Set loss (categorical cross entropy for multiple outputs)\n",
        "# Select optimiser - the adaptive momentum optimiser, Adam \n",
        "f_optimizer = 'adam'\n",
        "f_loss='categorical_crossentropy'\n",
        "\n",
        "#model.compile(optimizer = f_optimizer, loss=f_loss, accuracy = ['metrics'])\n",
        "model.compile(optimizer = f_optimizer, loss=f_loss, metrics = ['accuracy'])\n",
        "\n",
        "'''\n",
        "history = model.fit(train_sentences_X,\n",
        "                    #train_sentences_y,\n",
        "                    epochs = 1,\n",
        "                    batch_size=512,\n",
        "                    #validation_data=(test_sentences_X, test_sentences_y),\n",
        "                    verbose = 1)\n",
        "'''\n",
        "\n",
        "history = model.fit(train_sentences_X,\n",
        "                    y_train,\n",
        "                    epochs = 5,\n",
        "                    batch_size=512,\n",
        "                    verbose=1)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "167704/167704 [==============================] - 176s 1ms/step - loss: 2.0253 - acc: 0.5031\n",
            "Epoch 2/5\n",
            "167704/167704 [==============================] - 174s 1ms/step - loss: 1.3590 - acc: 0.6389\n",
            "Epoch 3/5\n",
            "167704/167704 [==============================] - 170s 1ms/step - loss: 1.1428 - acc: 0.6898\n",
            "Epoch 4/5\n",
            "167704/167704 [==============================] - 170s 1ms/step - loss: 1.0191 - acc: 0.7175\n",
            "Epoch 5/5\n",
            "167704/167704 [==============================] - 170s 1ms/step - loss: 0.9370 - acc: 0.7372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pE5aceOmAXwB",
        "colab_type": "code",
        "outputId": "29cc4f2a-1aef-4ded-a060-190554d5bedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "#val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "#val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "#plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XtgzvX///H7ddh1rTHa2IgohFBL\nQvk4ZYwZnfgqKjr4dCAfhMSy5nwm6ffpJB0+KulARzYUJck5SX1SK3K2jbHZ6dp1vX9/XB8Xyw4O\n265d1x63fz67Tu/r+dr7k8der/f79XqZDMMwEBEREZ9n9nYBIiIiUjIU6iIiIn5CoS4iIuInFOoi\nIiJ+QqEuIiLiJxTqIiIifkKhLn4vPj6e6OhooqOjadasGZ06dfI8zsjIuKBjRUdHk5KSUuR75syZ\nw+LFiy+l5BL34IMPsnTp0hI5VuPGjTl8+DCrVq1i7Nixl/R977//vufn8/ndikjRrN4uQKS0TZgw\nwfNzZGQkM2fOpGXLlhd1rISEhGLfM3LkyIs6tq+JiooiKirqoj+fnJzMa6+9xt133w2c3+9WRIqm\nnrpUeP379+e5556je/fubNu2jZSUFAYOHEh0dDSRkZG88cYbnvee7qVu3LiRe+65hzlz5tC9e3ci\nIyPZtGkTAGPGjOHFF18E3H9EvPfee/zf//0f7dq1Y/r06Z5jvfzyy7Rp04bevXvzzjvvEBkZWWB9\nH3zwAd27d6dr167cd999HDhwAIClS5cydOhQYmNj6datGzExMfz2228A7Nu3jz59+tClSxdGjhyJ\n0+k857hff/01t912W77n7rjjDr755psifwenLV26lAcffLDY7/vyyy+57bbb6NatG7169eKXX34B\noG/fvhw8eJDo6Ghyc3M9v1uA//znP8TExBAdHc2gQYM4duyY53c7f/58HnroITp16sRDDz1EVlbW\nObVlZWUxfPhwunXrRmRkJDNmzPC8tm/fPu677z6ioqLo3bs3u3btKvL5yMhItmzZ4vn86cf79++n\nXbt2TJ06lfvvv7/ItgK8+uqrdO7cmW7dujFt2jScTidt27Zl586dnve8/fbbDB48+Jz2iJwvhboI\n8NNPP/HFF1/QokULXnrpJa688koSEhJ46623mDNnDocOHTrnMz///DM33HADK1as4N577+Wll14q\n8NibN29myZIlfPTRR7z99tscPnyY3377jddee41PPvmEd999t9BeampqKhMnTuSNN95g5cqV1K1b\n1/MHA8A333zDvffeS2JiIjfffDNvvfUWALNnz6ZNmzasXr2aBx54gG3btp1z7DZt2nD48GH27dsH\nuEPt8OHD/OMf/zjv38FphX1fXl4eY8aMYdKkSSQmJuYL2KlTp3LFFVeQkJCAzWbzHOuHH35g4cKF\nLFq0iISEBGrVqsWcOXM8ryckJPDcc8+xatUqjh07xqpVq86pZ/HixZw6dYqEhASWLVvG0qVLPcEc\nFxdHjx49WLVqFYMGDWL06NFFPl+UtLQ0mjRpwttvv11kW7ds2cKHH37IJ598wmeffcbWrVtZuXIl\n3bt35/PPP/ccb9WqVfTo0aPY7xUpjEJdBOjYsSNms/s/h3HjxhEXFwdAnTp1CAsLY//+/ed8plKl\nSnTp0gWAZs2acfDgwQKPfdttt2GxWKhRowbVqlXj0KFDbN68mdatWxMeHo7dbqd3794FfrZatWps\n3bqVmjVrAtCyZUtPCAM0aNCA6667DoCmTZt6gnfLli3ExMQAEBERQf369c85ts1mo1OnTnz11VcA\nrF69mi5dumC1Ws/7d3BaYd9ntVr57rvvaN68eYH1F2Tt2rV069aNatWqAdCnTx/Wr1/veb1jx45c\nfvnlWK1WGjVqVOAfGw8//DAvvvgiJpOJqlWr0rBhQ/bv309OTg4bN26kZ8+eAHTu3Jn333+/0OeL\n43A4PJcgimrrN998Q8eOHalcuTI2m41FixbRtWtXevTowfLly3G5XKSlpfHTTz/RqVOnYr9XpDC6\npi4CVK1a1fPzzp07PT1Ts9lMcnIyLpfrnM8EBwd7fjabzQW+B6By5cqeny0WC06nk5MnT+b7zho1\nahT4WafTyfz58/nqq69wOp2cOnWKevXqFVjD6WMDnDhxIt/3VqlSpcDjd+vWjf/85z888MADrF69\n2jP0e76/g9OK+r5FixaxbNkycnNzyc3NxWQyFXocgGPHjhEeHp7vWKmpqcW2+Wx79uxh+vTp/PHH\nH5jNZg4fPkyvXr1IS0vD5XJ5jmEymahUqRJHjhwp8PniWCyWfO0urK3Hjx/P16bLLrsMgBtvvJGA\ngAA2bdrE4cOHadeuHUFBQcV+r0hh1FMX+ZunnnqKbt26kZiYSEJCAiEhISX+HZUrVyYzM9Pz+OjR\nowW+b/ny5Xz11Ve8/fbbJCYmMnTo0PM6fpUqVfLd2X/6mvTftW/fnv/+97/s2bOHPXv2cMsttwAX\n/jso7Pu2bdvGggULeOmll0hMTGTy5MnF1l69enXS0tI8j9PS0qhevXqxnzvbxIkTadiwIStWrCAh\nIYFrr70WgJCQEEwmE8ePHwfAMAz27t1b6POGYZzzB9uJEycK/M6i2hoSEuI5NrhD/vTjHj16kJCQ\nQEJCgme0Q+RiKdRF/iY1NZXrrrsOk8nEsmXLyMrKyhfAJSEiIoKNGzdy7NgxcnNz+fjjjwutpXbt\n2oSGhnL8+HFWrFjBqVOnij1+8+bNPdeat23bxl9//VXg+2w2G+3atWPWrFl07twZi8Xi+d4L+R0U\n9n3Hjh2jWrVq1KpVi6ysLJYtW0ZmZiaGYWC1WsnMzCQvLy/fsW699VZWrVrlCb333nuPjh07Ftvm\ns6WmptKkSRMsFgvr169n7969ZGZmYrPZaNu2LcuWLQNg3bp1PProo4U+bzKZCAsL47///S/g/iMr\nJyenwO8sqq2RkZF89dVXnDhxgry8PJ544gm+/fZbAHr27Mnq1avZvn37BbdT5O8U6iJ/M2zYMJ54\n4gluu+02MjMzueeee4iLiys0GC9GREQEd911F3fddRcDBgwo9Dpqz549SUtLIyoqipEjRzJ8+HAO\nHz6c7y76gjz11FOsWbOGLl268M477/CPf/yj0Pd269aN1atX0717d89zF/o7KOz72rdvT3h4OF26\ndOHhhx/mgQceIDg4mKFDh9K4cWOqVq1K27Zt892PEBERwaOPPsp9991HdHQ06enpPPnkk0W29+8G\nDRrEjBkz6NmzJ5s2bWLIkCG88MILbN26lSlTprBmzRo6d+7MvHnzmD17NkChzw8ePJg333yTnj17\nkpSUxDXXXFPgdxbV1ubNmzNw4EDuvPNOevToQdOmTT3X7xs3bszll19Ou3btCAwMvKB2ivydSfup\ni3iHYRiea65r165l3rx5hfbYxb898sgj3H///eqpyyVTT13EC44dO8Ytt9zCgQMHMAyDFStWeO6a\nlopl69atHDhwgPbt23u7FPEDuvtdxAtCQ0MZPnw4Dz74ICaTifr165/XvGjxL2PHjmXbtm3MmjXL\nM6VS5FJo+F1ERMRP6E9DERERP6FQFxER8RM+f009OTm9RI8XEhLE8eMlOyfZW9SW8sdf2gFqS3nk\nL+0AtaUoYWHBhb6mnvrfWK0Wb5dQYtSW8sdf2gFqS3nkL+0AteViKdRFRET8hEJdRETETyjURURE\n/IRCXURExE8o1EVERPyEQl1ERMRPKNRFRET8hM8vPlMevfDCc/z66y8cO5ZKdnY2tWrVpkqVqkyd\nOqvYzy5f/hmVKlWmY8eC99d+/vk59OnTl1q1apd02SIi4uMU6qXgX/96EnAH9B9/JDFkyPDz/mxM\nzG1Fvj5s2MhLqk1ERPyXQr0Mbdu2hffee5vMzEyGDHmS7du3snbtl7hcLtq0acvDDz/KwoWvcPnl\nl1OvXgOWLn0fk8nM3r1/cuutnXn44UcZMuRRRowYzZo1X3LqVAZ//bWXAwf2M3ToSNq0acvbb7/J\n6tUrqVWrNhYL3HXXPbRo0dJTw+bNG3nttZcJCAggODiYiROnExAQwLx5s/n555+wWCw89dRY6te/\npsDnRESk/PL7UB8/3s5nn51/M81mcLkqFfme227LY/z4nIuqJynpdxYvXorNZmP79q28+OJrmM1m\n7r77Du6559587/355128++5HuFwu+vS5jYcffjTf60ePHmH27Pl8//13fPLJRzRrdh1Ll37A4sUf\ncerUKfr168Vdd92T7zPp6enEx0+mVq3aTJr0LBs3bsBut3P06BFeffVNfvhhG19+uYrU1NRznlOo\ni4icH5cLdu82s327mQ4doHYZXTH1+1Avb665piE2mw2AwMBAhgx5FIvFQlpaGidPnsz33saNryUw\nMLDQY0VENAcgPDycjIwM9u/fR/36DbDbA7HbA4mIiDjnM5dffjkzZkzG6XRy8OABbrqpFcePH+P6\n628AoHnzFjRv3oJ33nnrnOdERKRgGRmwdauFLVssbN7s/t+TJ00AdO8Ob71VNnX4faiPH59zQb3q\nsLBgkpNPlVo9AQEBABw+fIglS97h9dffISgoiP797z7nvRZL0ZsAnP26YRgYBpjNZyY0mEymcz4z\nbdokZs2ax9VX12Pu3BkAmM0WDMOV730FPSciImAYsGePyRPgmzdb+OUXMy7XmX9z69d30b17Hi1b\nOhkwoPDOWUnz+1Avr9LS0ggJCSEoKIhff/0vhw8fxuFwXNIxr7jiCv74I4m8vDzS09P56aefznnP\nqVMZ1KhRk/T0dLZt20qDBg1p0qQpb7/9JvfeO4Ddu//LZ599QufOUec8N3Lk05dUn4iIL8rKgh07\nLGzZYvaEeErKmQ5UYKDBzTc7adnSSatWTlq2dFG9uuF5PSwskOTksqlVoe4lDRs24rLLghg06GGu\nv745d9zRizlzZhARccNFHzM0tBpRUdE88sgArrqqHhEREef09nv16sOgQQOpU6cu9903gNdff5WX\nXnqdq66qx+DB/wRg5MgxNGhwDevWfZ3vORGRiuDQIXcvfNMm9zD6jz+acTjO9MJr13Zxxx0OWrVy\nh3izZi7+d1XV60yGYRjFv638Sk5OL9HjuYffS/aYZWn58s+IiorGYrHw8MP3MnPm84SH1/B2WZfM\n18/Laf7SDlBbyiN/aQeUXVscDvj55zM98M2bLezff6YXbrUaXH+9yxPgLVs6qV37wmKzpNsSFhZc\n6GvqqfuZ1NRUHn30AQICbNx2221+EegiIiUlNdXE1q1nQnz7dgtZWWd64dWquYiOdtCypYvWrZ1E\nRDgJCvJiwRdIoe5n+vd/kP79HwT86692EZELdXpa2dm98KSks28mNrj22jO98FatnNSrZ1DAPcY+\nQ6EuIiJ+IT0dtm07E+Bbt56ZVgYQHGxw6615ngBv0cJJlSpeLLgUKNRFRMTnnJ5WdjrAt2wpfFrZ\n6WvhjRu7KGamsM9TqIuISLl3elqZO8DNFzytrKJQqIuISLlz4AAkJFg9vfCCppXdeafDE+LlaVqZ\nNynURUTEqxwO2LXLnG+Ftv37AS4D3NPKIiJcZ/XCL3xaWUWhUBcRkTJV3LSy6tVd3H47RETk0Lq1\nkxtucHLZZV4s2Ico1EVEpNRc7LSy8PBgkpNzvVi5b1Koi4hIidG0Mu9SqIuIyEX5+7Sy07uVGUbF\nnlbmTQp1ERE5L2dPK9u82X1j29+nld1yy5lh9JtuqpjTyrxJoS4iIgU6dCh/L3znzoKnlZ3uhWta\nmfcp1EVExDOt7PS88IJ2K4uIcHkCXNPKyieFuohIBZSaamLLljNzwwuaVnb2bmWaVuYbFOoiIn7O\n5YJff83fC//7tLImTc4s7uIPu5VVVAp1ERE/k54OW7eeCXBNK6s4FOoiIj4uOxu++cbC+vXw9ddB\nRU4ra9XKSaNGmlbmrxTqIiI+KD0dVq+2sny5ldWrrZw65Q7xwECzppVVYAp1EREfkZxsIjHRyhdf\nWFm3zkJurjvIr77axQMPOLj3Xhv16mUQEODlQsVrFOoiIuXYX3+ZWL7c3SPftMmCy+UO8mbNnMTE\n5NGjRx5NmrgwmSAszEZyspcLFq9SqIuIlCOG4b5T/Ysv3EG+c6f74rfJZNCqlTvIY2LyuPpqDanL\nuRTqIiJe5nLB9u2ngzyAP/5wTzcLCDDo1MndG+/WLY8aNRTkUjSFuoiIFzgc8N13FpYvt7JihZXD\nh91BHhRk0LOngx498ujSJY+qVb1cqPgUhbqISBnJzIS1a93D6itXWklLc18fDwkx6NvXQUyMg44d\ntXKbXDyFuohIKTpxAlaudAf5mjVWMjPdQX7FFS5693YQE5NHmzZOrPrXWEqA/m8kIlLCjhwxsWKF\ne+rZ+vUW8vLcQd6ggYsePXKJicmjeXMXZnMxBxK5QAp1EZES8Oef7qlnX3wRwNatZ1Z0u+GGM3es\nN2rk0nrqUqoU6iIiF8Ew3FuVnp569ssv7qlnZrNBmzbuIO/ePY86dXTHupQdhbqIyHlyuWDzZosn\nyP/6yz1+brMZREW5e+PduuVpWVbxGoW6iEgRcnPh22/dQZ6QYCU52R3klSsb3Hmne+pZ5855VK7s\n5UJFUKiLiJwjIwPWrHHf6LZ6tdWzbWn16i7uv999o1v79k7sdi8XKvI3CnUREeDYsTNTz9autZKd\n7Q7yK6900a+fe+pZ69ZObVkq5ZpCXUQqrIMH3VPPVq2Cr7+ujNPpDvLGjZ306OG+Rn799bpjXXyH\nQl1EKpTffzexfHkAX3xhZfv2M93um25y0b17Hj16OGjQQDe6iW8q1VCfOnUqO3bswGQyERsbS0RE\nBABHjhxh1KhRnvft27ePkSNHEh0dzZgxYzh48CAWi4Vp06ZRp06d0ixRRPycYcCPP5r/N4fcyu7d\n7iC3WAzat3f3xvv3D8Rmy/RypSKXrtRCfdOmTezdu5clS5aQlJREbGwsS5YsAaBGjRosWrQIgLy8\nPPr3709kZCSff/45VapUYc6cOXz77bfMmTOHefPmlVaJIuKnnE7YuNHi2Yd8/373HeuBgQbR0e7r\n41275hEa6n5/WFig9iEXv1Bqob5hwwa6dOkCQIMGDThx4gQZGRlU/tu8j2XLltGtWzcqVarEhg0b\nuPPOOwH4xz/+QWxsbGmVJyJ+Jjsb1q1zTz1LTLSSmuoO8ipVDM8a65GReVSq5OVCRUpRqYV6SkoK\nzZo18zwODQ0lOTn5nFD/4IMPeP311z2fCf3fn85msxmTyURubi42m620yhQRH5aeDqtXu3vjq1db\nOXXKfUdbeLiLAQPcU8/atXOif0KkoiizG+UM49wbT7Zv3079+vXPCfqiPvN3ISFBWK0lO8ckLCy4\nRI/nTWpL+eMv7QDvtOXoUfj0U1i2DFavdi8OA1C/Ptx1F/TqBbfcYsZstgHnn+b+cl78pR2gtlyM\nUgv18PBwUlJSPI+PHj1KWFhYvvesXbuWNm3a5PtMcnIy1157LQ6HA8Mwiu2lHz9esje3hIUFk5yc\nXqLH9Ba1pfzxl3ZA2bZl3z6T5/r4xo0WXC53j7xpU/ca6z165NG06ZmpZ6mpF3Z8fzkv/tIOUFuK\nO15hSi3U27ZtywsvvEDfvn3ZtWsX4eHh5/TId+7cSUxMTL7PJCQk0L59e9asWcPNN99cWuWJSDlm\nGPDrr2ZPkP/4o3s0zmQyaNnSRY8eDrp3z6NePU09EzlbqYV6ixYtaNasGX379sVkMhEfH8/SpUsJ\nDg4mKioKgOTkZKpVq+b5TExMDN999x39+vXDZrMxffr00ipPRMoZlwu2bz8d5AEkJblvdLNaDW69\n1d0bj47Oo0YNBblIYUr1mvrZc9EBrr322nyPP/vss3yPT89NF5GKweGADRvcU89WrLBy6JA7yIOC\nDHr2dN+xHhWVR9WqXi5UxEdoRTkRKVNZWbB2rXshmJUrraSluS+EX365wT33uIP81lvzuOwyLxcq\n4oMU6iJS6k6cgFWr3EG+Zo2VzEx3kF9xhYtevdxB3qaNk4AALxcq4uMU6iJSKo4cMZGQ4A7yb7+1\nkJfnDvIGDVzExLjnkN94owuz2cuFivgRhbqIlJg//zR5bnTbssWMYbiDPCLizNSzRo2065lIaVGo\ni8hFMwzYtevM1LOff3ZPPTObDW65xR3k3bvnUbeu7lgXKQsKdRG5YLt2mfn8c/jww0rs3eseP7fZ\nDLp0ce961q1bHmFhCnKRsqZQF5HzlpxsYto0G++8E4BhQKVKJu64w0GPHnl07pxHsP+s6inikxTq\nIlIshwMWLgxg9mw7J0+aaNzYybRpFlq2zCAw0NvVichpCnURKdJXX1mIi7Pz228WqlY1mDo1mwcf\ndHDFFcHag1yknFGoi0iB/vjDRHx8IImJVsxmgwceyGXMmFyqVdO1cpHySqEuIvlkZMBzz9l45RUb\nubkm2rTJY/LkHK6/3uXt0kSkGAp1EQHcG6p88IGVyZPtHDlipnZtF+PHZ3P77XmaVy7iIxTqIsK2\nbWaeeSaQrVstBAYajBqVw5AhuQQFebsyEbkQCnWRCuzIERNTpth57z33ouu33+4gPj6HOnV03VzE\nFynURSqg3Fx49dUA5s61k5FhomlTJ1Om5NC2rdPbpYnIJVCoi1Qwq1ZZiIsL5I8/zISEGMyYkU3/\n/g6s+tdAxOfpP2ORCuL3303ExQXy5ZdWLBaDgQNzGT06h5AQb1cmIiVFoS7i506ehDlz7CxYEEBe\nnon27d1T1Jo00RQ1EX+jUBfxUy4XvPeee4paSoqZunVdTJiQTUyMpqiJ+CuFuogf2rzZPUXthx8s\nBAUZjB2bw6BBuVqnXcTPKdRF/MjhwyYmTrTz4YfuKWq9ejl49tkcatXSFDWRikChLuIHsrPhlVds\nPPecjcxMExER7ilqN9+sKWoiFYlCXcSHGQasWGElPt7O3r1mqld3MXlyDv36ObBYvF2diJQ1hbqI\nj/r1VzPjxtn5+msrVqvBY4/lMmpUDlWrersyEfEWhbqIj0lLg1mz7Lz+egBOp4lOnfKYNCmHRo00\nRU2kolOoi/gIpxPefjuA6dNtpKaaufpqF5MmZdG1q1NT1EQEUKiL+ITvv7cQG2vnp58sVKpkMG5c\nDo89lovd7u3KRKQ8UaiLlGMHDpiYMMHOxx+7p6jdfbeDuLgcatTQFDUROZdCXaQcysqCf//bxgsv\n2MjKMtGihZMpU7K56SZdNxeRwinURcoRw4DPP7cyfrydffvMhIe7mDEjm7vvzsNs9nZ1IlLeKdRF\nyoldu9xT1NavtxIQYDBkSA5PPplLcLC3KxMRX6FQF/GyY8dgxgw7b70VgMtlomvXPCZOzKZ+fV03\nF5ELo1AX8ZK8PHjrrQBmzLCTlmbimmucTJ6cQ2SklnYVkYujUBfxgnXrLIwbZ+eXXywEBxtMmJDN\nwIEObDZvVyYivkyhLlKG9uyBIUMC+eKLAEwmg/vuy2Xs2FzCwzXULiKXTqEuUgZOnYIXXrDx739D\nTk4ArVo5mTo1mxtu0BQ1ESk5CnWRUmQYsGyZlYkT7Rw8aKZWLRg3LovevfO0tKuIlDiFukgp2bnT\nTGysnY0brdjtBsOH5zBpkp2srDxvlyYifkqhLlLCUlJMTJtm4+23AzAME927O5gwIYerrzaoXNlO\nVpa3KxQRf6VQFykhDge8/noAs2bZOXnSROPG7ilqHTtqipqIlA2FukgJWLPGQlycnd27LVStajBl\nSjYPPuggIMDblYlIRaJQF7kEf/5pIj7eTkKCe4ragAG5jBmTS/XqmqImImVPoS5yETIyYN48Gy+/\nbCM318Qtt+QxZUoO11+vKWoi4j0KdZEL4HLBhx9amTTJzpEjZmrXdhEfn80dd2iKmoh4n0Jd5Dxt\n324mNjaQrVstBAYajByZw7/+lUtQkLcrExFxU6iLFOPIERNTp9pZvNh919vttzuIj8+hTh1dNxeR\n8kWhLlKI3FxYsCCAOXPsZGSYaNrUyZQpObRtqylqIlI+KdRFCrB6tYW4uECSksyEhBjMmJFN//4O\nrPovRkTKMf0TJXKWpCQTcXGBrF5txWIxGDgwl9GjcwgJ8XZlIiLFK9VQnzp1Kjt27MBkMhEbG0tE\nRITntUOHDjFixAgcDgdNmzZl4sSJbNy4kWHDhtGwYUMAGjVqRFxcXGmWKAJAejrMmWNnwYIAHA4T\n7dvnMXlyDk2aaIqaiPiOUgv1TZs2sXfvXpYsWUJSUhKxsbEsWbLE8/r06dN5+OGHiYqKYsKECRw8\neBCA1q1bM3/+/NIqSyQflwvee8/K5Ml2UlLM1K3rYvz4bHr00BQ1EfE95tI68IYNG+jSpQsADRo0\n4MSJE2RkZADgcrnYunUrkZGRAMTHx1OrVq3SKkWkQJs3m4mODmL48MvIzDQxZkwO69adomdPBbqI\n+KZSC/WUlBRCzroQGRoaSnJyMgDHjh2jUqVKTJs2jX79+jFnzhzP+37//Xcef/xx+vXrx/r160ur\nPKnADh82MXhwID16VOKHHyz06uXgu+9OMWJELpdd5u3qREQuXpndKGcYRr6fjxw5woABA6hduzaP\nPvooa9eupUmTJgwZMoTu3buzb98+BgwYwMqVK7HZbIUeNyQkCKvVUqK1hoUFl+jxvEltOSM7G557\nDqZMgVOn4MYbYf58aNcuACi7nVd0Tsonf2mLv7QD1JaLUWqhHh4eTkpKiufx0aNHCQsLAyAkJIRa\ntWpRt25dANq0acNvv/3GrbfeSkxMDAB169alevXqHDlyhDp16hT6PcePZ5Zo3WFhwSQnp5foMb1F\nbXEzDEhIsPLss3b27jVTrZqLiRNzufdeBxYL/G8AqUzonJRP/tIWf2kHqC3FHa8wpTb83rZtWxIT\nEwHYtWsX4eHhVK5cGQCr1UqdOnXYs2eP5/V69erx6aefsnDhQgCSk5NJTU2lRo0apVWiVAC//mrm\n7rsv44EHLuPAAROPPZbL99+fon9/d6CLiPiTUuupt2jRgmbNmtG3b19MJhPx8fEsXbqU4OBgoqKi\niI2NZcyYMRiGQaNGjYiMjCQzM5NRo0bx5Zdf4nA4GD9+fJFD7yKFOXECZs2ys3BhAE6niU6d8pg0\nKYdGjTRFTUT8l8k4+2K3DypOuNzvAAAgAElEQVTp4RkN+ZRP59sWpxPeeSeAadNspKaaufpqF5Mm\nZdO1q7Nc3NFeEc+JL/CXtvhLO0BtKe54hdGKcuI3vv/ewjPP2Nm500KlSgbjxuXw2GO52O3erkxE\npGwo1MXnHThgYuJEO8uWue9gv/tuB+PG5VCzpk8PQomIXDCFuvisrCx48UUb8+fbyMoyceONTqZM\nyaZlS103F5GKSaEuPscw4PPPrYwfb2ffPjNhYS5mzMjm7rvzMJfafA4RkfJPoS4+5eefzYwbZ+fb\nb60EBBg88UQuI0bkEOw/a1SIiFw0hbr4hGPHYMwYO2++GYDLZSIqKo+JE7Np0EDXzUVETlOoS7lm\nGPDmmwHMmAHHjtlo0MDF5MlZdO7s9HZpIiLljkJdyrUFCwIYNy6QKlVg/Phs/vlPB1qPSESkYMWG\nelJSEg0aNCiLWkTy+esvE1On2gkJMfjxRxN2u8PbJYmIlGvF3is8dOhQ+vXrx0cffURWVlZZ1CSC\nYcCoUYFkZpqYNCmbK6/0dkUiIuVfsT31L774gt27d7NixQr69+9PkyZN6NOnDxEREWVRn1RQH3xg\nZe1aK5065dGnT563yxER8QnnNau3UaNGDBs2jDFjxpCUlMTgwYO57777PLusiZSk5GQTcXGBBAUZ\nzJ6dXS7WbBcR8QXF9tQPHDjAsmXL+Pzzz7nmmmt4/PHHad++PTt37uSpp57igw8+KIs6pQIZN87O\n8eMmpkzJpk4dTVkTETlfxYZ6//79+b//+z/eeuutfHubR0REaAheStzKlRaWLQvgppucPPywbowT\nEbkQxQ6/f/rpp1x99dWeQF+8eDGnTp0CIC4urnSrkwolPR1Gjw4kIMDgueeysVi8XZGIiG8pNtTH\njh1LSkqK53F2djajR48u1aKkYpo82c7Bg2aGDcvl2mu1KYuIyIUqNtTT0tIYMGCA5/FDDz3EyZMn\nS7UoqXi+/97CG2/YaNzYybBhud4uR0TEJxUb6g6Hg6SkJM/jn376CYdD1zql5GRnw4gRdkwmg7lz\ns7HbvV2RiIhvKvZGubFjxzJ48GDS09NxOp2EhoYyc+bMsqhNKoh582z8/ruFf/4zl1atNOwuInKx\nig31G264gcTERI4fP47JZOLyyy9n27ZtZVGbVAC7dpmZP9/GlVe6iI3N8XY5IiI+rdhQz8jI4JNP\nPuH48eOAezj+o48+4ttvvy314sS/OZ0wYkQgeXkmZs3KonJlb1ckIuLbir2mPnz4cH799VeWLl3K\nqVOnWLNmDePHjy+D0sTfvfpqANu3W+jd26GtVEVESkCxoZ6Tk8PEiROpXbs2Tz/9NP/5z39YsWJF\nWdQmfmzPHhPTp9upVs3F5MkadhcRKQnFDr87HA4yMzNxuVwcP36ckJAQ9u3bVxa1iZ86vQNbVpaJ\nuXOzqVZNS8GKiJSEYkP9jjvu4P3336dPnz7ExMQQGhrKVVddVRa1iZ9assTKN99Y6dIlj169tAOb\niEhJKTbU+/bti+l/22S1adOG1NRUmjRpUuqFiX86etTEs88GUqmSwcyZ2oFNRKQkFXtN/ezV5GrU\nqEHTpk09IS9yoZ55xk5amolx43K48koNu4uIlKRie+pNmjTh+eef58YbbyQgIMDzfJs2bUq1MPE/\nCQkWPvkkgFatnDz0kFYlFBEpacWG+i+//ALAli1bPM+ZTCaFulyQkyfdO7DZbO6lYM3FjhGJiMiF\nKjbUFy1aVBZ1iJ+bONHO4cNmRo/OoXFjLQUrIlIaig31e++9t8Br6O+8806pFCT+Z8MGC//5j40m\nTZwMHaod2ERESkuxoT58+HDPzw6Hg++//56goKBSLUr8h3sHtkDPDmw2m7crEhHxX8WGeuvWrfM9\nbtu2LY888kipFST+Zc4cG0lJZh57LJebbtKwu4hIaSo21P++etyhQ4f4888/S60g8R87d5r5f//P\nRp06Lp5+WkvBioiUtmJD/YEHHvD8bDKZqFy5MkOGDCnVosT35eW5h92dThOzZ2sHNhGRslBsqH/1\n1Ve4XC7M/5uD5HA48s1XFynIK68EsGOHhbvvdtCpk3ZgExEpC8XOFk5MTGTw4MGex/fddx8JCQml\nWpT4tj//NDFzpp3q1V1MnJjt7XJERCqMYkP9jTfeYNasWZ7Hr7/+Om+88UapFiW+6+wd2KZMySE0\n1NsViYhUHMWGumEYBAcHex5XrlxZa79LoRYvtrJunZWuXfO4807twCYiUpaKvaZ+3XXXMXz4cFq3\nbo1hGKxbt47rrruuLGoTH3PkiIn4+EAqV9YObCIi3lBsqI8bN45PP/2UH3/8EZPJxO233050dHRZ\n1CY+ZuxYOydOmJgxI5tatbQDm4hIWSs21LOysggICCAuLg6AxYsXk5WVRaVKlUq9OPEdX3xh5fPP\nA7j55jweeEA7sImIeEOx19SffvppUlJSPI+zs7MZPXp0qRYlvuXECRgzxv6/HdhytAObiIiXFPvP\nb1paGgMGDPA8fuihhzh58mSpFiW+ZeJEO0eOmBk5MpeGDbUUrIiItxQb6g6Hg6SkJM/jnTt34nBo\neFXc1q+3sGiRewe2IUO0A5uIiDcVe0197NixDB48mPT0dFwuFyEhIcycObMsapNyLivLvRSs2Www\nb142WmhQRMS7ig31G264gcTERA4dOsTGjRtZtmwZgwYN4ttvvy2L+qQcmz3bxp9/mnn88VxuvFHD\n7iIi3lZsqP/www8sXbqU5cuX43K5mDRpEl27di2L2qQc+/FHMy++aKNuXe3AJiJSXhR6TX3BggXE\nxMTw5JNPEhoaykcffUTdunXp0aOHNnSp4PLy4Mkn3TuwzZmTjWY3ioiUD4X21OfNm8c111zDs88+\nyy233AJwwcvDTp06lR07dmAymYiNjSUiIsLz2qFDhxgxYgQOh4OmTZsyceLEYj8j5cOLL9rYudNC\n374OOnbUDmwiIuVFoaG+du1ali1bRnx8PC6Xi7vuuuuC7nrftGkTe/fuZcmSJSQlJREbG8uSJUs8\nr0+fPp2HH36YqKgoJkyYwMGDB9m/f3+RnxHv++MPE7Nn2wgLczFhgnZgExEpTwodfg8LC+PRRx8l\nMTGRqVOn8tdff3HgwAEef/xxvv7662IPvGHDBrp06QJAgwYNOHHiBBkZGQC4XC62bt1KZGQkAPHx\n8dSqVavIz4j3GQaMHBlIdraJadNyCAnxdkUiInK281r7q1WrVkyfPp1169Zx66238u9//7vYz6Sk\npBBy1r/6oaGhJCcnA3Ds2DEqVarEtGnT6NevH3PmzCn2M+J9b78dwPr1VqKjHdx2m3ZgExEpb4q9\n+/1slStXpm/fvvTt2/eCv8gwjHw/HzlyhAEDBlC7dm0effRR1q5dW+RnChMSEoTVarngeooSFhZc\n/Jt8REm15eBBmDgRqlSB114LIDy87G+W9Jfz4i/tALWlPPKXdoDacjEuKNQvRHh4eL41448ePUpY\nWBgAISEh1KpVi7p16wLQpk0bfvvttyI/U5jjxzNLtO6wsGCSk9NL9JjeUpJteeSRQE6cCGDWrGxs\nNgdlPYDiL+fFX9oBakt55C/tALWluOMVptS23mjbti2JiYkA7Nq1i/DwcCpXrgyA1WqlTp067Nmz\nx/N6vXr1ivyMeM9nn1lZvjyANm3y6N9fSwSLiJRXpdZTb9GiBc2aNaNv376YTCbi4+NZunQpwcHB\nREVFERsby5gxYzAMg0aNGhEZGYnZbD7nM+JdaWnufdLtdoO5c7O1A5uISDlWaqEOMGrUqHyPr732\nWs/PV111FYsXLy72M+Jd48fbOXrUzDPP5NCgQfH3OIiIiPeo3yWF+uYbC+++a+O665wMHqwd2ERE\nyjuFuhQoM9M9J91sNnjuOe3AJiLiCxTqUqCZM+3s3Wtm0CAHN9ygHdhERHyBQl3O8cMPZl5+OYCr\nr3bx1FPagU1ExFco1CUfh8O9A5vL5d6BLSjI2xWJiMj5UqhLPi++aGPXLgv33ZdL+/bagU1ExJco\n1MXj99/dO7CFh7uIj9ewu4iIr1GoCwAuF4wYEUhOjnsHtssv93ZFIiJyoRTqAsCiRQF8/72VmBjt\nwCYi4qsU6sKhQyYmTrRTpYrB9OkadhcR8VWlukyslH+GAU8/bSc93cTcudnUrKmlYEVEfJV66hXc\np59aSUgIoG3bPO67TzuwiYj4MoV6BXb8uHsHtsBAgzlzsjGZvF2RiIhcCoV6BRYfH0hKipmnnsql\nfn0Nu4uI+DqFegW1dq2F994L4PrrnQwapB3YRET8gUK9Ajp1CkaNCsRiMZg3LxurbpcUEfELCvUK\naMYMO3/9ZWbw4Fyuv147sImI+AuFegWzfbuZV18NoF49F6NGadhdRMSfKNQrkNxcGD7cvQPb3LnZ\nXHaZtysSEZGSpFCvQP79bxu//GKhf/9c2rbVDmwiIv5GoV5B/PabmTlzbNSo4eLZZ7UUrIiIP1Ko\nVwDuHdjs5OaamDEjh6pVvV2RiIiUBoV6BfDmmwFs3GilZ08HMTHagU1ExF8p1P3cgQMmJk+2U7Wq\nwbRpGnYXEfFnWnbEj7l3YAskI8PEvHlZ1KihpWBFRPyZeup+bMkSWLnSSvv2efTrp2F3ERF/p1D3\nU8eOwdChcNllBrNnawc2EZGKQKHup559NpDkZBg9Ood69TTsLiJSESjU/dBXX1l4//0AbroJHnvM\n4e1yRESkjCjU/UxGBjz1lHsHtoUL0Q5sIiIViELdz0yfbmffPjNDhuRyww3erkZERMqSQt2PbN1q\nZsGCABo0cDFypHZgExGpaBTqfiI3F0aMCMQw3DuwBQZ6uyIRESlrCnU/MX++ewe2Bx7IpU0b7cAm\nIlIRKdT9wK+/mnnuORs1a7qIi9NSsCIiFZVC3ce5XPDkk4E4HCZmzsymShVvVyQiIt6iUPdxb7wR\nwJYtFu64w0F0tIbdRUQqMoW6D9u/370D2+WXG0yZomF3EZGKTkuT+CjDcC8yc+qUifnzswgP11Kw\nIiIVnXrqPmrpUitffmmlY8c87rlHO7CJiIhC3SelppoYN85OUJB2YBMRkTMU6j5o3Dg7qalmxozJ\n4aqrNOwuIiJuCnUf8+WXFj76KIAbb3TyyCPagU1ERM5QqPuQjAwYNSoQq9Vg7txsLBZvVyQiIuWJ\nQt2HTJ1q58ABM0OH5tKsmcvb5YiISDmjUPcRmzebWbgwgIYNnTz5pHZgExGRcynUfUBOjnspWMMw\nMWdODna7tysSEZHySKHuA55/3sbu3RYeeiiXW27RUrAiIlIwhXo599//mnn+eRu1arkYN05LwYqI\nSOFKdZnYqVOnsmPHDkwmE7GxsURERHhei4yMpGbNmlj+dwv37Nmz2bNnD8OGDaNhw4YANGrUiLi4\nuNIssVxzOs/egS2L4GBvVyQiIuVZqYX6pk2b2Lt3L0uWLCEpKYnY2FiWLFmS7z0LFiygUqVKnsd7\n9uyhdevWzJ8/v7TK8ikLFwawdauFu+5y0LWrht1FRKRopTb8vmHDBrp06QJAgwYNOHHiBBkZGaX1\ndX7nr79MTJ1qJyTEYPJkDbuLiEjxSi3UU1JSCAkJ8TwODQ0lOTk533vi4+Pp168fs2fPxjDcy53+\n/vvvPP744/Tr14/169eXVnnl2ukd2DIzTUyalE1YmJaCFRGR4pXZ1qunQ/u0oUOH0r59e6pWrcoT\nTzxBYmIiN954I0OGDKF79+7s27ePAQMGsHLlSmw2W6HHDQkJwmot2aXVwsK8e/F60SJYswa6dYPB\ngy+7pA1bvN2WkuQvbfGXdoDaUh75SztAbbkYpRbq4eHhpKSkeB4fPXqUsLAwz+M777zT83OHDh3Y\nvXs30dHRxMTEAFC3bl2qV6/OkSNHqFOnTqHfc/x4ZonWHRYWTHJyeoke80IkJ5sYNqwSQUEwZcop\nUlIuvpfu7baUJH9pi7+0A9SW8shf2gFqS3HHK0ypDb+3bduWxMREAHbt2kV4eDiVK1cGID09nYED\nB5Kb614ZbfPmzTRs2JBPP/2UhQsXApCcnExqaio1atQorRLLpbg4O8ePm4iNzaFuXQ27i4jI+Su1\nnnqLFi1o1qwZffv2xWQyER8fz9KlSwkODiYqKooOHTpwzz33YLfbadq0KdHR0Zw6dYpRo0bx5Zdf\n4nA4GD9+fJFD7/5m1SoLS5cGcNNNTgYO1A5sIiJyYUzG3y92+5iSHp7x1pBPejq0b1+J5GQTX36Z\nybXXXvqGLRq+Kn/8pR2gtpRH/tIOUFuKO15htKJcOTF5sp2DB907sJVEoIuISMWjUC8HNm608MYb\nNho1cjJ8uHZgExGRi6NQ97LsbBgxwo7JZPDcc9nagU1ERC6aQt3L5s2z8dtvFgYOdNCqlYbdRUTk\n4inUvWjXLjPz59uoXdtFbKyWghURkUujUPcSpxNGjAgkL8/E7NnZ/G8Kv4iIyEVTqHvJggUBbN9u\noXdvB507awc2ERG5dAp1L9i718T06XaqVXNpBzYRESkxZbahi7gZBowc6d6BbfbsbKpV8+m1f0RE\npBxRT72MLVli5ZtvrHTunEfv3nneLkdERPyIQr0MHT1q4tlnA6lUyWDWrOxL2lJVRETk7zT8Xoae\necZOWpqJadOyufJKDbuLiEjJUk+9jCQkWPjkkwBatnTy4IPagU1EREqeQr0MnDwJo0cHYrO5l4K1\nWLxdkYiI+COFehmYNMnO4cNmhg/PpXFjLQUrIiKlQ6FeyjZssPDWWzauvdbJ0KHagU1EREqPQr0U\nuXdgC8RkMpg7NxubzdsViYiIP1Ool6K5c20kJZl55BEHLVtq2F1EREqXQr2U/PSTmf/3/2zUqeNi\nzBgtBSsiIqVPoV4K8vLgySfdO7DNmqUd2EREpGwo1EvBK68EsGOHhT59HERGagc2EREpGwr1Evbn\nnyZmzrRTvbqLSZOyvV2OiIhUIFomtgQZBowaFUhWlol587IJDfV2RSIiUpGop16CFi+2sm6dlaio\nPO68UzuwiYhI2VKol5AjR0zExwdSubLBzJnagU1ERMqeht9LSGysnRMnTEyfnk3t2tqBTUREyp56\n6iVg+XIrn30WQOvWedqBTUREvEahfolOnICnn7b/bwe2HMz6jYqIiJcogi7RxIl2jhwxM2JELg0b\nailYERHxHoX6JVi/3sKiRTaaNHEyZIh2YBMREe9SqF+krCz3Dmxms8Fzz2kHNhER8T6F+kWaPdvG\nn3+6d2Br0ULD7iIi4n0K9Yvw449mXnzRRt262oFNRETKD4X6BTq9A5vTaWLOnGwqVfJ2RSIiIm4K\n9Qv00ks2du600Levg44dtQObiIiUHwr1C/DHHyZmzbJRvbqLCRO0A5uIiJQvWib2PBkGjBwZSHa2\niRdeyCYkxNsViYiI5Kee+nl6550A1q+3Eh3t4PbbtQObiIiUPwr183D4sInx4+0EBxvMmJGjHdhE\nRKRc0vD7eRgzxs7JkyZmzszmiiu0A5uIiJRP6qkX47PPrCxfHkCbNnkMGKAd2EREpPxSqBchLQ3G\njrVjtxvMnZutHdhERKRcU0wVYcIEO0ePmhk1KpcGDTTsLiIi5ZtCvRDr1ll45x0bzZo5GTxYO7CJ\niEj5p1AvQGZm/h3YAgK8XZGIiEjxFOoFmDXLzt69Zh5/3EHz5tqBTUREfINC/W+2boWXXgrgqqtc\njB6tHdhERMR3KNTP4nDAwIHgcrl3YAsK8nZFIiIi50+hfpZXXw1gxw64995cOnTQDmwiIuJbSnVF\nualTp7Jjxw5MJhOxsbFERER4XouMjKRmzZpYLBYAZs+eTY0aNYr8TGn7808z9erB+PEadhcREd9T\naqG+adMm9u7dy5IlS0hKSiI2NpYlS5bke8+CBQuoVKnSBX2mNM2alUNoqI3jx8vsK0VEREpMqQ2/\nb9iwgS5dugDQoEEDTpw4QUZGRol/piSZTGDVavgiIuKjSi3UU1JSCDlr0/HQ0FCSk5PzvSc+Pp5+\n/foxe/ZsDMM4r8+IiIhIwcqsX2oY+ZdZHTp0KO3bt6dq1ao88cQTJCYmFvuZgoSEBGG1WkqsToCw\nsOASPZ43qS3lj7+0A9SW8shf2gFqy8UotVAPDw8nJSXF8/jo0aOEhYV5Ht95552enzt06MDu3buL\n/UxBjh/PLMGq3b/45OT0Ej2mt6gt5Y+/tAPUlvLIX9oBaktxxytMqQ2/t23b1tP73rVrF+Hh4VSu\nXBmA9PR0Bg4cSG6ue031zZs307BhwyI/IyIiIkUrtZ56ixYtaNasGX379sVkMhEfH8/SpUsJDg4m\nKiqKDh06cM8992C322natCnR0dGYTKZzPiMiIiLnx2Scz4Xrcqykh2c05FM++Utb/KUdoLaUR/7S\nDlBbijteYbSinIiIiJ9QqIuIiPgJhbqIiIifUKiLiIj4CYW6iIiIn/D5u99FRETETT11ERERP6FQ\nFxER8RMKdRERET+hUBcREfETCnURERE/oVAXERHxE6W2S5sv2L17N4MHD+bBBx/k/vvvz/fad999\nx9y5c7FYLHTo0IEnnnjCS1Wen6LaEhkZSc2aNbFYLADMnj2bGjVqeKPMYs2cOZOtW7eSl5fHY489\nRteuXT2v+do5KaotvnJOsrKyGDNmDKmpqeTk5DB48GA6derked2XzklxbfGVc3K27OxsevbsyeDB\ng+nVq5fneV86L1B4O3zpnGzcuJFhw4bRsGFDABo1akRcXJzn9TI7J0YFderUKeP+++83xo0bZyxa\ntOic17t3724cPHjQcDqdRr9+/YzffvvNC1Wen+La0qlTJyMjI8MLlV2YDRs2GP/85z8NwzCMY8eO\nGR07dsz3ui+dk+La4ivn5IsvvjBeffVVwzAMY//+/UbXrl3zve5L56S4tvjKOTnb3LlzjV69ehkf\nffRRvud96bwYRuHt8KVz8v333xv/+te/Cn29rM5JhR1+t9lsLFiwgPDw8HNe27dvH1WrVuWKK67A\nbDbTsWNHNmzY4IUqz09RbfElrVq14vnnnwegSpUqZGVl4XQ6Ad87J0W1xZfExMTwyCOPAHDo0KF8\nvSRfOydFtcUXJSUl8fvvv3Prrbfme97Xzkth7fAnZXlOKuzwu9VqxWotuPnJycmEhoZ6HoeGhrJv\n376yKu2CFdWW0+Lj4zlw4AA33XQTI0eOxGQylVF1589isRAUFATAhx9+SIcOHTzDbr52Topqy2m+\ncE5O69u3L4cPH+bll1/2POdr5+S0gtpymi+dkxkzZhAXF8fHH3+c73lfOy+FteM0Xzonv//+O48/\n/jgnTpxgyJAhtG3bFijbc1JhQ70iGTp0KO3bt6dq1ao88cQTJCYmEh0d7e2yCrV69Wo+/PBDXn/9\ndW+XcskKa4uvnZP33nuPX375haeeeopPP/20XP/DWpzC2uJL5+Tjjz+mefPm1KlTx9ulXJLi2uFL\n5+Tqq69myJAhdO/enX379jFgwABWrlyJzWYr0zoU6gUIDw8nJSXF8/jIkSM+PbR95513en7u0KED\nu3fvLrf/Yaxbt46XX36Z1157jeDgYM/zvnhOCmsL+M45+emnn6hWrRpXXHEFTZo0wel0cuzYMapV\nq+Zz56SotoDvnBOAtWvXsm/fPtauXcvhw4ex2WzUrFmTf/zjHz51XopqB/jWOalRowYxMTEA1K1b\nl+rVq3PkyBHq1KlTpuekwl5TL8qVV15JRkYG+/fvJy8vjzVr1niGUXxNeno6AwcOJDc3F4DNmzd7\n7s4sb9LT05k5cyavvPIKl19+eb7XfO2cFNUWXzonW7Zs8YwypKSkkJmZSUhICOB756SotvjSOQGY\nN28eH330Ee+//z59+vRh8ODBniD0pfNSVDt87Zx8+umnLFy4EHAPt6empnru2yjLc1Jhd2n76aef\nmDFjBgcOHMBqtVKjRg0iIyO58soriYqKYvPmzcyePRuArl27MnDgQC9XXLji2vLWW2/x8ccfY7fb\nadq0KXFxceVy+HTJkiW88MIL1KtXz/PczTffTOPGjX3unBTXFl85J9nZ2TzzzDMcOnSI7OxshgwZ\nQlpaGsHBwT53Topri6+ck7974YUXqF27NoBPnpfTCmqHL52TjIwMRo0axcmTJ3E4HAwZMoTU1NQy\nPycVNtRFRET8jYbfRURE/IRCXURExE8o1EVERPyEQl1ERMRPKNRFRET8hBafEamA9u/fT3R0NDfe\neGO+5zt27Mg///nPSz7+xo0bmTdvHosXL77kY4nI+VOoi1RQoaGhLFq0yNtliEgJUqiLSD5NmzZl\n8ODBbNy4kVOnTjF9+nQaNWrEjh07mD59OlarFZPJxLPPPss111zDnj17iIuLw+VyYbfbmTZtGgAu\nl4v4+Hh++eUXbDYbr7zyCgAjR47k5MmT5OXl0alTJwYNGuTN5or4FV1TF5F8nE4nDRs2ZNGiRfTr\n14/58+cDMHr0aMaOHcuiRYt46KGHmDBhAuDeRWvgwIG888479O7dmxUrVgDuLTX/9a9/8f7772O1\nWvn222/57rvvyMvL49133+W9994jKCgIl8vltbaK+Bv11EUqqGPHjtG/f/98zz311FMAtGvXDoAW\nLVqwcOFCTp48SWpqKhEREQC0bt2aESNGAPDjjz/SunVrAHr06AG4r6nXr1+f6tWrA1CzZk1OnjxJ\nZGQk8+fPZ9iwYXTs2JE+ffpgNqtvIVJSFOoiFVRR19TPXj3aZDKds97231eXLqi3/ff94wGqVavG\nJ598wvbt2/nyyy/p3bs3y5YtIzAw8GKaICJ/oz+RReQc33//PQBbt26lcePGBAcHExYWxo4dOwDY\nsGEDzZs3B9y9+XXr1gGwfPly5s6dW+hxv/32W9auXctNN93E6NGjCQoKIjU1tZRbI1JxqKcuUkEV\nNPx+5ZVXAvDzzz+zePFiTpw4wYwZMwCYMWMG06dPx2KxYDabGT9+PABxcXHExcXx7rvvYrVamTp1\nKn/99VeB31mvXj3GjFxw0MoAAABhSURBVBnDa6+9hsVioV27dp6duUTk0mmXNhHJp3HjxuzatQur\nVX/zi/gaDb+LiIj4CfXURURE/IR66iIiIn5CoS4iIuInFOoiIiJ+QqEuIiLiJxTqIiIifkKhLiIi\n4if+P7u1Y+9KlD5iAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LXiAAsLRLa4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "BOmZ2MPTLbOF",
        "colab_type": "code",
        "outputId": "dedc8562-75cf-48c5-8e06-b8f30e9ce45e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13, 250, 4, 94, 35, 72, 21, 2555, 12, 5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "2LkONUKQkSrL",
        "colab_type": "code",
        "outputId": "bfd9deb7-8288-47b8-8d43-6d37917456c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_sentences_X, y_test, batch_size=100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55902/55902 [==============================] - 57s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ab0ZL1dqkTY4",
        "colab_type": "code",
        "outputId": "fba8d17b-16f5-40ba-e342-eaf91a9dd190",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(score)\n",
        "print(\"Accuracy:\", score[1]*100)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.1247087091539103, 0.6836785816821312]\n",
            "Accuracy: 68.36785816821312\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XHwoVCEwjEz7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons are？"
      ]
    },
    {
      "metadata": {
        "id": "hKHbOs4WkFaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "As the dataset is highly imbalanced, we can simply weight up the minority classes proportionally to their underrepresentation while training. "
      ]
    },
    {
      "metadata": {
        "id": "6L4kNdf6kGEa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "y_integers = np.argmax(tags_encoding, axis=1)\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "d_class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x4KljmlUm070",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#f_optimizer = 'adam'\n",
        "#f_loss='categorical_crossentropy'\n",
        "\n",
        "#model.compile(optimizer = f_optimizer, loss=f_loss, metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xB2McUREkL4B",
        "colab_type": "code",
        "outputId": "acf21120-39f1-4f6d-dd18-29f4f219d2dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_sentences_X, y_train, batch_size=100, epochs=5, class_weight = d_class_weights)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "167704/167704 [==============================] - 699s 4ms/step - loss: 5.2486 - acc: 0.3708\n",
            "Epoch 2/5\n",
            "167704/167704 [==============================] - 697s 4ms/step - loss: 4.1394 - acc: 0.3598\n",
            "Epoch 3/5\n",
            "167704/167704 [==============================] - 696s 4ms/step - loss: 3.0942 - acc: 0.3095\n",
            "Epoch 4/5\n",
            "167704/167704 [==============================] - 695s 4ms/step - loss: 2.1793 - acc: 0.3320\n",
            "Epoch 5/5\n",
            "167704/167704 [==============================] - 698s 4ms/step - loss: 1.4634 - acc: 0.3068\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb3afa2978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "la-J77AE4nwg",
        "colab_type": "code",
        "outputId": "b76dc47c-ca4d-4a8d-b90e-a575c2b3580e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import confusion_matrix\n",
        "\n",
        "len(sentences)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "223606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "DH99Dp39ZzcR",
        "colab_type": "code",
        "outputId": "213215d5-8c5d-4e2f-a520-e6acb2a190e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1992
        }
      },
      "cell_type": "code",
      "source": [
        "#print(class_names)\n",
        "reduced_df"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>act_tag</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>o</td>\n",
              "      <td>Okay, {F uh. } /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>qy</td>\n",
              "      <td>Do you have annual family reunions  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>%</td>\n",
              "      <td>{C or, } -/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ny</td>\n",
              "      <td>{F Uh, } yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>%</td>\n",
              "      <td>{F uh, }  our, - /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>%</td>\n",
              "      <td>the - /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>sd^e</td>\n",
              "      <td>my mother's side of the family  [ is quite lar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>sd</td>\n",
              "      <td>{C and, }  {F uh, } &lt;throat_clearing&gt; they, {F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>sd</td>\n",
              "      <td>back,  {F uh, } early on they used [ to, +  to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>b</td>\n",
              "      <td>Uh-huh. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>+</td>\n",
              "      <td>in the family.  &lt;sigh&gt;  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>%</td>\n",
              "      <td>{C And, } -/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>qy^d</td>\n",
              "      <td>All local.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>qy</td>\n",
              "      <td>Were they {D like } all in Dallas? /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>nn</td>\n",
              "      <td>{F Uh, } [ [ [ no, + no ] + not, ] + no, ]  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>sd^e</td>\n",
              "      <td>this was,  {D well, } I am actually from Missi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>%</td>\n",
              "      <td>{C and,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bk</td>\n",
              "      <td>Oh. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>+</td>\n",
              "      <td>{C so }  that's, - /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>sd</td>\n",
              "      <td>[ it,  + it ] was around there generally. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>b</td>\n",
              "      <td>Uh-huh,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>b^r</td>\n",
              "      <td>uh-huh. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>sd</td>\n",
              "      <td>{F Uh, } that's where the family --</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>qy^d</td>\n",
              "      <td>Just from city to city then  ((   )) . /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>+</td>\n",
              "      <td>-- grew.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>ny</td>\n",
              "      <td>Yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>sd^e</td>\n",
              "      <td>{C and } it was not only in Mississippi, in so...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>b</td>\n",
              "      <td>Uh-huh,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>b^r</td>\n",
              "      <td>uh-huh. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>+</td>\n",
              "      <td>as well,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223576</th>\n",
              "      <td>+</td>\n",
              "      <td>That [ it could be, + it could be ] added to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223577</th>\n",
              "      <td>sd</td>\n",
              "      <td>this is the way I feel about this,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223578</th>\n",
              "      <td>sd</td>\n",
              "      <td>{C and } this is the way, because [ [ I, + it'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223579</th>\n",
              "      <td>sv</td>\n",
              "      <td>[ [ [ when you, + when you, ] + if you, ] + if...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223580</th>\n",
              "      <td>b</td>\n",
              "      <td>Uh-huh. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223581</th>\n",
              "      <td>sv</td>\n",
              "      <td>{C But } if no one does anything when they're ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223582</th>\n",
              "      <td>aa</td>\n",
              "      <td>Yes.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223583</th>\n",
              "      <td>sd</td>\n",
              "      <td>I'm saying this as I'm trying to keep my nine ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223584</th>\n",
              "      <td>b</td>\n",
              "      <td>&lt;laughter&gt;  Yes,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223585</th>\n",
              "      <td>ba</td>\n",
              "      <td>I understand,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223586</th>\n",
              "      <td>sd</td>\n",
              "      <td>I wait until I put mine in bed before I make m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223587</th>\n",
              "      <td>b</td>\n",
              "      <td>&lt;laughter&gt;  Yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223588</th>\n",
              "      <td>sd</td>\n",
              "      <td>{D well, } she's usually in bed by this time,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223589</th>\n",
              "      <td>sd</td>\n",
              "      <td>{C but } she's staying up late tonight.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223590</th>\n",
              "      <td>%</td>\n",
              "      <td>{C But, } - /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223591</th>\n",
              "      <td>%</td>\n",
              "      <td>yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223592</th>\n",
              "      <td>sd</td>\n",
              "      <td>I want to have a relationship with her.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223593</th>\n",
              "      <td>sd</td>\n",
              "      <td>{D You know, } my dad was a very traditional d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223594</th>\n",
              "      <td>sd</td>\n",
              "      <td>{C and } when I was a child I didn't really kn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223595</th>\n",
              "      <td>sd</td>\n",
              "      <td>{C and } I miss that.   /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223596</th>\n",
              "      <td>%</td>\n",
              "      <td>{C And } I want, -/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223597</th>\n",
              "      <td>aa</td>\n",
              "      <td>That's the thing,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223598</th>\n",
              "      <td>sv</td>\n",
              "      <td>[ if woman's role, + like we said, if women's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223599</th>\n",
              "      <td>aa</td>\n",
              "      <td>Yeah,  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223600</th>\n",
              "      <td>sv</td>\n",
              "      <td>{C but } men have to be convinced of that. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223601</th>\n",
              "      <td>sv</td>\n",
              "      <td>{C And } a lot of it's for the better.  /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223602</th>\n",
              "      <td>sv</td>\n",
              "      <td>[ Some of the, + some of the ] women's roles, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223603</th>\n",
              "      <td>sv</td>\n",
              "      <td>{C but } I think [ if we can, + if we can ] ex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223604</th>\n",
              "      <td>aa</td>\n",
              "      <td>No. /</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223605</th>\n",
              "      <td>sd</td>\n",
              "      <td>-- I know mine almost never did ... /</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>223606 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       act_tag                                               text\n",
              "0            o                                   Okay, {F uh. } /\n",
              "1           qy              Do you have annual family reunions  /\n",
              "2            %                                        {C or, } -/\n",
              "3           ny                                  {F Uh, } yeah,  /\n",
              "4            %                                 {F uh, }  our, - /\n",
              "5            %                                            the - /\n",
              "6         sd^e  my mother's side of the family  [ is quite lar...\n",
              "7           sd  {C and, }  {F uh, } <throat_clearing> they, {F...\n",
              "8           sd  back,  {F uh, } early on they used [ to, +  to...\n",
              "9            b                                          Uh-huh. /\n",
              "10           +                          in the family.  <sigh>  /\n",
              "11           %                                       {C And, } -/\n",
              "12        qy^d                                      All local.  /\n",
              "13          qy               Were they {D like } all in Dallas? /\n",
              "14          nn      {F Uh, } [ [ [ no, + no ] + not, ] + no, ]  /\n",
              "15        sd^e  this was,  {D well, } I am actually from Missi...\n",
              "16           %                                            {C and,\n",
              "17          bk                                              Oh. /\n",
              "18           +                               {C so }  that's, - /\n",
              "19          sd        [ it,  + it ] was around there generally. /\n",
              "20           b                                         Uh-huh,  /\n",
              "21         b^r                                          uh-huh. /\n",
              "22          sd                {F Uh, } that's where the family --\n",
              "23        qy^d           Just from city to city then  ((   )) . /\n",
              "24           +                                        -- grew.  /\n",
              "25          ny                                           Yeah,  /\n",
              "26        sd^e  {C and } it was not only in Mississippi, in so...\n",
              "27           b                                         Uh-huh,  /\n",
              "28         b^r                                          uh-huh. /\n",
              "29           +                                        as well,  /\n",
              "...        ...                                                ...\n",
              "223576       +  That [ it could be, + it could be ] added to a...\n",
              "223577      sd              this is the way I feel about this,  /\n",
              "223578      sd  {C and } this is the way, because [ [ I, + it'...\n",
              "223579      sv  [ [ [ when you, + when you, ] + if you, ] + if...\n",
              "223580       b                                          Uh-huh. /\n",
              "223581      sv  {C But } if no one does anything when they're ...\n",
              "223582      aa                                            Yes.  /\n",
              "223583      sd  I'm saying this as I'm trying to keep my nine ...\n",
              "223584       b                                <laughter>  Yes,  /\n",
              "223585      ba                                   I understand,  /\n",
              "223586      sd  I wait until I put mine in bed before I make m...\n",
              "223587       b                               <laughter>  Yeah,  /\n",
              "223588      sd   {D well, } she's usually in bed by this time,  /\n",
              "223589      sd         {C but } she's staying up late tonight.  /\n",
              "223590       %                                      {C But, } - /\n",
              "223591       %                                           yeah,  /\n",
              "223592      sd         I want to have a relationship with her.  /\n",
              "223593      sd  {D You know, } my dad was a very traditional d...\n",
              "223594      sd  {C and } when I was a child I didn't really kn...\n",
              "223595      sd                          {C and } I miss that.   /\n",
              "223596       %                                {C And } I want, -/\n",
              "223597      aa                               That's the thing,  /\n",
              "223598      sv  [ if woman's role, + like we said, if women's ...\n",
              "223599      aa                                           Yeah,  /\n",
              "223600      sv       {C but } men have to be convinced of that. /\n",
              "223601      sv          {C And } a lot of it's for the better.  /\n",
              "223602      sv  [ Some of the, + some of the ] women's roles, ...\n",
              "223603      sv  {C but } I think [ if we can, + if we can ] ex...\n",
              "223604      aa                                              No. /\n",
              "223605      sd              -- I know mine almost never did ... /\n",
              "\n",
              "[223606 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "fM7VWweco0Et",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
      ]
    },
    {
      "metadata": {
        "id": "fW4g5mQkkaFv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Can we improve things by using context information?  Next we try to build a model which predicts DA tag from the sequence of \n",
        "previous DA tags, plus the utterance representation. "
      ]
    },
    {
      "metadata": {
        "id": "WfrGWuZ6nk4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Using Context for Dialog Act Classification\n",
        "We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the sentence CNN representation. The CNN model learns textual information in each sentence for DA classification. Here, we use bidirectional-LSTM (BLSTM) to learn the context before and after the current sentence. The left-to-right LSTM output and the one from the reverse direction are concatenated and input to a hidden layer for classification."
      ]
    },
    {
      "metadata": {
        "id": "Sz693CIUvcca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Functions for creating weights and biases."
      ]
    },
    {
      "metadata": {
        "id": "9hMj-KaKvfHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def weights_init(shape):\n",
        "    return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oitwAO5ivkbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bias_init(shape):\n",
        "    return tf.Variable(tf.zeros(shape=shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DuJLqgjWqcIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " This is classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN.\n",
        "    \n",
        "      Input(s): \n",
        "              input - word_embedings\n",
        "              filter_size - size of width and height of the Conv kernel\n",
        "              number_of_channels - in this case it is always 1\n",
        "              number_of_filters - how many representation of the input utterance are we going to output from this layer \n",
        "              strides - how many does kernel move to the side and up/down\n",
        "              activation - a activation function\n",
        "              max_pool - boolean value which will trigger a max_pool operation on the output tensor\n",
        "      Output(s): \n",
        "               text_conv layer\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "UuaiZkOjZGx1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_conv(input, filter_size, number_of_channels, number_of_filters, strides=(1, 1), activation=tf.nn.relu, max_pool=True):\n",
        "    \n",
        "    weights = weights_init([filter_size, filter_size, number_of_channels, number_of_filters])\n",
        "    bias = bias_init([number_of_filters])\n",
        "    \n",
        "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding='SAME')\n",
        "    \n",
        "    if activation != None:\n",
        "        layer = activation(layer)\n",
        "    \n",
        "    if max_pool:\n",
        "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2 ,1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mU0Xa3QOqwS3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    This method is used to create LSTM layer. And the data we’re working with has temporal properties which we want to model as well — hence the use of a LSTM. You can create a BiLSTM by modifying this.\n",
        "    \n",
        "    Input(s): lstm_cell_unitis - used to define the number of units in a LSTM layer\n",
        "              number_of_layers - used to define how many of LSTM layers do we want in the network\n",
        "              batch_size - in this method this information is used to build starting state for the network\n",
        "              dropout_rate - used to define how many cells in a layer do we want to 'turn off'\n",
        "              \n",
        "    Output(s): cell - lstm layer\n",
        "               init_state - zero vectors used as a starting state for the network"
      ]
    },
    {
      "metadata": {
        "id": "QaxUrmPIZI7S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lstm_layer(lstm_size, number_of_layers, batch_size, dropout_rate):\n",
        "\n",
        "    def cell(size, dropout_rate=None):\n",
        "        layer = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
        "        \n",
        "        return tf.contrib.rnn.DropoutWrapper(layer, output_keep_prob=dropout_rate)\n",
        "            \n",
        "    cell = tf.contrib.rnn.MultiRNNCell([cell(lstm_size, dropout_rate) for _ in range(number_of_layers)])\n",
        "    \n",
        "    init_state = cell.zero_state(batch_size, tf.float32)\n",
        "    return cell, init_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxlRv9dDrcy5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    Use to transform/reshape conv output to 2d matrix, if it's necessary\n",
        "    \n",
        "    Input(s): Layer - text_cnn layer\n",
        "              batch_size - how many samples do we feed at once\n",
        "              seq_len - number of time steps\n",
        "              \n",
        "    Output(s): reshaped_layer - the layer with new shape\n",
        "               number_of_elements - this param is used as a in_size for next layer"
      ]
    },
    {
      "metadata": {
        "id": "hYxHsBvwZRA4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def flatten(layer, batch_size, seq_len):\n",
        "\n",
        "    dims = layer.get_shape()\n",
        "    number_of_elements = dims[2:].num_elements()\n",
        "    \n",
        "    reshaped_layer = tf.reshape(layer, [batch_size, int(seq_len/2), number_of_elements])\n",
        "    return reshaped_layer, number_of_elements"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PMOjnT8Drmpa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    Output layer for the lstm netowrk\n",
        "    \n",
        "    Input(s): lstm_outputs - outputs from the RNN part of the network\n",
        "              input_size - in this case it is RNN size (number of neuros in RNN layer)\n",
        "              output_size - number of neuros for the output layer == number of classes\n",
        "              \n",
        "    Output(s) - logits, "
      ]
    },
    {
      "metadata": {
        "id": "d84nFOulZkhP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dense_layer(input, in_size, out_size, dropout=False, activation=tf.nn.relu):\n",
        "  \n",
        "    weights = weights_init([in_size, out_size])\n",
        "    bias = bias_init([out_size])\n",
        "    \n",
        "    layer = tf.matmul(input, weights) + bias\n",
        "    \n",
        "    if activation != None:\n",
        "        layer = activation(layer)\n",
        "    \n",
        "    if dropout:\n",
        "        layer = tf.nn.dropout(layer, 0.5)\n",
        "        \n",
        "    return layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VX3KofjJruhZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "    Function used to calculate loss and minimize it\n",
        "    \n",
        "    Input(s): rnn_out - logits from the fully_connected layer\n",
        "              targets - targets used to train network\n",
        "              learning_rate/step_size\n",
        "    \n",
        "    \n",
        "    Output(s): optimizer - optimizer of choice\n",
        "               loss - calculated loss function"
      ]
    },
    {
      "metadata": {
        "id": "kNu0_J2VZlKB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_optimizer(logits, targets, learning_rate, ):\n",
        "\n",
        "    #loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "    return loss, optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s3Acv6U_r1rG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To create the model you can use these inputs:     \n",
        "       \n",
        "       Input(s): learning_rate/step_size - how fast are we going to find global minima\n",
        "                  batch_size -  the nuber of samples to feed at once\n",
        "                  seq_len - the number of timesteps in unrolled RNN\n",
        "                  vocab_size - the number of nunique words in the vocab\n",
        "                  embed_size - length of word embed vectors\n",
        "                  conv_filters - number of filters in output tensor from CNN layer\n",
        "                  conv_filter_size - height and width of conv kernel\n",
        "                  number_of_lstm_layers - the number of layers used in the LSTM part of the network\n",
        "                  lstm_units - the number of neurons/cells in a LSTM layer"
      ]
    },
    {
      "metadata": {
        "id": "Rapk0invUdyS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class DATagging(object):\n",
        "    \n",
        "    def __init__(self, learning_rate=0.001, batch_size=256, seq_len=250, vocab_size=10000, embed_size=300,\n",
        "                conv_filters=32, conv_filter_size=5, number_of_lstm_layers=1, lstm_units=128):\n",
        "      \n",
        "        #Clear graph\n",
        "        tf.reset_default_graph()\n",
        "        \n",
        "        # Set up inputs - placeholders for values to be filled later\n",
        "        # Note: Shape of targets and logits needs to be the same\n",
        "        self.inputs = tf.placeholder(tf.int32, [batch_size, seq_len], name='inputs')\n",
        "#        self.targets = tf.placeholder(tf.float32, [batch_size, 1], name='targets')\n",
        "        self.targets = tf.placeholder(tf.float32, [batch_size, 303], name='targets')\n",
        "        self.keep_probs = tf.placeholder(tf.float32, name='keep_probs')\n",
        "        \n",
        "        \n",
        "        \n",
        "        #self.inputs, self.targets, self.keep_probs = define_inputs(batch_size, seq_len)\n",
        "        \n",
        "        # Embedding layer\n",
        "        # Start with vocab size as input shape\n",
        "        # Note the need to reshape by adding a dimension of 1 at the end before\n",
        "        # passing the output to the convolutional layer which is conv2d \n",
        "        # c.f. adding a channel in image processing with conv2d\n",
        "        sentence_embeddings = tf.Variable(tf.random_uniform([vocab_size,embed_size]))\n",
        "        embedding_layer = tf.nn.embedding_lookup(sentence_embeddings, self.inputs)\n",
        "        embedding_layer=tf.expand_dims(embedding_layer, -1)\n",
        "\n",
        "        \n",
        "        # Building the network\n",
        "        # Convolutional layers are good at picking up context (because of their\n",
        "        # use of a kernel / 'filter')\n",
        "        conv_layer = text_conv(embedding_layer, conv_filter_size, 1, conv_filters)\n",
        "        \n",
        "        # Flatten the conv2d output before passing to the lstm (i.e. an rnn) layer\n",
        "        conv_flatten, num_elements = flatten(conv_layer, batch_size, seq_len)\n",
        "        \n",
        "        # LSTM layer to process the output from the convolutional layer 'as normal'\n",
        "        cell, init_state = lstm_layer(lstm_units, number_of_lstm_layers, batch_size, self.keep_probs)\n",
        "        \n",
        "        outputs, states = tf.nn.dynamic_rnn(cell, conv_flatten, initial_state=init_state)\n",
        "        \n",
        "        review_outputs = outputs[:, -1, :]   #slicing - need to use tf\n",
        "        #review_outputs = tf.slice(outputs,[:, -1, :])\n",
        "        #review_outputs =tf.expand_dims(outputs,-1)\n",
        "        #new_outputs=tf.reshape(outputs,[-2])\n",
        "        #print('got up to here')\n",
        "        #print(outputs.shape)\n",
        "        #print(review_outputs.shape)\n",
        "        #print(new_outputs.shape)\n",
        "        #logits = dense_layer(review_outputs, lstm_units, 1, activation=None)\n",
        "        \n",
        "        # Fully connected dense layer to produce the logits (hence no activation)\n",
        "        #  - again relatively 'standard'\n",
        "        logits = dense_layer(review_outputs, lstm_units, 303, activation=None)\n",
        "        \n",
        "        self.loss, self.opt = loss_optimizer(logits, self.targets, learning_rate)\n",
        "        \n",
        "        preds = tf.nn.sigmoid(logits)\n",
        "        currect_pred = tf.equal(tf.cast(tf.round(preds), tf.int32), tf.cast(self.targets, tf.int32))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(currect_pred, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3bVXv36axfu",
        "colab_type": "code",
        "outputId": "513b9beb-9718-4bfa-e387-53f90cca6b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "cell_type": "code",
      "source": [
        "model = DATagging(learning_rate=0.001, \n",
        "                     batch_size=256, \n",
        "                     seq_len=250,\n",
        "                     vocab_size=len(wordvectors) + 1, \n",
        "                     embed_size=300,\n",
        "                     conv_filters=32, \n",
        "                     conv_filter_size=5, \n",
        "                     number_of_lstm_layers=1, \n",
        "                     lstm_units=128\n",
        "                 )\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-43-902c7c1092d4>:4: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-43-902c7c1092d4>:8: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-47-5be3d154f00d>:42: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-46-e32b9a92a8d0>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d38A1d4VuyvC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEltPhMmf-WK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ugyBjmdJgDw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "batch_size = 256\n",
        "drop_rate = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UHIKriE5nHUA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "X_train= pad_sequences(X_train, maxlen=250, padding='post')\n",
        "#X_test = pad_sequences(X_test, maxlen=seq_len padding='post')\n",
        "\n",
        "y_train = pad_sequences(y_train, maxlen=250, padding='post')\n",
        "#y_test = pad_sequences(y_test, maxlen=MAX_LENGTH, padding='post')\n",
        "'''\n",
        "train_sentences_X = pad_sequences(X_train, maxlen=250, padding='post')\n",
        "test_sentences_X = pad_sequences(X_test, maxlen=250, padding='post')\n",
        "\n",
        "# train_sentences_y = pad_sequences(y_train, maxlen=250, padding='post')\n",
        "# test_sentences_y = pad_sequences(y_test, maxlen=250, padding='post')\n",
        "#X_train = features[:6400]\n",
        "#y_train = labels[:6400]\n",
        "\n",
        "#X_test = features[6400:]\n",
        "#y_test = labels[6400:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZXzuhKv5gMvH",
        "colab_type": "code",
        "outputId": "32269923-e996-4fdb-fd5f-98d70415da8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57533
        }
      },
      "cell_type": "code",
      "source": [
        "#Training\n",
        "from tqdm import tqdm_notebook as tqdm #Shows output in notebook while running\n",
        "\n",
        "for i in range(epochs):\n",
        "    epoch_loss = []\n",
        "    train_accuracy = []\n",
        "    for ii in tqdm(range(0, len(X_train) - len(X_train) % batch_size, batch_size)):\n",
        "        #X_batch = X_train[ii:ii+batch_size]\n",
        "        X_batch = train_sentences_X[ii:ii+batch_size] \n",
        "        y_batch = y_train[ii:ii+batch_size] \n",
        "\n",
        "        c, _, a = session.run([model.loss, model.opt, model.accuracy], feed_dict={model.inputs:X_batch,         # or train_sentences_X, \n",
        "                                                                                  model.targets:y_batch,       #or y_train?\n",
        "                                                                                  model.keep_probs:drop_rate}) \n",
        "        epoch_loss.append(c)\n",
        "        train_accuracy.append(a)\n",
        "    \n",
        "        \n",
        "        print(\"Epoch: {}/{}\".format(i+1, epochs), \" | Epoch loss: {}\".format(np.mean(epoch_loss)),\n",
        "            \" | Mean train accuracy: {}\".format(np.mean(train_accuracy)))\n",
        "        \n",
        "print(\"Training finished\")  \n",
        "#session.close()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57c2eb0ec9a64b6a8ea2c87353a7f034",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=655), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5  | Epoch loss: 5.6850080490112305  | Mean train accuracy: 0.5035581588745117\n",
            "Epoch: 1/5  | Epoch loss: 5.483322620391846  | Mean train accuracy: 0.5176296830177307\n",
            "Epoch: 1/5  | Epoch loss: 5.335533142089844  | Mean train accuracy: 0.5340862274169922\n",
            "Epoch: 1/5  | Epoch loss: 5.208924770355225  | Mean train accuracy: 0.5529696941375732\n",
            "Epoch: 1/5  | Epoch loss: 5.093967437744141  | Mean train accuracy: 0.5734659433364868\n",
            "Epoch: 1/5  | Epoch loss: 4.973893642425537  | Mean train accuracy: 0.5937178134918213\n",
            "Epoch: 1/5  | Epoch loss: 4.843373775482178  | Mean train accuracy: 0.6138300895690918\n",
            "Epoch: 1/5  | Epoch loss: 4.726916790008545  | Mean train accuracy: 0.633389413356781\n",
            "Epoch: 1/5  | Epoch loss: 4.635969638824463  | Mean train accuracy: 0.6524526476860046\n",
            "Epoch: 1/5  | Epoch loss: 4.516843318939209  | Mean train accuracy: 0.6703022122383118\n",
            "Epoch: 1/5  | Epoch loss: 4.430835247039795  | Mean train accuracy: 0.6868448853492737\n",
            "Epoch: 1/5  | Epoch loss: 4.348265647888184  | Mean train accuracy: 0.7021613121032715\n",
            "Epoch: 1/5  | Epoch loss: 4.269095420837402  | Mean train accuracy: 0.7162489295005798\n",
            "Epoch: 1/5  | Epoch loss: 4.202867031097412  | Mean train accuracy: 0.7292090058326721\n",
            "Epoch: 1/5  | Epoch loss: 4.12159538269043  | Mean train accuracy: 0.7408698797225952\n",
            "Epoch: 1/5  | Epoch loss: 4.049805641174316  | Mean train accuracy: 0.7515679597854614\n",
            "Epoch: 1/5  | Epoch loss: 3.9890127182006836  | Mean train accuracy: 0.7612614631652832\n",
            "Epoch: 1/5  | Epoch loss: 3.9248836040496826  | Mean train accuracy: 0.7700117826461792\n",
            "Epoch: 1/5  | Epoch loss: 3.8837273120880127  | Mean train accuracy: 0.7779421806335449\n",
            "Epoch: 1/5  | Epoch loss: 3.8193938732147217  | Mean train accuracy: 0.7851600646972656\n",
            "Epoch: 1/5  | Epoch loss: 3.7809107303619385  | Mean train accuracy: 0.7917261719703674\n",
            "Epoch: 1/5  | Epoch loss: 3.7449028491973877  | Mean train accuracy: 0.7977409958839417\n",
            "Epoch: 1/5  | Epoch loss: 3.7075915336608887  | Mean train accuracy: 0.8032637238502502\n",
            "Epoch: 1/5  | Epoch loss: 3.6676673889160156  | Mean train accuracy: 0.80832839012146\n",
            "Epoch: 1/5  | Epoch loss: 3.6326730251312256  | Mean train accuracy: 0.8129775524139404\n",
            "Epoch: 1/5  | Epoch loss: 3.6036298274993896  | Mean train accuracy: 0.8172814249992371\n",
            "Epoch: 1/5  | Epoch loss: 3.5761539936065674  | Mean train accuracy: 0.8212450742721558\n",
            "Epoch: 1/5  | Epoch loss: 3.5499234199523926  | Mean train accuracy: 0.8249080777168274\n",
            "Epoch: 1/5  | Epoch loss: 3.52378511428833  | Mean train accuracy: 0.828288197517395\n",
            "Epoch: 1/5  | Epoch loss: 3.499380350112915  | Mean train accuracy: 0.831430971622467\n",
            "Epoch: 1/5  | Epoch loss: 3.4752449989318848  | Mean train accuracy: 0.8343451619148254\n",
            "Epoch: 1/5  | Epoch loss: 3.4494497776031494  | Mean train accuracy: 0.8370527029037476\n",
            "Epoch: 1/5  | Epoch loss: 3.4283957481384277  | Mean train accuracy: 0.8395602107048035\n",
            "Epoch: 1/5  | Epoch loss: 3.403505325317383  | Mean train accuracy: 0.8419334292411804\n",
            "Epoch: 1/5  | Epoch loss: 3.384220600128174  | Mean train accuracy: 0.8441430926322937\n",
            "Epoch: 1/5  | Epoch loss: 3.3598756790161133  | Mean train accuracy: 0.8462227582931519\n",
            "Epoch: 1/5  | Epoch loss: 3.3405239582061768  | Mean train accuracy: 0.8482002019882202\n",
            "Epoch: 1/5  | Epoch loss: 3.318859100341797  | Mean train accuracy: 0.8500497937202454\n",
            "Epoch: 1/5  | Epoch loss: 3.3056657314300537  | Mean train accuracy: 0.8517889380455017\n",
            "Epoch: 1/5  | Epoch loss: 3.292532444000244  | Mean train accuracy: 0.8534356951713562\n",
            "Epoch: 1/5  | Epoch loss: 3.2749619483947754  | Mean train accuracy: 0.8549930453300476\n",
            "Epoch: 1/5  | Epoch loss: 3.260044574737549  | Mean train accuracy: 0.8564568758010864\n",
            "Epoch: 1/5  | Epoch loss: 3.246413230895996  | Mean train accuracy: 0.8578271269798279\n",
            "Epoch: 1/5  | Epoch loss: 3.2314863204956055  | Mean train accuracy: 0.8591468334197998\n",
            "Epoch: 1/5  | Epoch loss: 3.2200918197631836  | Mean train accuracy: 0.8603957891464233\n",
            "Epoch: 1/5  | Epoch loss: 3.2125301361083984  | Mean train accuracy: 0.8615823984146118\n",
            "Epoch: 1/5  | Epoch loss: 3.2017014026641846  | Mean train accuracy: 0.862699568271637\n",
            "Epoch: 1/5  | Epoch loss: 3.191476821899414  | Mean train accuracy: 0.8637537360191345\n",
            "Epoch: 1/5  | Epoch loss: 3.1805694103240967  | Mean train accuracy: 0.864758312702179\n",
            "Epoch: 1/5  | Epoch loss: 3.1687564849853516  | Mean train accuracy: 0.8657109141349792\n",
            "Epoch: 1/5  | Epoch loss: 3.161992073059082  | Mean train accuracy: 0.8666104078292847\n",
            "Epoch: 1/5  | Epoch loss: 3.157151222229004  | Mean train accuracy: 0.8674617409706116\n",
            "Epoch: 1/5  | Epoch loss: 3.1507439613342285  | Mean train accuracy: 0.8682677745819092\n",
            "Epoch: 1/5  | Epoch loss: 3.141937732696533  | Mean train accuracy: 0.8690248131752014\n",
            "Epoch: 1/5  | Epoch loss: 3.1298656463623047  | Mean train accuracy: 0.8697599768638611\n",
            "Epoch: 1/5  | Epoch loss: 3.12290358543396  | Mean train accuracy: 0.8704388737678528\n",
            "Epoch: 1/5  | Epoch loss: 3.112092971801758  | Mean train accuracy: 0.8710747957229614\n",
            "Epoch: 1/5  | Epoch loss: 3.1050779819488525  | Mean train accuracy: 0.8716848492622375\n",
            "Epoch: 1/5  | Epoch loss: 3.1000683307647705  | Mean train accuracy: 0.8722730875015259\n",
            "Epoch: 1/5  | Epoch loss: 3.0910568237304688  | Mean train accuracy: 0.8728253841400146\n",
            "Epoch: 1/5  | Epoch loss: 3.0824015140533447  | Mean train accuracy: 0.8733534216880798\n",
            "Epoch: 1/5  | Epoch loss: 3.073478937149048  | Mean train accuracy: 0.873864471912384\n",
            "Epoch: 1/5  | Epoch loss: 3.068275213241577  | Mean train accuracy: 0.8743460178375244\n",
            "Epoch: 1/5  | Epoch loss: 3.0619640350341797  | Mean train accuracy: 0.8748060464859009\n",
            "Epoch: 1/5  | Epoch loss: 3.0575315952301025  | Mean train accuracy: 0.8752501010894775\n",
            "Epoch: 1/5  | Epoch loss: 3.0542054176330566  | Mean train accuracy: 0.8756698369979858\n",
            "Epoch: 1/5  | Epoch loss: 3.0459110736846924  | Mean train accuracy: 0.8760771751403809\n",
            "Epoch: 1/5  | Epoch loss: 3.0406577587127686  | Mean train accuracy: 0.876467227935791\n",
            "Epoch: 1/5  | Epoch loss: 3.033787727355957  | Mean train accuracy: 0.876839280128479\n",
            "Epoch: 1/5  | Epoch loss: 3.028745174407959  | Mean train accuracy: 0.8771955370903015\n",
            "Epoch: 1/5  | Epoch loss: 3.023409128189087  | Mean train accuracy: 0.8775468468666077\n",
            "Epoch: 1/5  | Epoch loss: 3.0181515216827393  | Mean train accuracy: 0.877877414226532\n",
            "Epoch: 1/5  | Epoch loss: 3.0114614963531494  | Mean train accuracy: 0.8781977891921997\n",
            "Epoch: 1/5  | Epoch loss: 3.0052318572998047  | Mean train accuracy: 0.8785104751586914\n",
            "Epoch: 1/5  | Epoch loss: 2.999136447906494  | Mean train accuracy: 0.8788129687309265\n",
            "Epoch: 1/5  | Epoch loss: 2.993201494216919  | Mean train accuracy: 0.8791053295135498\n",
            "Epoch: 1/5  | Epoch loss: 2.9906325340270996  | Mean train accuracy: 0.8793897032737732\n",
            "Epoch: 1/5  | Epoch loss: 2.986264705657959  | Mean train accuracy: 0.8796654939651489\n",
            "Epoch: 1/5  | Epoch loss: 2.9802520275115967  | Mean train accuracy: 0.8799388408660889\n",
            "Epoch: 1/5  | Epoch loss: 2.9794468879699707  | Mean train accuracy: 0.8801944851875305\n",
            "Epoch: 1/5  | Epoch loss: 2.9759559631347656  | Mean train accuracy: 0.8804450035095215\n",
            "Epoch: 1/5  | Epoch loss: 2.9718945026397705  | Mean train accuracy: 0.8806928992271423\n",
            "Epoch: 1/5  | Epoch loss: 2.968214750289917  | Mean train accuracy: 0.8809260129928589\n",
            "Epoch: 1/5  | Epoch loss: 2.9626693725585938  | Mean train accuracy: 0.8811540007591248\n",
            "Epoch: 1/5  | Epoch loss: 2.958083391189575  | Mean train accuracy: 0.881378173828125\n",
            "Epoch: 1/5  | Epoch loss: 2.9534993171691895  | Mean train accuracy: 0.8815892934799194\n",
            "Epoch: 1/5  | Epoch loss: 2.9506680965423584  | Mean train accuracy: 0.8818098306655884\n",
            "Epoch: 1/5  | Epoch loss: 2.946031332015991  | Mean train accuracy: 0.8820183277130127\n",
            "Epoch: 1/5  | Epoch loss: 2.9430606365203857  | Mean train accuracy: 0.8822219371795654\n",
            "Epoch: 1/5  | Epoch loss: 2.94010591506958  | Mean train accuracy: 0.8824272155761719\n",
            "Epoch: 1/5  | Epoch loss: 2.936758279800415  | Mean train accuracy: 0.8826212882995605\n",
            "Epoch: 1/5  | Epoch loss: 2.9332919120788574  | Mean train accuracy: 0.8828045129776001\n",
            "Epoch: 1/5  | Epoch loss: 2.9290239810943604  | Mean train accuracy: 0.8829881548881531\n",
            "Epoch: 1/5  | Epoch loss: 2.927234172821045  | Mean train accuracy: 0.8831590414047241\n",
            "Epoch: 1/5  | Epoch loss: 2.9270598888397217  | Mean train accuracy: 0.8833342790603638\n",
            "Epoch: 1/5  | Epoch loss: 2.9254720211029053  | Mean train accuracy: 0.8834991455078125\n",
            "Epoch: 1/5  | Epoch loss: 2.9223403930664062  | Mean train accuracy: 0.8836569786071777\n",
            "Epoch: 1/5  | Epoch loss: 2.9193339347839355  | Mean train accuracy: 0.8838125467300415\n",
            "Epoch: 1/5  | Epoch loss: 2.9170618057250977  | Mean train accuracy: 0.8839585781097412\n",
            "Epoch: 1/5  | Epoch loss: 2.9120397567749023  | Mean train accuracy: 0.8840957880020142\n",
            "Epoch: 1/5  | Epoch loss: 2.9110500812530518  | Mean train accuracy: 0.8842251300811768\n",
            "Epoch: 1/5  | Epoch loss: 2.9101150035858154  | Mean train accuracy: 0.884345531463623\n",
            "Epoch: 1/5  | Epoch loss: 2.907576084136963  | Mean train accuracy: 0.8844643831253052\n",
            "Epoch: 1/5  | Epoch loss: 2.906116247177124  | Mean train accuracy: 0.8845773935317993\n",
            "Epoch: 1/5  | Epoch loss: 2.90305495262146  | Mean train accuracy: 0.8846850395202637\n",
            "Epoch: 1/5  | Epoch loss: 2.8980870246887207  | Mean train accuracy: 0.8847913146018982\n",
            "Epoch: 1/5  | Epoch loss: 2.8948707580566406  | Mean train accuracy: 0.8848931789398193\n",
            "Epoch: 1/5  | Epoch loss: 2.8916711807250977  | Mean train accuracy: 0.8849881291389465\n",
            "Epoch: 1/5  | Epoch loss: 2.8884117603302  | Mean train accuracy: 0.8850856423377991\n",
            "Epoch: 1/5  | Epoch loss: 2.8859665393829346  | Mean train accuracy: 0.8851794600486755\n",
            "Epoch: 1/5  | Epoch loss: 2.8828396797180176  | Mean train accuracy: 0.8852699995040894\n",
            "Epoch: 1/5  | Epoch loss: 2.8795454502105713  | Mean train accuracy: 0.8853664398193359\n",
            "Epoch: 1/5  | Epoch loss: 2.8786709308624268  | Mean train accuracy: 0.8854630589485168\n",
            "Epoch: 1/5  | Epoch loss: 2.877800226211548  | Mean train accuracy: 0.8855600357055664\n",
            "Epoch: 1/5  | Epoch loss: 2.8750202655792236  | Mean train accuracy: 0.8856542706489563\n",
            "Epoch: 1/5  | Epoch loss: 2.872750997543335  | Mean train accuracy: 0.8857553601264954\n",
            "Epoch: 1/5  | Epoch loss: 2.870946168899536  | Mean train accuracy: 0.8858529329299927\n",
            "Epoch: 1/5  | Epoch loss: 2.8698577880859375  | Mean train accuracy: 0.8859496116638184\n",
            "Epoch: 1/5  | Epoch loss: 2.8670125007629395  | Mean train accuracy: 0.8860454559326172\n",
            "Epoch: 1/5  | Epoch loss: 2.865823984146118  | Mean train accuracy: 0.8861428499221802\n",
            "Epoch: 1/5  | Epoch loss: 2.863487482070923  | Mean train accuracy: 0.8862322568893433\n",
            "Epoch: 1/5  | Epoch loss: 2.8608057498931885  | Mean train accuracy: 0.8863241672515869\n",
            "Epoch: 1/5  | Epoch loss: 2.8585708141326904  | Mean train accuracy: 0.8864110708236694\n",
            "Epoch: 1/5  | Epoch loss: 2.856670618057251  | Mean train accuracy: 0.8864977955818176\n",
            "Epoch: 1/5  | Epoch loss: 2.856029510498047  | Mean train accuracy: 0.8865863084793091\n",
            "Epoch: 1/5  | Epoch loss: 2.8532443046569824  | Mean train accuracy: 0.886667013168335\n",
            "Epoch: 1/5  | Epoch loss: 2.8513457775115967  | Mean train accuracy: 0.8867498636245728\n",
            "Epoch: 1/5  | Epoch loss: 2.84883451461792  | Mean train accuracy: 0.8868265151977539\n",
            "Epoch: 1/5  | Epoch loss: 2.847647190093994  | Mean train accuracy: 0.8869055509567261\n",
            "Epoch: 1/5  | Epoch loss: 2.8459951877593994  | Mean train accuracy: 0.8869861364364624\n",
            "Epoch: 1/5  | Epoch loss: 2.843555212020874  | Mean train accuracy: 0.8870660066604614\n",
            "Epoch: 1/5  | Epoch loss: 2.841905117034912  | Mean train accuracy: 0.8871426582336426\n",
            "Epoch: 1/5  | Epoch loss: 2.840090274810791  | Mean train accuracy: 0.8872170448303223\n",
            "Epoch: 1/5  | Epoch loss: 2.838193655014038  | Mean train accuracy: 0.8872904777526855\n",
            "Epoch: 1/5  | Epoch loss: 2.8362536430358887  | Mean train accuracy: 0.8873575925827026\n",
            "Epoch: 1/5  | Epoch loss: 2.8341619968414307  | Mean train accuracy: 0.8874304294586182\n",
            "Epoch: 1/5  | Epoch loss: 2.833024263381958  | Mean train accuracy: 0.8874994516372681\n",
            "Epoch: 1/5  | Epoch loss: 2.8310959339141846  | Mean train accuracy: 0.8875716328620911\n",
            "Epoch: 1/5  | Epoch loss: 2.8294332027435303  | Mean train accuracy: 0.8876418471336365\n",
            "Epoch: 1/5  | Epoch loss: 2.827676773071289  | Mean train accuracy: 0.8877105116844177\n",
            "Epoch: 1/5  | Epoch loss: 2.8258585929870605  | Mean train accuracy: 0.8877772092819214\n",
            "Epoch: 1/5  | Epoch loss: 2.8245413303375244  | Mean train accuracy: 0.8878447413444519\n",
            "Epoch: 1/5  | Epoch loss: 2.8225393295288086  | Mean train accuracy: 0.887904703617096\n",
            "Epoch: 1/5  | Epoch loss: 2.8207292556762695  | Mean train accuracy: 0.8879711627960205\n",
            "Epoch: 1/5  | Epoch loss: 2.8187716007232666  | Mean train accuracy: 0.8880282640457153\n",
            "Epoch: 1/5  | Epoch loss: 2.817326307296753  | Mean train accuracy: 0.8880938291549683\n",
            "Epoch: 1/5  | Epoch loss: 2.8161885738372803  | Mean train accuracy: 0.8881555795669556\n",
            "Epoch: 1/5  | Epoch loss: 2.8151321411132812  | Mean train accuracy: 0.8882155418395996\n",
            "Epoch: 1/5  | Epoch loss: 2.814664840698242  | Mean train accuracy: 0.8882764577865601\n",
            "Epoch: 1/5  | Epoch loss: 2.812664270401001  | Mean train accuracy: 0.8883299827575684\n",
            "Epoch: 1/5  | Epoch loss: 2.8119924068450928  | Mean train accuracy: 0.8883873224258423\n",
            "Epoch: 1/5  | Epoch loss: 2.810741424560547  | Mean train accuracy: 0.8884418606758118\n",
            "Epoch: 1/5  | Epoch loss: 2.8089420795440674  | Mean train accuracy: 0.8884970545768738\n",
            "Epoch: 1/5  | Epoch loss: 2.807234764099121  | Mean train accuracy: 0.888546347618103\n",
            "Epoch: 1/5  | Epoch loss: 2.805422306060791  | Mean train accuracy: 0.8885984420776367\n",
            "Epoch: 1/5  | Epoch loss: 2.803027629852295  | Mean train accuracy: 0.8886477947235107\n",
            "Epoch: 1/5  | Epoch loss: 2.8010427951812744  | Mean train accuracy: 0.8886983394622803\n",
            "Epoch: 1/5  | Epoch loss: 2.799537420272827  | Mean train accuracy: 0.8887454867362976\n",
            "Epoch: 1/5  | Epoch loss: 2.799013614654541  | Mean train accuracy: 0.8887903690338135\n",
            "Epoch: 1/5  | Epoch loss: 2.7975339889526367  | Mean train accuracy: 0.8888327479362488\n",
            "Epoch: 1/5  | Epoch loss: 2.796736478805542  | Mean train accuracy: 0.88887619972229\n",
            "Epoch: 1/5  | Epoch loss: 2.7959706783294678  | Mean train accuracy: 0.8889188170433044\n",
            "Epoch: 1/5  | Epoch loss: 2.7944514751434326  | Mean train accuracy: 0.8889601826667786\n",
            "Epoch: 1/5  | Epoch loss: 2.792835235595703  | Mean train accuracy: 0.8890010714530945\n",
            "Epoch: 1/5  | Epoch loss: 2.792166233062744  | Mean train accuracy: 0.8890358805656433\n",
            "Epoch: 1/5  | Epoch loss: 2.791635036468506  | Mean train accuracy: 0.889070451259613\n",
            "Epoch: 1/5  | Epoch loss: 2.79020619392395  | Mean train accuracy: 0.8891037702560425\n",
            "Epoch: 1/5  | Epoch loss: 2.7884838581085205  | Mean train accuracy: 0.8891361951828003\n",
            "Epoch: 1/5  | Epoch loss: 2.7873666286468506  | Mean train accuracy: 0.8891703486442566\n",
            "Epoch: 1/5  | Epoch loss: 2.7868118286132812  | Mean train accuracy: 0.8892050385475159\n",
            "Epoch: 1/5  | Epoch loss: 2.785029888153076  | Mean train accuracy: 0.8892362117767334\n",
            "Epoch: 1/5  | Epoch loss: 2.7829582691192627  | Mean train accuracy: 0.8892710208892822\n",
            "Epoch: 1/5  | Epoch loss: 2.782179832458496  | Mean train accuracy: 0.8893024921417236\n",
            "Epoch: 1/5  | Epoch loss: 2.781244993209839  | Mean train accuracy: 0.889333188533783\n",
            "Epoch: 1/5  | Epoch loss: 2.779726266860962  | Mean train accuracy: 0.8893647789955139\n",
            "Epoch: 1/5  | Epoch loss: 2.7776339054107666  | Mean train accuracy: 0.8893986344337463\n",
            "Epoch: 1/5  | Epoch loss: 2.7765820026397705  | Mean train accuracy: 0.8894317746162415\n",
            "Epoch: 1/5  | Epoch loss: 2.7748985290527344  | Mean train accuracy: 0.8894631862640381\n",
            "Epoch: 1/5  | Epoch loss: 2.7735390663146973  | Mean train accuracy: 0.8894993662834167\n",
            "Epoch: 1/5  | Epoch loss: 2.7711431980133057  | Mean train accuracy: 0.8895333409309387\n",
            "Epoch: 1/5  | Epoch loss: 2.7703964710235596  | Mean train accuracy: 0.889565646648407\n",
            "Epoch: 1/5  | Epoch loss: 2.769665479660034  | Mean train accuracy: 0.8895995020866394\n",
            "Epoch: 1/5  | Epoch loss: 2.769005060195923  | Mean train accuracy: 0.8896331191062927\n",
            "Epoch: 1/5  | Epoch loss: 2.7692208290100098  | Mean train accuracy: 0.8896685838699341\n",
            "Epoch: 1/5  | Epoch loss: 2.768036365509033  | Mean train accuracy: 0.8897117972373962\n",
            "Epoch: 1/5  | Epoch loss: 2.7671308517456055  | Mean train accuracy: 0.8897524476051331\n",
            "Epoch: 1/5  | Epoch loss: 2.76802659034729  | Mean train accuracy: 0.8897895812988281\n",
            "Epoch: 1/5  | Epoch loss: 2.7658445835113525  | Mean train accuracy: 0.8898265361785889\n",
            "Epoch: 1/5  | Epoch loss: 2.7640573978424072  | Mean train accuracy: 0.8898664712905884\n",
            "Epoch: 1/5  | Epoch loss: 2.7635324001312256  | Mean train accuracy: 0.8899046182632446\n",
            "Epoch: 1/5  | Epoch loss: 2.7631900310516357  | Mean train accuracy: 0.8899428844451904\n",
            "Epoch: 1/5  | Epoch loss: 2.7626373767852783  | Mean train accuracy: 0.8899790644645691\n",
            "Epoch: 1/5  | Epoch loss: 2.7620315551757812  | Mean train accuracy: 0.8900156021118164\n",
            "Epoch: 1/5  | Epoch loss: 2.7618415355682373  | Mean train accuracy: 0.8900488615036011\n",
            "Epoch: 1/5  | Epoch loss: 2.7605624198913574  | Mean train accuracy: 0.8900831937789917\n",
            "Epoch: 1/5  | Epoch loss: 2.7602016925811768  | Mean train accuracy: 0.8901137709617615\n",
            "Epoch: 1/5  | Epoch loss: 2.759342908859253  | Mean train accuracy: 0.8901450634002686\n",
            "Epoch: 1/5  | Epoch loss: 2.758955478668213  | Mean train accuracy: 0.8901723027229309\n",
            "Epoch: 1/5  | Epoch loss: 2.757852792739868  | Mean train accuracy: 0.8902013301849365\n",
            "Epoch: 1/5  | Epoch loss: 2.757145404815674  | Mean train accuracy: 0.8902292847633362\n",
            "Epoch: 1/5  | Epoch loss: 2.7569901943206787  | Mean train accuracy: 0.8902542591094971\n",
            "Epoch: 1/5  | Epoch loss: 2.7563247680664062  | Mean train accuracy: 0.8902810215950012\n",
            "Epoch: 1/5  | Epoch loss: 2.7554783821105957  | Mean train accuracy: 0.8903055191040039\n",
            "Epoch: 1/5  | Epoch loss: 2.7547147274017334  | Mean train accuracy: 0.8903268575668335\n",
            "Epoch: 1/5  | Epoch loss: 2.754474401473999  | Mean train accuracy: 0.8903525471687317\n",
            "Epoch: 1/5  | Epoch loss: 2.753094434738159  | Mean train accuracy: 0.8903763890266418\n",
            "Epoch: 1/5  | Epoch loss: 2.752068042755127  | Mean train accuracy: 0.8903994560241699\n",
            "Epoch: 1/5  | Epoch loss: 2.75195050239563  | Mean train accuracy: 0.8904218673706055\n",
            "Epoch: 1/5  | Epoch loss: 2.750990390777588  | Mean train accuracy: 0.8904430866241455\n",
            "Epoch: 1/5  | Epoch loss: 2.7493839263916016  | Mean train accuracy: 0.8904626965522766\n",
            "Epoch: 1/5  | Epoch loss: 2.747946262359619  | Mean train accuracy: 0.8904858827590942\n",
            "Epoch: 1/5  | Epoch loss: 2.7474186420440674  | Mean train accuracy: 0.8905104398727417\n",
            "Epoch: 1/5  | Epoch loss: 2.746823787689209  | Mean train accuracy: 0.8905333280563354\n",
            "Epoch: 1/5  | Epoch loss: 2.7457549571990967  | Mean train accuracy: 0.8905531167984009\n",
            "Epoch: 1/5  | Epoch loss: 2.744962453842163  | Mean train accuracy: 0.8905808329582214\n",
            "Epoch: 1/5  | Epoch loss: 2.7440433502197266  | Mean train accuracy: 0.890605628490448\n",
            "Epoch: 1/5  | Epoch loss: 2.743802070617676  | Mean train accuracy: 0.8906269669532776\n",
            "Epoch: 1/5  | Epoch loss: 2.742600202560425  | Mean train accuracy: 0.8906492590904236\n",
            "Epoch: 1/5  | Epoch loss: 2.741830825805664  | Mean train accuracy: 0.8906710147857666\n",
            "Epoch: 1/5  | Epoch loss: 2.7419168949127197  | Mean train accuracy: 0.8906946778297424\n",
            "Epoch: 1/5  | Epoch loss: 2.741067886352539  | Mean train accuracy: 0.8907199501991272\n",
            "Epoch: 1/5  | Epoch loss: 2.7399544715881348  | Mean train accuracy: 0.8907419443130493\n",
            "Epoch: 1/5  | Epoch loss: 2.740304946899414  | Mean train accuracy: 0.890762984752655\n",
            "Epoch: 1/5  | Epoch loss: 2.740731716156006  | Mean train accuracy: 0.8907848596572876\n",
            "Epoch: 1/5  | Epoch loss: 2.739598035812378  | Mean train accuracy: 0.8908071517944336\n",
            "Epoch: 1/5  | Epoch loss: 2.738710641860962  | Mean train accuracy: 0.8908259868621826\n",
            "Epoch: 1/5  | Epoch loss: 2.738110303878784  | Mean train accuracy: 0.8908446431159973\n",
            "Epoch: 1/5  | Epoch loss: 2.7378785610198975  | Mean train accuracy: 0.8908619284629822\n",
            "Epoch: 1/5  | Epoch loss: 2.7375452518463135  | Mean train accuracy: 0.8908800482749939\n",
            "Epoch: 1/5  | Epoch loss: 2.7372000217437744  | Mean train accuracy: 0.890892744064331\n",
            "Epoch: 1/5  | Epoch loss: 2.736461877822876  | Mean train accuracy: 0.890905499458313\n",
            "Epoch: 1/5  | Epoch loss: 2.7352309226989746  | Mean train accuracy: 0.8909210562705994\n",
            "Epoch: 1/5  | Epoch loss: 2.735351085662842  | Mean train accuracy: 0.8909323811531067\n",
            "Epoch: 1/5  | Epoch loss: 2.73502516746521  | Mean train accuracy: 0.8909425735473633\n",
            "Epoch: 1/5  | Epoch loss: 2.7348217964172363  | Mean train accuracy: 0.8909521102905273\n",
            "Epoch: 1/5  | Epoch loss: 2.7337636947631836  | Mean train accuracy: 0.8909627795219421\n",
            "Epoch: 1/5  | Epoch loss: 2.732801914215088  | Mean train accuracy: 0.8909690380096436\n",
            "Epoch: 1/5  | Epoch loss: 2.732900857925415  | Mean train accuracy: 0.8909745216369629\n",
            "Epoch: 1/5  | Epoch loss: 2.7317981719970703  | Mean train accuracy: 0.8909831047058105\n",
            "Epoch: 1/5  | Epoch loss: 2.7309632301330566  | Mean train accuracy: 0.8909900188446045\n",
            "Epoch: 1/5  | Epoch loss: 2.730787754058838  | Mean train accuracy: 0.8909918665885925\n",
            "Epoch: 1/5  | Epoch loss: 2.7303073406219482  | Mean train accuracy: 0.8909977674484253\n",
            "Epoch: 1/5  | Epoch loss: 2.729715585708618  | Mean train accuracy: 0.8910017609596252\n",
            "Epoch: 1/5  | Epoch loss: 2.7295007705688477  | Mean train accuracy: 0.8910053372383118\n",
            "Epoch: 1/5  | Epoch loss: 2.7286970615386963  | Mean train accuracy: 0.8910068869590759\n",
            "Epoch: 1/5  | Epoch loss: 2.7286806106567383  | Mean train accuracy: 0.8910101652145386\n",
            "Epoch: 1/5  | Epoch loss: 2.7280020713806152  | Mean train accuracy: 0.8910132050514221\n",
            "Epoch: 1/5  | Epoch loss: 2.727492332458496  | Mean train accuracy: 0.8910158276557922\n",
            "Epoch: 1/5  | Epoch loss: 2.7270004749298096  | Mean train accuracy: 0.8910216093063354\n",
            "Epoch: 1/5  | Epoch loss: 2.7269082069396973  | Mean train accuracy: 0.8910275101661682\n",
            "Epoch: 1/5  | Epoch loss: 2.726114511489868  | Mean train accuracy: 0.891037106513977\n",
            "Epoch: 1/5  | Epoch loss: 2.726578712463379  | Mean train accuracy: 0.8910447955131531\n",
            "Epoch: 1/5  | Epoch loss: 2.7263100147247314  | Mean train accuracy: 0.8910528421401978\n",
            "Epoch: 1/5  | Epoch loss: 2.7259557247161865  | Mean train accuracy: 0.8910601139068604\n",
            "Epoch: 1/5  | Epoch loss: 2.7257578372955322  | Mean train accuracy: 0.8910694718360901\n",
            "Epoch: 1/5  | Epoch loss: 2.7252213954925537  | Mean train accuracy: 0.8910741806030273\n",
            "Epoch: 1/5  | Epoch loss: 2.724522352218628  | Mean train accuracy: 0.8910783529281616\n",
            "Epoch: 1/5  | Epoch loss: 2.7245004177093506  | Mean train accuracy: 0.8910841941833496\n",
            "Epoch: 1/5  | Epoch loss: 2.7245638370513916  | Mean train accuracy: 0.8910896182060242\n",
            "Epoch: 1/5  | Epoch loss: 2.7243285179138184  | Mean train accuracy: 0.8910952806472778\n",
            "Epoch: 1/5  | Epoch loss: 2.7239301204681396  | Mean train accuracy: 0.8911035656929016\n",
            "Epoch: 1/5  | Epoch loss: 2.7238659858703613  | Mean train accuracy: 0.8911102414131165\n",
            "Epoch: 1/5  | Epoch loss: 2.7235748767852783  | Mean train accuracy: 0.8911176919937134\n",
            "Epoch: 1/5  | Epoch loss: 2.7228095531463623  | Mean train accuracy: 0.8911271691322327\n",
            "Epoch: 1/5  | Epoch loss: 2.722400188446045  | Mean train accuracy: 0.8911338448524475\n",
            "Epoch: 1/5  | Epoch loss: 2.721698760986328  | Mean train accuracy: 0.8911415338516235\n",
            "Epoch: 1/5  | Epoch loss: 2.7220277786254883  | Mean train accuracy: 0.891149640083313\n",
            "Epoch: 1/5  | Epoch loss: 2.7216718196868896  | Mean train accuracy: 0.891156017780304\n",
            "Epoch: 1/5  | Epoch loss: 2.721144676208496  | Mean train accuracy: 0.8911638855934143\n",
            "Epoch: 1/5  | Epoch loss: 2.721095561981201  | Mean train accuracy: 0.8911721706390381\n",
            "Epoch: 1/5  | Epoch loss: 2.720977306365967  | Mean train accuracy: 0.8911818861961365\n",
            "Epoch: 1/5  | Epoch loss: 2.720998764038086  | Mean train accuracy: 0.8911904692649841\n",
            "Epoch: 1/5  | Epoch loss: 2.7205734252929688  | Mean train accuracy: 0.8911983370780945\n",
            "Epoch: 1/5  | Epoch loss: 2.7197608947753906  | Mean train accuracy: 0.8912047147750854\n",
            "Epoch: 1/5  | Epoch loss: 2.7189087867736816  | Mean train accuracy: 0.8912078738212585\n",
            "Epoch: 1/5  | Epoch loss: 2.718414783477783  | Mean train accuracy: 0.891213059425354\n",
            "Epoch: 1/5  | Epoch loss: 2.717667579650879  | Mean train accuracy: 0.8912183046340942\n",
            "Epoch: 1/5  | Epoch loss: 2.717289924621582  | Mean train accuracy: 0.8912250399589539\n",
            "Epoch: 1/5  | Epoch loss: 2.7169888019561768  | Mean train accuracy: 0.8912305235862732\n",
            "Epoch: 1/5  | Epoch loss: 2.716179370880127  | Mean train accuracy: 0.8912368416786194\n",
            "Epoch: 1/5  | Epoch loss: 2.7161757946014404  | Mean train accuracy: 0.891243577003479\n",
            "Epoch: 1/5  | Epoch loss: 2.7154221534729004  | Mean train accuracy: 0.8912501335144043\n",
            "Epoch: 1/5  | Epoch loss: 2.7150113582611084  | Mean train accuracy: 0.8912578225135803\n",
            "Epoch: 1/5  | Epoch loss: 2.714400291442871  | Mean train accuracy: 0.8912625312805176\n",
            "Epoch: 1/5  | Epoch loss: 2.7134742736816406  | Mean train accuracy: 0.891272246837616\n",
            "Epoch: 1/5  | Epoch loss: 2.713080406188965  | Mean train accuracy: 0.8912822008132935\n",
            "Epoch: 1/5  | Epoch loss: 2.7128965854644775  | Mean train accuracy: 0.8912914991378784\n",
            "Epoch: 1/5  | Epoch loss: 2.7121572494506836  | Mean train accuracy: 0.891302764415741\n",
            "Epoch: 1/5  | Epoch loss: 2.7120091915130615  | Mean train accuracy: 0.8913102149963379\n",
            "Epoch: 1/5  | Epoch loss: 2.7113189697265625  | Mean train accuracy: 0.8913173079490662\n",
            "Epoch: 1/5  | Epoch loss: 2.711139678955078  | Mean train accuracy: 0.8913278579711914\n",
            "Epoch: 1/5  | Epoch loss: 2.7099976539611816  | Mean train accuracy: 0.8913385272026062\n",
            "Epoch: 1/5  | Epoch loss: 2.7096455097198486  | Mean train accuracy: 0.8913477063179016\n",
            "Epoch: 1/5  | Epoch loss: 2.7090113162994385  | Mean train accuracy: 0.8913577198982239\n",
            "Epoch: 1/5  | Epoch loss: 2.708883047103882  | Mean train accuracy: 0.891368567943573\n",
            "Epoch: 1/5  | Epoch loss: 2.708071708679199  | Mean train accuracy: 0.8913780450820923\n",
            "Epoch: 1/5  | Epoch loss: 2.7070915699005127  | Mean train accuracy: 0.8913868069648743\n",
            "Epoch: 1/5  | Epoch loss: 2.7064208984375  | Mean train accuracy: 0.891399621963501\n",
            "Epoch: 1/5  | Epoch loss: 2.7068932056427  | Mean train accuracy: 0.891409695148468\n",
            "Epoch: 1/5  | Epoch loss: 2.7065296173095703  | Mean train accuracy: 0.8914196491241455\n",
            "Epoch: 1/5  | Epoch loss: 2.7063212394714355  | Mean train accuracy: 0.8914326429367065\n",
            "Epoch: 1/5  | Epoch loss: 2.7063052654266357  | Mean train accuracy: 0.8914433121681213\n",
            "Epoch: 1/5  | Epoch loss: 2.705904960632324  | Mean train accuracy: 0.8914536833763123\n",
            "Epoch: 1/5  | Epoch loss: 2.7054905891418457  | Mean train accuracy: 0.8914656639099121\n",
            "Epoch: 1/5  | Epoch loss: 2.705130100250244  | Mean train accuracy: 0.891475260257721\n",
            "Epoch: 1/5  | Epoch loss: 2.7050957679748535  | Mean train accuracy: 0.8914860486984253\n",
            "Epoch: 1/5  | Epoch loss: 2.7054479122161865  | Mean train accuracy: 0.8914933800697327\n",
            "Epoch: 1/5  | Epoch loss: 2.7051119804382324  | Mean train accuracy: 0.8915010690689087\n",
            "Epoch: 1/5  | Epoch loss: 2.7046570777893066  | Mean train accuracy: 0.8915069699287415\n",
            "Epoch: 1/5  | Epoch loss: 2.7042031288146973  | Mean train accuracy: 0.8915113806724548\n",
            "Epoch: 1/5  | Epoch loss: 2.703296184539795  | Mean train accuracy: 0.8915165662765503\n",
            "Epoch: 1/5  | Epoch loss: 2.703263998031616  | Mean train accuracy: 0.891521155834198\n",
            "Epoch: 1/5  | Epoch loss: 2.7025516033172607  | Mean train accuracy: 0.8915256857872009\n",
            "Epoch: 1/5  | Epoch loss: 2.7021660804748535  | Mean train accuracy: 0.8915309906005859\n",
            "Epoch: 1/5  | Epoch loss: 2.702115058898926  | Mean train accuracy: 0.8915351033210754\n",
            "Epoch: 1/5  | Epoch loss: 2.7021327018737793  | Mean train accuracy: 0.8915407061576843\n",
            "Epoch: 1/5  | Epoch loss: 2.7018773555755615  | Mean train accuracy: 0.8915445804595947\n",
            "Epoch: 1/5  | Epoch loss: 2.701718807220459  | Mean train accuracy: 0.8915515542030334\n",
            "Epoch: 1/5  | Epoch loss: 2.701125144958496  | Mean train accuracy: 0.8915593028068542\n",
            "Epoch: 1/5  | Epoch loss: 2.7007699012756348  | Mean train accuracy: 0.8915656805038452\n",
            "Epoch: 1/5  | Epoch loss: 2.7007360458374023  | Mean train accuracy: 0.8915718793869019\n",
            "Epoch: 1/5  | Epoch loss: 2.7007524967193604  | Mean train accuracy: 0.8915786147117615\n",
            "Epoch: 1/5  | Epoch loss: 2.7005465030670166  | Mean train accuracy: 0.8915841579437256\n",
            "Epoch: 1/5  | Epoch loss: 2.7003836631774902  | Mean train accuracy: 0.891588568687439\n",
            "Epoch: 1/5  | Epoch loss: 2.69974422454834  | Mean train accuracy: 0.891593873500824\n",
            "Epoch: 1/5  | Epoch loss: 2.6997833251953125  | Mean train accuracy: 0.8915974497795105\n",
            "Epoch: 1/5  | Epoch loss: 2.699462413787842  | Mean train accuracy: 0.8916013836860657\n",
            "Epoch: 1/5  | Epoch loss: 2.69889497756958  | Mean train accuracy: 0.8916066884994507\n",
            "Epoch: 1/5  | Epoch loss: 2.6985344886779785  | Mean train accuracy: 0.8916113972663879\n",
            "Epoch: 1/5  | Epoch loss: 2.6982510089874268  | Mean train accuracy: 0.891615629196167\n",
            "Epoch: 1/5  | Epoch loss: 2.6974682807922363  | Mean train accuracy: 0.8916198015213013\n",
            "Epoch: 1/5  | Epoch loss: 2.6969094276428223  | Mean train accuracy: 0.8916234374046326\n",
            "Epoch: 1/5  | Epoch loss: 2.696225166320801  | Mean train accuracy: 0.8916270136833191\n",
            "Epoch: 1/5  | Epoch loss: 2.6957736015319824  | Mean train accuracy: 0.8916311860084534\n",
            "Epoch: 1/5  | Epoch loss: 2.6957006454467773  | Mean train accuracy: 0.8916358351707458\n",
            "Epoch: 1/5  | Epoch loss: 2.695946216583252  | Mean train accuracy: 0.8916398286819458\n",
            "Epoch: 1/5  | Epoch loss: 2.695392608642578  | Mean train accuracy: 0.8916436433792114\n",
            "Epoch: 1/5  | Epoch loss: 2.6947896480560303  | Mean train accuracy: 0.891647458076477\n",
            "Epoch: 1/5  | Epoch loss: 2.694807529449463  | Mean train accuracy: 0.8916500210762024\n",
            "Epoch: 1/5  | Epoch loss: 2.6939198970794678  | Mean train accuracy: 0.8916541337966919\n",
            "Epoch: 1/5  | Epoch loss: 2.6934077739715576  | Mean train accuracy: 0.8916566967964172\n",
            "Epoch: 1/5  | Epoch loss: 2.6929311752319336  | Mean train accuracy: 0.8916588425636292\n",
            "Epoch: 1/5  | Epoch loss: 2.692878246307373  | Mean train accuracy: 0.89165860414505\n",
            "Epoch: 1/5  | Epoch loss: 2.692302942276001  | Mean train accuracy: 0.8916600942611694\n",
            "Epoch: 1/5  | Epoch loss: 2.6919572353363037  | Mean train accuracy: 0.891663134098053\n",
            "Epoch: 1/5  | Epoch loss: 2.6917319297790527  | Mean train accuracy: 0.891665518283844\n",
            "Epoch: 1/5  | Epoch loss: 2.692103862762451  | Mean train accuracy: 0.8916689157485962\n",
            "Epoch: 1/5  | Epoch loss: 2.692176342010498  | Mean train accuracy: 0.8916729688644409\n",
            "Epoch: 1/5  | Epoch loss: 2.6914925575256348  | Mean train accuracy: 0.8916758894920349\n",
            "Epoch: 1/5  | Epoch loss: 2.6915714740753174  | Mean train accuracy: 0.8916793465614319\n",
            "Epoch: 1/5  | Epoch loss: 2.691188097000122  | Mean train accuracy: 0.8916801810264587\n",
            "Epoch: 1/5  | Epoch loss: 2.691084146499634  | Mean train accuracy: 0.891683042049408\n",
            "Epoch: 1/5  | Epoch loss: 2.6906278133392334  | Mean train accuracy: 0.8916845321655273\n",
            "Epoch: 1/5  | Epoch loss: 2.6906652450561523  | Mean train accuracy: 0.8916869163513184\n",
            "Epoch: 1/5  | Epoch loss: 2.6904735565185547  | Mean train accuracy: 0.8916847705841064\n",
            "Epoch: 1/5  | Epoch loss: 2.6904866695404053  | Mean train accuracy: 0.8916869163513184\n",
            "Epoch: 1/5  | Epoch loss: 2.6899967193603516  | Mean train accuracy: 0.8916879296302795\n",
            "Epoch: 1/5  | Epoch loss: 2.689905881881714  | Mean train accuracy: 0.89168781042099\n",
            "Epoch: 1/5  | Epoch loss: 2.689884901046753  | Mean train accuracy: 0.8916857242584229\n",
            "Epoch: 1/5  | Epoch loss: 2.6904306411743164  | Mean train accuracy: 0.8916834592819214\n",
            "Epoch: 1/5  | Epoch loss: 2.6902782917022705  | Mean train accuracy: 0.891685426235199\n",
            "Epoch: 1/5  | Epoch loss: 2.689758539199829  | Mean train accuracy: 0.8916865587234497\n",
            "Epoch: 1/5  | Epoch loss: 2.689882516860962  | Mean train accuracy: 0.8916873335838318\n",
            "Epoch: 1/5  | Epoch loss: 2.689333200454712  | Mean train accuracy: 0.891687273979187\n",
            "Epoch: 1/5  | Epoch loss: 2.6890363693237305  | Mean train accuracy: 0.891687273979187\n",
            "Epoch: 1/5  | Epoch loss: 2.6892220973968506  | Mean train accuracy: 0.8916885852813721\n",
            "Epoch: 1/5  | Epoch loss: 2.689094305038452  | Mean train accuracy: 0.8916891813278198\n",
            "Epoch: 1/5  | Epoch loss: 2.6887545585632324  | Mean train accuracy: 0.891689658164978\n",
            "Epoch: 1/5  | Epoch loss: 2.6884045600891113  | Mean train accuracy: 0.891690731048584\n",
            "Epoch: 1/5  | Epoch loss: 2.6880087852478027  | Mean train accuracy: 0.8916919827461243\n",
            "Epoch: 1/5  | Epoch loss: 2.687723159790039  | Mean train accuracy: 0.8916938304901123\n",
            "Epoch: 1/5  | Epoch loss: 2.6874196529388428  | Mean train accuracy: 0.8916972279548645\n",
            "Epoch: 1/5  | Epoch loss: 2.6873412132263184  | Mean train accuracy: 0.8916990756988525\n",
            "Epoch: 1/5  | Epoch loss: 2.6868093013763428  | Mean train accuracy: 0.8917019367218018\n",
            "Epoch: 1/5  | Epoch loss: 2.6865651607513428  | Mean train accuracy: 0.891704261302948\n",
            "Epoch: 1/5  | Epoch loss: 2.6862189769744873  | Mean train accuracy: 0.8917078971862793\n",
            "Epoch: 1/5  | Epoch loss: 2.68557071685791  | Mean train accuracy: 0.8917102217674255\n",
            "Epoch: 1/5  | Epoch loss: 2.68524169921875  | Mean train accuracy: 0.8917125463485718\n",
            "Epoch: 1/5  | Epoch loss: 2.6851115226745605  | Mean train accuracy: 0.8917147517204285\n",
            "Epoch: 1/5  | Epoch loss: 2.68469500541687  | Mean train accuracy: 0.891717791557312\n",
            "Epoch: 1/5  | Epoch loss: 2.6845948696136475  | Mean train accuracy: 0.8917218446731567\n",
            "Epoch: 1/5  | Epoch loss: 2.6850790977478027  | Mean train accuracy: 0.8917258977890015\n",
            "Epoch: 1/5  | Epoch loss: 2.6849920749664307  | Mean train accuracy: 0.8917295336723328\n",
            "Epoch: 1/5  | Epoch loss: 2.6843512058258057  | Mean train accuracy: 0.8917340636253357\n",
            "Epoch: 1/5  | Epoch loss: 2.684063673019409  | Mean train accuracy: 0.8917374014854431\n",
            "Epoch: 1/5  | Epoch loss: 2.68394136428833  | Mean train accuracy: 0.8917418122291565\n",
            "Epoch: 1/5  | Epoch loss: 2.6835777759552  | Mean train accuracy: 0.8917461037635803\n",
            "Epoch: 1/5  | Epoch loss: 2.6834943294525146  | Mean train accuracy: 0.8917496800422668\n",
            "Epoch: 1/5  | Epoch loss: 2.6835432052612305  | Mean train accuracy: 0.891753613948822\n",
            "Epoch: 1/5  | Epoch loss: 2.683851957321167  | Mean train accuracy: 0.8917576670646667\n",
            "Epoch: 1/5  | Epoch loss: 2.683906316757202  | Mean train accuracy: 0.8917624950408936\n",
            "Epoch: 1/5  | Epoch loss: 2.6834750175476074  | Mean train accuracy: 0.8917658925056458\n",
            "Epoch: 1/5  | Epoch loss: 2.6833341121673584  | Mean train accuracy: 0.8917690515518188\n",
            "Epoch: 1/5  | Epoch loss: 2.683166742324829  | Mean train accuracy: 0.8917708992958069\n",
            "Epoch: 1/5  | Epoch loss: 2.6822550296783447  | Mean train accuracy: 0.8917739987373352\n",
            "Epoch: 1/5  | Epoch loss: 2.6821234226226807  | Mean train accuracy: 0.8917757868766785\n",
            "Epoch: 1/5  | Epoch loss: 2.682051181793213  | Mean train accuracy: 0.8917781114578247\n",
            "Epoch: 1/5  | Epoch loss: 2.6819868087768555  | Mean train accuracy: 0.8917810916900635\n",
            "Epoch: 1/5  | Epoch loss: 2.681673288345337  | Mean train accuracy: 0.8917841911315918\n",
            "Epoch: 1/5  | Epoch loss: 2.680798292160034  | Mean train accuracy: 0.8917877078056335\n",
            "Epoch: 1/5  | Epoch loss: 2.6805665493011475  | Mean train accuracy: 0.8917913436889648\n",
            "Epoch: 1/5  | Epoch loss: 2.6802144050598145  | Mean train accuracy: 0.8917958736419678\n",
            "Epoch: 1/5  | Epoch loss: 2.680081605911255  | Mean train accuracy: 0.8918008804321289\n",
            "Epoch: 1/5  | Epoch loss: 2.679844379425049  | Mean train accuracy: 0.89180588722229\n",
            "Epoch: 1/5  | Epoch loss: 2.67976975440979  | Mean train accuracy: 0.8918120861053467\n",
            "Epoch: 1/5  | Epoch loss: 2.679593801498413  | Mean train accuracy: 0.8918198943138123\n",
            "Epoch: 1/5  | Epoch loss: 2.679448366165161  | Mean train accuracy: 0.8918259143829346\n",
            "Epoch: 1/5  | Epoch loss: 2.6789300441741943  | Mean train accuracy: 0.8918315172195435\n",
            "Epoch: 1/5  | Epoch loss: 2.678597927093506  | Mean train accuracy: 0.891838550567627\n",
            "Epoch: 1/5  | Epoch loss: 2.6780707836151123  | Mean train accuracy: 0.8918441534042358\n",
            "Epoch: 1/5  | Epoch loss: 2.67814564704895  | Mean train accuracy: 0.8918513655662537\n",
            "Epoch: 1/5  | Epoch loss: 2.677838087081909  | Mean train accuracy: 0.8918576836585999\n",
            "Epoch: 1/5  | Epoch loss: 2.6776134967803955  | Mean train accuracy: 0.8918628692626953\n",
            "Epoch: 1/5  | Epoch loss: 2.6777963638305664  | Mean train accuracy: 0.8918678164482117\n",
            "Epoch: 1/5  | Epoch loss: 2.6777796745300293  | Mean train accuracy: 0.8918719291687012\n",
            "Epoch: 1/5  | Epoch loss: 2.677926778793335  | Mean train accuracy: 0.8918769359588623\n",
            "Epoch: 1/5  | Epoch loss: 2.6774842739105225  | Mean train accuracy: 0.891882061958313\n",
            "Epoch: 1/5  | Epoch loss: 2.6773033142089844  | Mean train accuracy: 0.8918864130973816\n",
            "Epoch: 1/5  | Epoch loss: 2.677161693572998  | Mean train accuracy: 0.8918910026550293\n",
            "Epoch: 1/5  | Epoch loss: 2.6766607761383057  | Mean train accuracy: 0.8918957114219666\n",
            "Epoch: 1/5  | Epoch loss: 2.676295518875122  | Mean train accuracy: 0.8918986320495605\n",
            "Epoch: 1/5  | Epoch loss: 2.6759536266326904  | Mean train accuracy: 0.891901969909668\n",
            "Epoch: 1/5  | Epoch loss: 2.6760923862457275  | Mean train accuracy: 0.8919057250022888\n",
            "Epoch: 1/5  | Epoch loss: 2.6756787300109863  | Mean train accuracy: 0.8919088840484619\n",
            "Epoch: 1/5  | Epoch loss: 2.6748552322387695  | Mean train accuracy: 0.8919126391410828\n",
            "Epoch: 1/5  | Epoch loss: 2.6740894317626953  | Mean train accuracy: 0.8919167518615723\n",
            "Epoch: 1/5  | Epoch loss: 2.673780679702759  | Mean train accuracy: 0.8919208645820618\n",
            "Epoch: 1/5  | Epoch loss: 2.673732280731201  | Mean train accuracy: 0.8919239044189453\n",
            "Epoch: 1/5  | Epoch loss: 2.673431396484375  | Mean train accuracy: 0.891929566860199\n",
            "Epoch: 1/5  | Epoch loss: 2.673264265060425  | Mean train accuracy: 0.8919342756271362\n",
            "Epoch: 1/5  | Epoch loss: 2.673107147216797  | Mean train accuracy: 0.8919399976730347\n",
            "Epoch: 1/5  | Epoch loss: 2.6731176376342773  | Mean train accuracy: 0.8919455409049988\n",
            "Epoch: 1/5  | Epoch loss: 2.6729795932769775  | Mean train accuracy: 0.8919516801834106\n",
            "Epoch: 1/5  | Epoch loss: 2.6733930110931396  | Mean train accuracy: 0.8919575810432434\n",
            "Epoch: 1/5  | Epoch loss: 2.673126459121704  | Mean train accuracy: 0.891963541507721\n",
            "Epoch: 1/5  | Epoch loss: 2.6731183528900146  | Mean train accuracy: 0.8919684886932373\n",
            "Epoch: 1/5  | Epoch loss: 2.6728708744049072  | Mean train accuracy: 0.8919735550880432\n",
            "Epoch: 1/5  | Epoch loss: 2.672487497329712  | Mean train accuracy: 0.8919793963432312\n",
            "Epoch: 1/5  | Epoch loss: 2.6722114086151123  | Mean train accuracy: 0.8919861316680908\n",
            "Epoch: 1/5  | Epoch loss: 2.6719815731048584  | Mean train accuracy: 0.8919907808303833\n",
            "Epoch: 1/5  | Epoch loss: 2.671381950378418  | Mean train accuracy: 0.891994833946228\n",
            "Epoch: 1/5  | Epoch loss: 2.671321153640747  | Mean train accuracy: 0.8919997215270996\n",
            "Epoch: 1/5  | Epoch loss: 2.6710362434387207  | Mean train accuracy: 0.8920040726661682\n",
            "Epoch: 1/5  | Epoch loss: 2.6711947917938232  | Mean train accuracy: 0.8920089602470398\n",
            "Epoch: 1/5  | Epoch loss: 2.6710543632507324  | Mean train accuracy: 0.8920129537582397\n",
            "Epoch: 1/5  | Epoch loss: 2.670501470565796  | Mean train accuracy: 0.8920155763626099\n",
            "Epoch: 1/5  | Epoch loss: 2.6702511310577393  | Mean train accuracy: 0.8920179605484009\n",
            "Epoch: 1/5  | Epoch loss: 2.670261859893799  | Mean train accuracy: 0.8920197486877441\n",
            "Epoch: 1/5  | Epoch loss: 2.670076370239258  | Mean train accuracy: 0.8920226693153381\n",
            "Epoch: 1/5  | Epoch loss: 2.669541120529175  | Mean train accuracy: 0.8920243382453918\n",
            "Epoch: 1/5  | Epoch loss: 2.6689999103546143  | Mean train accuracy: 0.8920244574546814\n",
            "Epoch: 1/5  | Epoch loss: 2.668877363204956  | Mean train accuracy: 0.8920257091522217\n",
            "Epoch: 1/5  | Epoch loss: 2.6688153743743896  | Mean train accuracy: 0.8920256495475769\n",
            "Epoch: 1/5  | Epoch loss: 2.6689815521240234  | Mean train accuracy: 0.8920231461524963\n",
            "Epoch: 1/5  | Epoch loss: 2.6690266132354736  | Mean train accuracy: 0.8920219540596008\n",
            "Epoch: 1/5  | Epoch loss: 2.66902232170105  | Mean train accuracy: 0.8920194506645203\n",
            "Epoch: 1/5  | Epoch loss: 2.668329954147339  | Mean train accuracy: 0.8920156955718994\n",
            "Epoch: 1/5  | Epoch loss: 2.667830228805542  | Mean train accuracy: 0.8920124173164368\n",
            "Epoch: 1/5  | Epoch loss: 2.6673245429992676  | Mean train accuracy: 0.8920082449913025\n",
            "Epoch: 1/5  | Epoch loss: 2.6667416095733643  | Mean train accuracy: 0.8920058608055115\n",
            "Epoch: 1/5  | Epoch loss: 2.666983127593994  | Mean train accuracy: 0.8920029997825623\n",
            "Epoch: 1/5  | Epoch loss: 2.666823148727417  | Mean train accuracy: 0.8920013308525085\n",
            "Epoch: 1/5  | Epoch loss: 2.6668272018432617  | Mean train accuracy: 0.8919976353645325\n",
            "Epoch: 1/5  | Epoch loss: 2.666996479034424  | Mean train accuracy: 0.8919939398765564\n",
            "Epoch: 1/5  | Epoch loss: 2.6667075157165527  | Mean train accuracy: 0.891991138458252\n",
            "Epoch: 1/5  | Epoch loss: 2.6666111946105957  | Mean train accuracy: 0.8919907212257385\n",
            "Epoch: 1/5  | Epoch loss: 2.6663684844970703  | Mean train accuracy: 0.8919885754585266\n",
            "Epoch: 1/5  | Epoch loss: 2.666393756866455  | Mean train accuracy: 0.891987144947052\n",
            "Epoch: 1/5  | Epoch loss: 2.666125535964966  | Mean train accuracy: 0.891986072063446\n",
            "Epoch: 1/5  | Epoch loss: 2.6660573482513428  | Mean train accuracy: 0.8919870853424072\n",
            "Epoch: 1/5  | Epoch loss: 2.6660354137420654  | Mean train accuracy: 0.8919874429702759\n",
            "Epoch: 1/5  | Epoch loss: 2.6659767627716064  | Mean train accuracy: 0.8919894695281982\n",
            "Epoch: 1/5  | Epoch loss: 2.6656646728515625  | Mean train accuracy: 0.8919901251792908\n",
            "Epoch: 1/5  | Epoch loss: 2.6653172969818115  | Mean train accuracy: 0.891991376876831\n",
            "Epoch: 1/5  | Epoch loss: 2.665064811706543  | Mean train accuracy: 0.891992449760437\n",
            "Epoch: 1/5  | Epoch loss: 2.664665460586548  | Mean train accuracy: 0.8919945955276489\n",
            "Epoch: 1/5  | Epoch loss: 2.6639974117279053  | Mean train accuracy: 0.8919978141784668\n",
            "Epoch: 1/5  | Epoch loss: 2.6636810302734375  | Mean train accuracy: 0.8920009136199951\n",
            "Epoch: 1/5  | Epoch loss: 2.6635262966156006  | Mean train accuracy: 0.892002284526825\n",
            "Epoch: 1/5  | Epoch loss: 2.663726568222046  | Mean train accuracy: 0.8920053839683533\n",
            "Epoch: 1/5  | Epoch loss: 2.663623809814453  | Mean train accuracy: 0.8920087814331055\n",
            "Epoch: 1/5  | Epoch loss: 2.663573741912842  | Mean train accuracy: 0.8920136094093323\n",
            "Epoch: 1/5  | Epoch loss: 2.663410186767578  | Mean train accuracy: 0.8920175433158875\n",
            "Epoch: 1/5  | Epoch loss: 2.66318941116333  | Mean train accuracy: 0.892021894454956\n",
            "Epoch: 1/5  | Epoch loss: 2.663270950317383  | Mean train accuracy: 0.8920267820358276\n",
            "Epoch: 1/5  | Epoch loss: 2.662963628768921  | Mean train accuracy: 0.8920322060585022\n",
            "Epoch: 1/5  | Epoch loss: 2.662982225418091  | Mean train accuracy: 0.8920384645462036\n",
            "Epoch: 1/5  | Epoch loss: 2.662909746170044  | Mean train accuracy: 0.8920445442199707\n",
            "Epoch: 1/5  | Epoch loss: 2.6626996994018555  | Mean train accuracy: 0.8920505046844482\n",
            "Epoch: 1/5  | Epoch loss: 2.662752628326416  | Mean train accuracy: 0.8920574188232422\n",
            "Epoch: 1/5  | Epoch loss: 2.6626086235046387  | Mean train accuracy: 0.8920634984970093\n",
            "Epoch: 1/5  | Epoch loss: 2.6621787548065186  | Mean train accuracy: 0.8920708298683167\n",
            "Epoch: 1/5  | Epoch loss: 2.6621198654174805  | Mean train accuracy: 0.8920772075653076\n",
            "Epoch: 1/5  | Epoch loss: 2.6619300842285156  | Mean train accuracy: 0.8920819163322449\n",
            "Epoch: 1/5  | Epoch loss: 2.6619269847869873  | Mean train accuracy: 0.8920868635177612\n",
            "Epoch: 1/5  | Epoch loss: 2.6615841388702393  | Mean train accuracy: 0.8920926451683044\n",
            "Epoch: 1/5  | Epoch loss: 2.661907911300659  | Mean train accuracy: 0.8920977115631104\n",
            "Epoch: 1/5  | Epoch loss: 2.6617279052734375  | Mean train accuracy: 0.8921035528182983\n",
            "Epoch: 1/5  | Epoch loss: 2.661663770675659  | Mean train accuracy: 0.8921093344688416\n",
            "Epoch: 1/5  | Epoch loss: 2.6613857746124268  | Mean train accuracy: 0.8921130299568176\n",
            "Epoch: 1/5  | Epoch loss: 2.661512851715088  | Mean train accuracy: 0.892116904258728\n",
            "Epoch: 1/5  | Epoch loss: 2.661125898361206  | Mean train accuracy: 0.8921219706535339\n",
            "Epoch: 1/5  | Epoch loss: 2.6612274646759033  | Mean train accuracy: 0.8921273350715637\n",
            "Epoch: 1/5  | Epoch loss: 2.661062002182007  | Mean train accuracy: 0.8921321034431458\n",
            "Epoch: 1/5  | Epoch loss: 2.6609137058258057  | Mean train accuracy: 0.8921359181404114\n",
            "Epoch: 1/5  | Epoch loss: 2.6609842777252197  | Mean train accuracy: 0.892139732837677\n",
            "Epoch: 1/5  | Epoch loss: 2.6610312461853027  | Mean train accuracy: 0.8921433687210083\n",
            "Epoch: 1/5  | Epoch loss: 2.661015510559082  | Mean train accuracy: 0.8921482563018799\n",
            "Epoch: 1/5  | Epoch loss: 2.66070294380188  | Mean train accuracy: 0.8921523094177246\n",
            "Epoch: 1/5  | Epoch loss: 2.6607728004455566  | Mean train accuracy: 0.8921555280685425\n",
            "Epoch: 1/5  | Epoch loss: 2.6606221199035645  | Mean train accuracy: 0.8921590447425842\n",
            "Epoch: 1/5  | Epoch loss: 2.6605663299560547  | Mean train accuracy: 0.8921643495559692\n",
            "Epoch: 1/5  | Epoch loss: 2.660404682159424  | Mean train accuracy: 0.8921681046485901\n",
            "Epoch: 1/5  | Epoch loss: 2.660108804702759  | Mean train accuracy: 0.8921713829040527\n",
            "Epoch: 1/5  | Epoch loss: 2.65960955619812  | Mean train accuracy: 0.8921743035316467\n",
            "Epoch: 1/5  | Epoch loss: 2.659611701965332  | Mean train accuracy: 0.8921768069267273\n",
            "Epoch: 1/5  | Epoch loss: 2.659327983856201  | Mean train accuracy: 0.8921796679496765\n",
            "Epoch: 1/5  | Epoch loss: 2.6591763496398926  | Mean train accuracy: 0.8921828269958496\n",
            "Epoch: 1/5  | Epoch loss: 2.659008502960205  | Mean train accuracy: 0.8921860456466675\n",
            "Epoch: 1/5  | Epoch loss: 2.659102201461792  | Mean train accuracy: 0.892188310623169\n",
            "Epoch: 1/5  | Epoch loss: 2.6590962409973145  | Mean train accuracy: 0.8921898007392883\n",
            "Epoch: 1/5  | Epoch loss: 2.659010887145996  | Mean train accuracy: 0.892192542552948\n",
            "Epoch: 1/5  | Epoch loss: 2.6588644981384277  | Mean train accuracy: 0.8921943306922913\n",
            "Epoch: 1/5  | Epoch loss: 2.658757448196411  | Mean train accuracy: 0.892196774482727\n",
            "Epoch: 1/5  | Epoch loss: 2.658504009246826  | Mean train accuracy: 0.8921986818313599\n",
            "Epoch: 1/5  | Epoch loss: 2.657944679260254  | Mean train accuracy: 0.8922003507614136\n",
            "Epoch: 1/5  | Epoch loss: 2.6577107906341553  | Mean train accuracy: 0.8922017812728882\n",
            "Epoch: 1/5  | Epoch loss: 2.657620429992676  | Mean train accuracy: 0.8922032117843628\n",
            "Epoch: 1/5  | Epoch loss: 2.6572906970977783  | Mean train accuracy: 0.892204225063324\n",
            "Epoch: 1/5  | Epoch loss: 2.657064914703369  | Mean train accuracy: 0.8922058343887329\n",
            "Epoch: 1/5  | Epoch loss: 2.6566965579986572  | Mean train accuracy: 0.8922063112258911\n",
            "Epoch: 1/5  | Epoch loss: 2.6567413806915283  | Mean train accuracy: 0.8922073245048523\n",
            "Epoch: 1/5  | Epoch loss: 2.656543731689453  | Mean train accuracy: 0.8922091126441956\n",
            "Epoch: 1/5  | Epoch loss: 2.656812906265259  | Mean train accuracy: 0.8922098875045776\n",
            "Epoch: 1/5  | Epoch loss: 2.6564202308654785  | Mean train accuracy: 0.8922115564346313\n",
            "Epoch: 1/5  | Epoch loss: 2.6561129093170166  | Mean train accuracy: 0.8922116756439209\n",
            "Epoch: 1/5  | Epoch loss: 2.6558210849761963  | Mean train accuracy: 0.8922131061553955\n",
            "Epoch: 1/5  | Epoch loss: 2.655750036239624  | Mean train accuracy: 0.8922136425971985\n",
            "Epoch: 1/5  | Epoch loss: 2.65574312210083  | Mean train accuracy: 0.8922147154808044\n",
            "Epoch: 1/5  | Epoch loss: 2.655471086502075  | Mean train accuracy: 0.892216145992279\n",
            "Epoch: 1/5  | Epoch loss: 2.6553852558135986  | Mean train accuracy: 0.8922187089920044\n",
            "Epoch: 1/5  | Epoch loss: 2.6551995277404785  | Mean train accuracy: 0.8922216892242432\n",
            "Epoch: 1/5  | Epoch loss: 2.6553232669830322  | Mean train accuracy: 0.8922232985496521\n",
            "Epoch: 1/5  | Epoch loss: 2.6553194522857666  | Mean train accuracy: 0.8922256231307983\n",
            "Epoch: 1/5  | Epoch loss: 2.655024766921997  | Mean train accuracy: 0.8922284245491028\n",
            "Epoch: 1/5  | Epoch loss: 2.6548008918762207  | Mean train accuracy: 0.8922305703163147\n",
            "Epoch: 1/5  | Epoch loss: 2.6551547050476074  | Mean train accuracy: 0.8922330141067505\n",
            "Epoch: 1/5  | Epoch loss: 2.655308961868286  | Mean train accuracy: 0.8922357559204102\n",
            "Epoch: 1/5  | Epoch loss: 2.6554441452026367  | Mean train accuracy: 0.8922379016876221\n",
            "Epoch: 1/5  | Epoch loss: 2.6554136276245117  | Mean train accuracy: 0.8922402858734131\n",
            "Epoch: 1/5  | Epoch loss: 2.6553306579589844  | Mean train accuracy: 0.8922430276870728\n",
            "Epoch: 1/5  | Epoch loss: 2.6552884578704834  | Mean train accuracy: 0.8922458291053772\n",
            "Epoch: 1/5  | Epoch loss: 2.655200242996216  | Mean train accuracy: 0.8922471404075623\n",
            "Epoch: 1/5  | Epoch loss: 2.6551616191864014  | Mean train accuracy: 0.8922499418258667\n",
            "Epoch: 1/5  | Epoch loss: 2.6554250717163086  | Mean train accuracy: 0.8922522664070129\n",
            "Epoch: 1/5  | Epoch loss: 2.655491352081299  | Mean train accuracy: 0.8922544717788696\n",
            "Epoch: 1/5  | Epoch loss: 2.6558451652526855  | Mean train accuracy: 0.8922549486160278\n",
            "Epoch: 1/5  | Epoch loss: 2.655689001083374  | Mean train accuracy: 0.8922547101974487\n",
            "Epoch: 1/5  | Epoch loss: 2.655524730682373  | Mean train accuracy: 0.8922544717788696\n",
            "Epoch: 1/5  | Epoch loss: 2.6556968688964844  | Mean train accuracy: 0.892253577709198\n",
            "Epoch: 1/5  | Epoch loss: 2.655566692352295  | Mean train accuracy: 0.8922523260116577\n",
            "Epoch: 1/5  | Epoch loss: 2.655198574066162  | Mean train accuracy: 0.8922505974769592\n",
            "Epoch: 1/5  | Epoch loss: 2.65496563911438  | Mean train accuracy: 0.8922484517097473\n",
            "Epoch: 1/5  | Epoch loss: 2.6551029682159424  | Mean train accuracy: 0.8922468423843384\n",
            "Epoch: 1/5  | Epoch loss: 2.6550815105438232  | Mean train accuracy: 0.8922434449195862\n",
            "Epoch: 1/5  | Epoch loss: 2.6549410820007324  | Mean train accuracy: 0.8922411203384399\n",
            "Epoch: 1/5  | Epoch loss: 2.6546974182128906  | Mean train accuracy: 0.8922393918037415\n",
            "Epoch: 1/5  | Epoch loss: 2.654470443725586  | Mean train accuracy: 0.8922365307807922\n",
            "Epoch: 1/5  | Epoch loss: 2.654231548309326  | Mean train accuracy: 0.892234206199646\n",
            "Epoch: 1/5  | Epoch loss: 2.654132127761841  | Mean train accuracy: 0.8922314047813416\n",
            "Epoch: 1/5  | Epoch loss: 2.6542305946350098  | Mean train accuracy: 0.8922286033630371\n",
            "Epoch: 1/5  | Epoch loss: 2.653709650039673  | Mean train accuracy: 0.8922253847122192\n",
            "Epoch: 1/5  | Epoch loss: 2.653233289718628  | Mean train accuracy: 0.8922231197357178\n",
            "Epoch: 1/5  | Epoch loss: 2.6529481410980225  | Mean train accuracy: 0.8922206163406372\n",
            "Epoch: 1/5  | Epoch loss: 2.6529219150543213  | Mean train accuracy: 0.8922191262245178\n",
            "Epoch: 1/5  | Epoch loss: 2.652780771255493  | Mean train accuracy: 0.8922175168991089\n",
            "Epoch: 1/5  | Epoch loss: 2.652339220046997  | Mean train accuracy: 0.8922160267829895\n",
            "Epoch: 1/5  | Epoch loss: 2.652092695236206  | Mean train accuracy: 0.8922149538993835\n",
            "Epoch: 1/5  | Epoch loss: 2.6519834995269775  | Mean train accuracy: 0.8922142386436462\n",
            "Epoch: 1/5  | Epoch loss: 2.6518566608428955  | Mean train accuracy: 0.8922144174575806\n",
            "Epoch: 1/5  | Epoch loss: 2.65156626701355  | Mean train accuracy: 0.8922156691551208\n",
            "Epoch: 1/5  | Epoch loss: 2.651657819747925  | Mean train accuracy: 0.8922160267829895\n",
            "Epoch: 1/5  | Epoch loss: 2.6518704891204834  | Mean train accuracy: 0.8922181129455566\n",
            "Epoch: 1/5  | Epoch loss: 2.6515650749206543  | Mean train accuracy: 0.8922196626663208\n",
            "Epoch: 1/5  | Epoch loss: 2.6514482498168945  | Mean train accuracy: 0.8922217488288879\n",
            "Epoch: 1/5  | Epoch loss: 2.6512045860290527  | Mean train accuracy: 0.8922238945960999\n",
            "Epoch: 1/5  | Epoch loss: 2.6509218215942383  | Mean train accuracy: 0.8922252058982849\n",
            "Epoch: 1/5  | Epoch loss: 2.6509687900543213  | Mean train accuracy: 0.892227053642273\n",
            "Epoch: 1/5  | Epoch loss: 2.6507675647735596  | Mean train accuracy: 0.8922303318977356\n",
            "Epoch: 1/5  | Epoch loss: 2.6504526138305664  | Mean train accuracy: 0.8922340273857117\n",
            "Epoch: 1/5  | Epoch loss: 2.6502819061279297  | Mean train accuracy: 0.8922382593154907\n",
            "Epoch: 1/5  | Epoch loss: 2.6504008769989014  | Mean train accuracy: 0.8922428488731384\n",
            "Epoch: 1/5  | Epoch loss: 2.650224208831787  | Mean train accuracy: 0.8922476172447205\n",
            "Epoch: 1/5  | Epoch loss: 2.6500086784362793  | Mean train accuracy: 0.8922532200813293\n",
            "Epoch: 1/5  | Epoch loss: 2.6497817039489746  | Mean train accuracy: 0.8922585844993591\n",
            "Epoch: 1/5  | Epoch loss: 2.6498963832855225  | Mean train accuracy: 0.8922641277313232\n",
            "Epoch: 1/5  | Epoch loss: 2.649507522583008  | Mean train accuracy: 0.8922696709632874\n",
            "Epoch: 1/5  | Epoch loss: 2.649552822113037  | Mean train accuracy: 0.8922745585441589\n",
            "Epoch: 1/5  | Epoch loss: 2.64917254447937  | Mean train accuracy: 0.8922801613807678\n",
            "Epoch: 1/5  | Epoch loss: 2.6488142013549805  | Mean train accuracy: 0.8922861814498901\n",
            "Epoch: 1/5  | Epoch loss: 2.649016857147217  | Mean train accuracy: 0.8922916650772095\n",
            "Epoch: 1/5  | Epoch loss: 2.648899793624878  | Mean train accuracy: 0.8922967910766602\n",
            "Epoch: 1/5  | Epoch loss: 2.6486880779266357  | Mean train accuracy: 0.8923022150993347\n",
            "Epoch: 1/5  | Epoch loss: 2.648585081100464  | Mean train accuracy: 0.8923068046569824\n",
            "Epoch: 1/5  | Epoch loss: 2.648364782333374  | Mean train accuracy: 0.8923125863075256\n",
            "Epoch: 1/5  | Epoch loss: 2.6483070850372314  | Mean train accuracy: 0.8923161625862122\n",
            "Epoch: 1/5  | Epoch loss: 2.648301601409912  | Mean train accuracy: 0.8923200964927673\n",
            "Epoch: 1/5  | Epoch loss: 2.648142099380493  | Mean train accuracy: 0.892322838306427\n",
            "Epoch: 1/5  | Epoch loss: 2.6483192443847656  | Mean train accuracy: 0.8923252820968628\n",
            "Epoch: 1/5  | Epoch loss: 2.648082733154297  | Mean train accuracy: 0.8923276662826538\n",
            "Epoch: 1/5  | Epoch loss: 2.6484365463256836  | Mean train accuracy: 0.892329216003418\n",
            "Epoch: 1/5  | Epoch loss: 2.6481096744537354  | Mean train accuracy: 0.8923301696777344\n",
            "Epoch: 1/5  | Epoch loss: 2.647880792617798  | Mean train accuracy: 0.8923299312591553\n",
            "Epoch: 1/5  | Epoch loss: 2.64786434173584  | Mean train accuracy: 0.8923296928405762\n",
            "Epoch: 1/5  | Epoch loss: 2.6478819847106934  | Mean train accuracy: 0.8923287987709045\n",
            "Epoch: 1/5  | Epoch loss: 2.647840738296509  | Mean train accuracy: 0.8923262357711792\n",
            "Epoch: 1/5  | Epoch loss: 2.6477034091949463  | Mean train accuracy: 0.8923243880271912\n",
            "Epoch: 1/5  | Epoch loss: 2.647880792617798  | Mean train accuracy: 0.8923216462135315\n",
            "Epoch: 1/5  | Epoch loss: 2.6478824615478516  | Mean train accuracy: 0.8923190236091614\n",
            "Epoch: 1/5  | Epoch loss: 2.6478755474090576  | Mean train accuracy: 0.8923160433769226\n",
            "Epoch: 1/5  | Epoch loss: 2.64772367477417  | Mean train accuracy: 0.8923130631446838\n",
            "Epoch: 1/5  | Epoch loss: 2.647883892059326  | Mean train accuracy: 0.8923090696334839\n",
            "Epoch: 1/5  | Epoch loss: 2.647932291030884  | Mean train accuracy: 0.8923051953315735\n",
            "Epoch: 1/5  | Epoch loss: 2.6478588581085205  | Mean train accuracy: 0.8923011422157288\n",
            "Epoch: 1/5  | Epoch loss: 2.6474807262420654  | Mean train accuracy: 0.8922984600067139\n",
            "Epoch: 1/5  | Epoch loss: 2.6475911140441895  | Mean train accuracy: 0.8922951817512512\n",
            "Epoch: 1/5  | Epoch loss: 2.6471755504608154  | Mean train accuracy: 0.8922917246818542\n",
            "Epoch: 1/5  | Epoch loss: 2.6473019123077393  | Mean train accuracy: 0.892288863658905\n",
            "Epoch: 1/5  | Epoch loss: 2.647043466567993  | Mean train accuracy: 0.8922868371009827\n",
            "Epoch: 1/5  | Epoch loss: 2.647132635116577  | Mean train accuracy: 0.8922844529151917\n",
            "Epoch: 1/5  | Epoch loss: 2.647054433822632  | Mean train accuracy: 0.8922819495201111\n",
            "Epoch: 1/5  | Epoch loss: 2.646916151046753  | Mean train accuracy: 0.8922795057296753\n",
            "Epoch: 1/5  | Epoch loss: 2.6467225551605225  | Mean train accuracy: 0.8922767043113708\n",
            "Epoch: 1/5  | Epoch loss: 2.646651268005371  | Mean train accuracy: 0.8922743201255798\n",
            "Epoch: 1/5  | Epoch loss: 2.64665150642395  | Mean train accuracy: 0.8922706246376038\n",
            "Epoch: 1/5  | Epoch loss: 2.6466257572174072  | Mean train accuracy: 0.8922668099403381\n",
            "Epoch: 1/5  | Epoch loss: 2.646279811859131  | Mean train accuracy: 0.8922630548477173\n",
            "Epoch: 1/5  | Epoch loss: 2.646296262741089  | Mean train accuracy: 0.8922587037086487\n",
            "Epoch: 1/5  | Epoch loss: 2.6463170051574707  | Mean train accuracy: 0.8922531604766846\n",
            "Epoch: 1/5  | Epoch loss: 2.646313190460205  | Mean train accuracy: 0.8922479748725891\n",
            "Epoch: 1/5  | Epoch loss: 2.646419048309326  | Mean train accuracy: 0.8922420144081116\n",
            "Epoch: 1/5  | Epoch loss: 2.646305561065674  | Mean train accuracy: 0.8922373652458191\n",
            "Epoch: 1/5  | Epoch loss: 2.645948648452759  | Mean train accuracy: 0.8922327160835266\n",
            "Epoch: 1/5  | Epoch loss: 2.646124839782715  | Mean train accuracy: 0.8922275304794312\n",
            "Epoch: 1/5  | Epoch loss: 2.646174430847168  | Mean train accuracy: 0.8922222256660461\n",
            "Epoch: 1/5  | Epoch loss: 2.6457369327545166  | Mean train accuracy: 0.8922176957130432\n",
            "Epoch: 1/5  | Epoch loss: 2.6457412242889404  | Mean train accuracy: 0.8922115564346313\n",
            "Epoch: 1/5  | Epoch loss: 2.6457619667053223  | Mean train accuracy: 0.8922064304351807\n",
            "Epoch: 1/5  | Epoch loss: 2.645765542984009  | Mean train accuracy: 0.8922014832496643\n",
            "Epoch: 1/5  | Epoch loss: 2.6460487842559814  | Mean train accuracy: 0.8921958804130554\n",
            "Epoch: 1/5  | Epoch loss: 2.6460540294647217  | Mean train accuracy: 0.8921914100646973\n",
            "Epoch: 1/5  | Epoch loss: 2.6460373401641846  | Mean train accuracy: 0.8921867609024048\n",
            "Epoch: 1/5  | Epoch loss: 2.645928382873535  | Mean train accuracy: 0.8921816945075989\n",
            "Epoch: 1/5  | Epoch loss: 2.645878553390503  | Mean train accuracy: 0.892176628112793\n",
            "Epoch: 1/5  | Epoch loss: 2.6458871364593506  | Mean train accuracy: 0.8921723961830139\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "815f5e41961a44fcb366c9bcb6732668",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=655), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 2/5  | Epoch loss: 2.5534605979919434  | Mean train accuracy: 0.8891810774803162\n",
            "Epoch: 2/5  | Epoch loss: 2.537715196609497  | Mean train accuracy: 0.8889168500900269\n",
            "Epoch: 2/5  | Epoch loss: 2.5774247646331787  | Mean train accuracy: 0.8889705538749695\n",
            "Epoch: 2/5  | Epoch loss: 2.600968599319458  | Mean train accuracy: 0.8891230821609497\n",
            "Epoch: 2/5  | Epoch loss: 2.6073737144470215  | Mean train accuracy: 0.8890237808227539\n",
            "Epoch: 2/5  | Epoch loss: 2.602677822113037  | Mean train accuracy: 0.8889705538749695\n",
            "Epoch: 2/5  | Epoch loss: 2.6000869274139404  | Mean train accuracy: 0.8890079855918884\n",
            "Epoch: 2/5  | Epoch loss: 2.6011781692504883  | Mean train accuracy: 0.888918399810791\n",
            "Epoch: 2/5  | Epoch loss: 2.615891933441162  | Mean train accuracy: 0.88895183801651\n",
            "Epoch: 2/5  | Epoch loss: 2.59487247467041  | Mean train accuracy: 0.8890186548233032\n",
            "Epoch: 2/5  | Epoch loss: 2.6031267642974854  | Mean train accuracy: 0.8889970779418945\n",
            "Epoch: 2/5  | Epoch loss: 2.6061527729034424  | Mean train accuracy: 0.8889995217323303\n",
            "Epoch: 2/5  | Epoch loss: 2.60890793800354  | Mean train accuracy: 0.8890214562416077\n",
            "Epoch: 2/5  | Epoch loss: 2.6159586906433105  | Mean train accuracy: 0.8890393376350403\n",
            "Epoch: 2/5  | Epoch loss: 2.6013259887695312  | Mean train accuracy: 0.8890367746353149\n",
            "Epoch: 2/5  | Epoch loss: 2.597510814666748  | Mean train accuracy: 0.889026403427124\n",
            "Epoch: 2/5  | Epoch loss: 2.596745491027832  | Mean train accuracy: 0.8890135288238525\n",
            "Epoch: 2/5  | Epoch loss: 2.590517282485962  | Mean train accuracy: 0.8889862895011902\n",
            "Epoch: 2/5  | Epoch loss: 2.6010520458221436  | Mean train accuracy: 0.8889945149421692\n",
            "Epoch: 2/5  | Epoch loss: 2.587278127670288  | Mean train accuracy: 0.8889825940132141\n",
            "Epoch: 2/5  | Epoch loss: 2.592137575149536  | Mean train accuracy: 0.8889619708061218\n",
            "Epoch: 2/5  | Epoch loss: 2.595735788345337  | Mean train accuracy: 0.8889343738555908\n",
            "Epoch: 2/5  | Epoch loss: 2.601364850997925  | Mean train accuracy: 0.8888835310935974\n",
            "Epoch: 2/5  | Epoch loss: 2.6004509925842285  | Mean train accuracy: 0.8888373374938965\n",
            "Epoch: 2/5  | Epoch loss: 2.600762367248535  | Mean train accuracy: 0.8888010382652283\n",
            "Epoch: 2/5  | Epoch loss: 2.6039962768554688  | Mean train accuracy: 0.8887462615966797\n",
            "Epoch: 2/5  | Epoch loss: 2.606264352798462  | Mean train accuracy: 0.8887141346931458\n",
            "Epoch: 2/5  | Epoch loss: 2.610975742340088  | Mean train accuracy: 0.888671875\n",
            "Epoch: 2/5  | Epoch loss: 2.612797260284424  | Mean train accuracy: 0.8886476755142212\n",
            "Epoch: 2/5  | Epoch loss: 2.615135669708252  | Mean train accuracy: 0.8886220455169678\n",
            "Epoch: 2/5  | Epoch loss: 2.615227222442627  | Mean train accuracy: 0.8885835409164429\n",
            "Epoch: 2/5  | Epoch loss: 2.613480567932129  | Mean train accuracy: 0.8885611295700073\n",
            "Epoch: 2/5  | Epoch loss: 2.6155738830566406  | Mean train accuracy: 0.8885400891304016\n",
            "Epoch: 2/5  | Epoch loss: 2.6125521659851074  | Mean train accuracy: 0.8885248303413391\n",
            "Epoch: 2/5  | Epoch loss: 2.6126487255096436  | Mean train accuracy: 0.888495683670044\n",
            "Epoch: 2/5  | Epoch loss: 2.6061618328094482  | Mean train accuracy: 0.8884885907173157\n",
            "Epoch: 2/5  | Epoch loss: 2.6045000553131104  | Mean train accuracy: 0.8884909152984619\n",
            "Epoch: 2/5  | Epoch loss: 2.6004860401153564  | Mean train accuracy: 0.8884809613227844\n",
            "Epoch: 2/5  | Epoch loss: 2.6032185554504395  | Mean train accuracy: 0.8884863257408142\n",
            "Epoch: 2/5  | Epoch loss: 2.6055991649627686  | Mean train accuracy: 0.8885013461112976\n",
            "Epoch: 2/5  | Epoch loss: 2.6026861667633057  | Mean train accuracy: 0.8885157704353333\n",
            "Epoch: 2/5  | Epoch loss: 2.6022441387176514  | Mean train accuracy: 0.8885285258293152\n",
            "Epoch: 2/5  | Epoch loss: 2.6030759811401367  | Mean train accuracy: 0.8885257840156555\n",
            "Epoch: 2/5  | Epoch loss: 2.601255178451538  | Mean train accuracy: 0.8885394930839539\n",
            "Epoch: 2/5  | Epoch loss: 2.6011807918548584  | Mean train accuracy: 0.8885520100593567\n",
            "Epoch: 2/5  | Epoch loss: 2.6040101051330566  | Mean train accuracy: 0.8885547518730164\n",
            "Epoch: 2/5  | Epoch loss: 2.6039795875549316  | Mean train accuracy: 0.8885639905929565\n",
            "Epoch: 2/5  | Epoch loss: 2.60554575920105  | Mean train accuracy: 0.8885787129402161\n",
            "Epoch: 2/5  | Epoch loss: 2.603891134262085  | Mean train accuracy: 0.8885918259620667\n",
            "Epoch: 2/5  | Epoch loss: 2.602437973022461  | Mean train accuracy: 0.8885997533798218\n",
            "Epoch: 2/5  | Epoch loss: 2.6047303676605225  | Mean train accuracy: 0.8886038064956665\n",
            "Epoch: 2/5  | Epoch loss: 2.609042167663574  | Mean train accuracy: 0.8886188864707947\n",
            "Epoch: 2/5  | Epoch loss: 2.611320734024048  | Mean train accuracy: 0.8886292576789856\n",
            "Epoch: 2/5  | Epoch loss: 2.6111834049224854  | Mean train accuracy: 0.8886383175849915\n",
            "Epoch: 2/5  | Epoch loss: 2.6072840690612793  | Mean train accuracy: 0.8886617422103882\n",
            "Epoch: 2/5  | Epoch loss: 2.608027935028076  | Mean train accuracy: 0.8886709809303284\n",
            "Epoch: 2/5  | Epoch loss: 2.6058177947998047  | Mean train accuracy: 0.8886733651161194\n",
            "Epoch: 2/5  | Epoch loss: 2.606961727142334  | Mean train accuracy: 0.8886914253234863\n",
            "Epoch: 2/5  | Epoch loss: 2.6086909770965576  | Mean train accuracy: 0.8887060284614563\n",
            "Epoch: 2/5  | Epoch loss: 2.6063125133514404  | Mean train accuracy: 0.8887255191802979\n",
            "Epoch: 2/5  | Epoch loss: 2.6035521030426025  | Mean train accuracy: 0.8887554407119751\n",
            "Epoch: 2/5  | Epoch loss: 2.6016077995300293  | Mean train accuracy: 0.8887861967086792\n",
            "Epoch: 2/5  | Epoch loss: 2.602600574493408  | Mean train accuracy: 0.8888090252876282\n",
            "Epoch: 2/5  | Epoch loss: 2.602830410003662  | Mean train accuracy: 0.8888352513313293\n",
            "Epoch: 2/5  | Epoch loss: 2.6040961742401123  | Mean train accuracy: 0.888861358165741\n",
            "Epoch: 2/5  | Epoch loss: 2.606393575668335  | Mean train accuracy: 0.8888916373252869\n",
            "Epoch: 2/5  | Epoch loss: 2.604097604751587  | Mean train accuracy: 0.8889163732528687\n",
            "Epoch: 2/5  | Epoch loss: 2.604780435562134  | Mean train accuracy: 0.8889443278312683\n",
            "Epoch: 2/5  | Epoch loss: 2.6031689643859863  | Mean train accuracy: 0.8889776468276978\n",
            "Epoch: 2/5  | Epoch loss: 2.60278582572937  | Mean train accuracy: 0.888999342918396\n",
            "Epoch: 2/5  | Epoch loss: 2.6032471656799316  | Mean train accuracy: 0.8890241980552673\n",
            "Epoch: 2/5  | Epoch loss: 2.603149890899658  | Mean train accuracy: 0.8890482783317566\n",
            "Epoch: 2/5  | Epoch loss: 2.601435899734497  | Mean train accuracy: 0.8890843987464905\n",
            "Epoch: 2/5  | Epoch loss: 2.5990381240844727  | Mean train accuracy: 0.8891229629516602\n",
            "Epoch: 2/5  | Epoch loss: 2.5968191623687744  | Mean train accuracy: 0.8891432881355286\n",
            "Epoch: 2/5  | Epoch loss: 2.595698356628418  | Mean train accuracy: 0.8891642689704895\n",
            "Epoch: 2/5  | Epoch loss: 2.597277879714966  | Mean train accuracy: 0.8891939520835876\n",
            "Epoch: 2/5  | Epoch loss: 2.5969743728637695  | Mean train accuracy: 0.8892155885696411\n",
            "Epoch: 2/5  | Epoch loss: 2.595031261444092  | Mean train accuracy: 0.8892422318458557\n",
            "Epoch: 2/5  | Epoch loss: 2.598005533218384  | Mean train accuracy: 0.8892631530761719\n",
            "Epoch: 2/5  | Epoch loss: 2.5982632637023926  | Mean train accuracy: 0.8892812728881836\n",
            "Epoch: 2/5  | Epoch loss: 2.597844123840332  | Mean train accuracy: 0.8893056511878967\n",
            "Epoch: 2/5  | Epoch loss: 2.598147392272949  | Mean train accuracy: 0.889326274394989\n",
            "Epoch: 2/5  | Epoch loss: 2.5967655181884766  | Mean train accuracy: 0.8893499970436096\n",
            "Epoch: 2/5  | Epoch loss: 2.5962679386138916  | Mean train accuracy: 0.8893654346466064\n",
            "Epoch: 2/5  | Epoch loss: 2.59584903717041  | Mean train accuracy: 0.8893779516220093\n",
            "Epoch: 2/5  | Epoch loss: 2.5966482162475586  | Mean train accuracy: 0.8893968462944031\n",
            "Epoch: 2/5  | Epoch loss: 2.5950820446014404  | Mean train accuracy: 0.8894181847572327\n",
            "Epoch: 2/5  | Epoch loss: 2.595886468887329  | Mean train accuracy: 0.8894330263137817\n",
            "Epoch: 2/5  | Epoch loss: 2.5962064266204834  | Mean train accuracy: 0.8894524574279785\n",
            "Epoch: 2/5  | Epoch loss: 2.596069574356079  | Mean train accuracy: 0.8894731402397156\n",
            "Epoch: 2/5  | Epoch loss: 2.5958471298217773  | Mean train accuracy: 0.8894888758659363\n",
            "Epoch: 2/5  | Epoch loss: 2.595027208328247  | Mean train accuracy: 0.8895048499107361\n",
            "Epoch: 2/5  | Epoch loss: 2.5962722301483154  | Mean train accuracy: 0.8895236253738403\n",
            "Epoch: 2/5  | Epoch loss: 2.5987796783447266  | Mean train accuracy: 0.8895410895347595\n",
            "Epoch: 2/5  | Epoch loss: 2.599994659423828  | Mean train accuracy: 0.8895511627197266\n",
            "Epoch: 2/5  | Epoch loss: 2.5999703407287598  | Mean train accuracy: 0.8895607590675354\n",
            "Epoch: 2/5  | Epoch loss: 2.5999984741210938  | Mean train accuracy: 0.8895739316940308\n",
            "Epoch: 2/5  | Epoch loss: 2.600038766860962  | Mean train accuracy: 0.8895850777626038\n",
            "Epoch: 2/5  | Epoch loss: 2.598048210144043  | Mean train accuracy: 0.889599621295929\n",
            "Epoch: 2/5  | Epoch loss: 2.5998194217681885  | Mean train accuracy: 0.88960862159729\n",
            "Epoch: 2/5  | Epoch loss: 2.601503610610962  | Mean train accuracy: 0.8896194696426392\n",
            "Epoch: 2/5  | Epoch loss: 2.6013448238372803  | Mean train accuracy: 0.8896270990371704\n",
            "Epoch: 2/5  | Epoch loss: 2.60248064994812  | Mean train accuracy: 0.8896391987800598\n",
            "Epoch: 2/5  | Epoch loss: 2.601696729660034  | Mean train accuracy: 0.889643669128418\n",
            "Epoch: 2/5  | Epoch loss: 2.5991530418395996  | Mean train accuracy: 0.8896512985229492\n",
            "Epoch: 2/5  | Epoch loss: 2.598270893096924  | Mean train accuracy: 0.8896570801734924\n",
            "Epoch: 2/5  | Epoch loss: 2.5971477031707764  | Mean train accuracy: 0.8896648287773132\n",
            "Epoch: 2/5  | Epoch loss: 2.59611177444458  | Mean train accuracy: 0.8896726369857788\n",
            "Epoch: 2/5  | Epoch loss: 2.596147298812866  | Mean train accuracy: 0.8896874189376831\n",
            "Epoch: 2/5  | Epoch loss: 2.5950663089752197  | Mean train accuracy: 0.889703094959259\n",
            "Epoch: 2/5  | Epoch loss: 2.59411883354187  | Mean train accuracy: 0.8897219896316528\n",
            "Epoch: 2/5  | Epoch loss: 2.595427989959717  | Mean train accuracy: 0.8897420763969421\n",
            "Epoch: 2/5  | Epoch loss: 2.596285104751587  | Mean train accuracy: 0.889763593673706\n",
            "Epoch: 2/5  | Epoch loss: 2.595273494720459  | Mean train accuracy: 0.8897823095321655\n",
            "Epoch: 2/5  | Epoch loss: 2.5951833724975586  | Mean train accuracy: 0.8898069262504578\n",
            "Epoch: 2/5  | Epoch loss: 2.595226287841797  | Mean train accuracy: 0.8898252844810486\n",
            "Epoch: 2/5  | Epoch loss: 2.5959877967834473  | Mean train accuracy: 0.8898512125015259\n",
            "Epoch: 2/5  | Epoch loss: 2.594940423965454  | Mean train accuracy: 0.8898765444755554\n",
            "Epoch: 2/5  | Epoch loss: 2.595367193222046  | Mean train accuracy: 0.8899021744728088\n",
            "Epoch: 2/5  | Epoch loss: 2.5947930812835693  | Mean train accuracy: 0.8899303078651428\n",
            "Epoch: 2/5  | Epoch loss: 2.594073534011841  | Mean train accuracy: 0.8899587988853455\n",
            "Epoch: 2/5  | Epoch loss: 2.5937411785125732  | Mean train accuracy: 0.8899798393249512\n",
            "Epoch: 2/5  | Epoch loss: 2.593533754348755  | Mean train accuracy: 0.8900034427642822\n",
            "Epoch: 2/5  | Epoch loss: 2.5949134826660156  | Mean train accuracy: 0.8900293707847595\n",
            "Epoch: 2/5  | Epoch loss: 2.5940845012664795  | Mean train accuracy: 0.8900483250617981\n",
            "Epoch: 2/5  | Epoch loss: 2.593982458114624  | Mean train accuracy: 0.8900706768035889\n",
            "Epoch: 2/5  | Epoch loss: 2.592958688735962  | Mean train accuracy: 0.8900934457778931\n",
            "Epoch: 2/5  | Epoch loss: 2.5936057567596436  | Mean train accuracy: 0.8901147246360779\n",
            "Epoch: 2/5  | Epoch loss: 2.5940537452697754  | Mean train accuracy: 0.8901394605636597\n",
            "Epoch: 2/5  | Epoch loss: 2.593090534210205  | Mean train accuracy: 0.8901633024215698\n",
            "Epoch: 2/5  | Epoch loss: 2.592790126800537  | Mean train accuracy: 0.8901830911636353\n",
            "Epoch: 2/5  | Epoch loss: 2.5924205780029297  | Mean train accuracy: 0.8902072310447693\n",
            "Epoch: 2/5  | Epoch loss: 2.592172145843506  | Mean train accuracy: 0.8902311325073242\n",
            "Epoch: 2/5  | Epoch loss: 2.591744899749756  | Mean train accuracy: 0.8902523517608643\n",
            "Epoch: 2/5  | Epoch loss: 2.590989351272583  | Mean train accuracy: 0.8902759552001953\n",
            "Epoch: 2/5  | Epoch loss: 2.5911803245544434  | Mean train accuracy: 0.8903003334999084\n",
            "Epoch: 2/5  | Epoch loss: 2.591099977493286  | Mean train accuracy: 0.8903230428695679\n",
            "Epoch: 2/5  | Epoch loss: 2.5910227298736572  | Mean train accuracy: 0.8903430700302124\n",
            "Epoch: 2/5  | Epoch loss: 2.5907251834869385  | Mean train accuracy: 0.8903672099113464\n",
            "Epoch: 2/5  | Epoch loss: 2.590127944946289  | Mean train accuracy: 0.8903881311416626\n",
            "Epoch: 2/5  | Epoch loss: 2.589940071105957  | Mean train accuracy: 0.8904080390930176\n",
            "Epoch: 2/5  | Epoch loss: 2.589308023452759  | Mean train accuracy: 0.8904294371604919\n",
            "Epoch: 2/5  | Epoch loss: 2.5893070697784424  | Mean train accuracy: 0.8904538750648499\n",
            "Epoch: 2/5  | Epoch loss: 2.5885465145111084  | Mean train accuracy: 0.8904717564582825\n",
            "Epoch: 2/5  | Epoch loss: 2.5883305072784424  | Mean train accuracy: 0.8904926776885986\n",
            "Epoch: 2/5  | Epoch loss: 2.588425397872925  | Mean train accuracy: 0.8905123472213745\n",
            "Epoch: 2/5  | Epoch loss: 2.5888314247131348  | Mean train accuracy: 0.8905274868011475\n",
            "Epoch: 2/5  | Epoch loss: 2.589609146118164  | Mean train accuracy: 0.8905448913574219\n",
            "Epoch: 2/5  | Epoch loss: 2.588639259338379  | Mean train accuracy: 0.8905631303787231\n",
            "Epoch: 2/5  | Epoch loss: 2.589115858078003  | Mean train accuracy: 0.8905820250511169\n",
            "Epoch: 2/5  | Epoch loss: 2.589141607284546  | Mean train accuracy: 0.890597403049469\n",
            "Epoch: 2/5  | Epoch loss: 2.5887398719787598  | Mean train accuracy: 0.8906124234199524\n",
            "Epoch: 2/5  | Epoch loss: 2.5881028175354004  | Mean train accuracy: 0.8906266093254089\n",
            "Epoch: 2/5  | Epoch loss: 2.587346315383911  | Mean train accuracy: 0.8906368017196655\n",
            "Epoch: 2/5  | Epoch loss: 2.5863587856292725  | Mean train accuracy: 0.8906477093696594\n",
            "Epoch: 2/5  | Epoch loss: 2.585537910461426  | Mean train accuracy: 0.8906582593917847\n",
            "Epoch: 2/5  | Epoch loss: 2.5852155685424805  | Mean train accuracy: 0.8906698226928711\n",
            "Epoch: 2/5  | Epoch loss: 2.5859029293060303  | Mean train accuracy: 0.8906762599945068\n",
            "Epoch: 2/5  | Epoch loss: 2.585519313812256  | Mean train accuracy: 0.8906809091567993\n",
            "Epoch: 2/5  | Epoch loss: 2.5857326984405518  | Mean train accuracy: 0.8906843066215515\n",
            "Epoch: 2/5  | Epoch loss: 2.5861828327178955  | Mean train accuracy: 0.8906899690628052\n",
            "Epoch: 2/5  | Epoch loss: 2.5858993530273438  | Mean train accuracy: 0.8906957507133484\n",
            "Epoch: 2/5  | Epoch loss: 2.5855369567871094  | Mean train accuracy: 0.8906999826431274\n",
            "Epoch: 2/5  | Epoch loss: 2.5856168270111084  | Mean train accuracy: 0.890701174736023\n",
            "Epoch: 2/5  | Epoch loss: 2.5859763622283936  | Mean train accuracy: 0.8907051682472229\n",
            "Epoch: 2/5  | Epoch loss: 2.585362672805786  | Mean train accuracy: 0.8907074332237244\n",
            "Epoch: 2/5  | Epoch loss: 2.584674596786499  | Mean train accuracy: 0.8907103538513184\n",
            "Epoch: 2/5  | Epoch loss: 2.584399461746216  | Mean train accuracy: 0.8907122015953064\n",
            "Epoch: 2/5  | Epoch loss: 2.584540843963623  | Mean train accuracy: 0.8907145857810974\n",
            "Epoch: 2/5  | Epoch loss: 2.583920955657959  | Mean train accuracy: 0.8907161951065063\n",
            "Epoch: 2/5  | Epoch loss: 2.5825936794281006  | Mean train accuracy: 0.8907207250595093\n",
            "Epoch: 2/5  | Epoch loss: 2.582590341567993  | Mean train accuracy: 0.8907257318496704\n",
            "Epoch: 2/5  | Epoch loss: 2.5825722217559814  | Mean train accuracy: 0.8907314538955688\n",
            "Epoch: 2/5  | Epoch loss: 2.5819056034088135  | Mean train accuracy: 0.8907368779182434\n",
            "Epoch: 2/5  | Epoch loss: 2.580733299255371  | Mean train accuracy: 0.8907430768013\n",
            "Epoch: 2/5  | Epoch loss: 2.580566167831421  | Mean train accuracy: 0.8907464742660522\n",
            "Epoch: 2/5  | Epoch loss: 2.5796170234680176  | Mean train accuracy: 0.8907530903816223\n",
            "Epoch: 2/5  | Epoch loss: 2.5792744159698486  | Mean train accuracy: 0.8907595872879028\n",
            "Epoch: 2/5  | Epoch loss: 2.577709197998047  | Mean train accuracy: 0.8907697796821594\n",
            "Epoch: 2/5  | Epoch loss: 2.5779762268066406  | Mean train accuracy: 0.8907797932624817\n",
            "Epoch: 2/5  | Epoch loss: 2.5781497955322266  | Mean train accuracy: 0.8907939195632935\n",
            "Epoch: 2/5  | Epoch loss: 2.578181266784668  | Mean train accuracy: 0.8908080458641052\n",
            "Epoch: 2/5  | Epoch loss: 2.579138994216919  | Mean train accuracy: 0.8908208608627319\n",
            "Epoch: 2/5  | Epoch loss: 2.5788824558258057  | Mean train accuracy: 0.8908377885818481\n",
            "Epoch: 2/5  | Epoch loss: 2.578533411026001  | Mean train accuracy: 0.8908515572547913\n",
            "Epoch: 2/5  | Epoch loss: 2.580186605453491  | Mean train accuracy: 0.8908676505088806\n",
            "Epoch: 2/5  | Epoch loss: 2.578972578048706  | Mean train accuracy: 0.8908846378326416\n",
            "Epoch: 2/5  | Epoch loss: 2.5779595375061035  | Mean train accuracy: 0.8909014463424683\n",
            "Epoch: 2/5  | Epoch loss: 2.578251838684082  | Mean train accuracy: 0.8909152150154114\n",
            "Epoch: 2/5  | Epoch loss: 2.5789337158203125  | Mean train accuracy: 0.8909329175949097\n",
            "Epoch: 2/5  | Epoch loss: 2.579089879989624  | Mean train accuracy: 0.8909473419189453\n",
            "Epoch: 2/5  | Epoch loss: 2.5793347358703613  | Mean train accuracy: 0.8909626007080078\n",
            "Epoch: 2/5  | Epoch loss: 2.5796899795532227  | Mean train accuracy: 0.8909757733345032\n",
            "Epoch: 2/5  | Epoch loss: 2.579244613647461  | Mean train accuracy: 0.8909912109375\n",
            "Epoch: 2/5  | Epoch loss: 2.5796430110931396  | Mean train accuracy: 0.8910052180290222\n",
            "Epoch: 2/5  | Epoch loss: 2.579730272293091  | Mean train accuracy: 0.8910178542137146\n",
            "Epoch: 2/5  | Epoch loss: 2.5800583362579346  | Mean train accuracy: 0.8910297155380249\n",
            "Epoch: 2/5  | Epoch loss: 2.5797383785247803  | Mean train accuracy: 0.8910374641418457\n",
            "Epoch: 2/5  | Epoch loss: 2.5798752307891846  | Mean train accuracy: 0.8910435438156128\n",
            "Epoch: 2/5  | Epoch loss: 2.580281972885132  | Mean train accuracy: 0.8910498023033142\n",
            "Epoch: 2/5  | Epoch loss: 2.5804872512817383  | Mean train accuracy: 0.8910587430000305\n",
            "Epoch: 2/5  | Epoch loss: 2.5803956985473633  | Mean train accuracy: 0.8910650014877319\n",
            "Epoch: 2/5  | Epoch loss: 2.5804388523101807  | Mean train accuracy: 0.8910703659057617\n",
            "Epoch: 2/5  | Epoch loss: 2.580756902694702  | Mean train accuracy: 0.8910748958587646\n",
            "Epoch: 2/5  | Epoch loss: 2.5802671909332275  | Mean train accuracy: 0.8910802602767944\n",
            "Epoch: 2/5  | Epoch loss: 2.5799872875213623  | Mean train accuracy: 0.8910845518112183\n",
            "Epoch: 2/5  | Epoch loss: 2.5806126594543457  | Mean train accuracy: 0.8910903930664062\n",
            "Epoch: 2/5  | Epoch loss: 2.580259084701538  | Mean train accuracy: 0.8910947442054749\n",
            "Epoch: 2/5  | Epoch loss: 2.5795578956604004  | Mean train accuracy: 0.8910994529724121\n",
            "Epoch: 2/5  | Epoch loss: 2.5790116786956787  | Mean train accuracy: 0.8911066055297852\n",
            "Epoch: 2/5  | Epoch loss: 2.5792388916015625  | Mean train accuracy: 0.8911173343658447\n",
            "Epoch: 2/5  | Epoch loss: 2.57915997505188  | Mean train accuracy: 0.8911281228065491\n",
            "Epoch: 2/5  | Epoch loss: 2.578606367111206  | Mean train accuracy: 0.8911356925964355\n",
            "Epoch: 2/5  | Epoch loss: 2.578404664993286  | Mean train accuracy: 0.8911489844322205\n",
            "Epoch: 2/5  | Epoch loss: 2.5779571533203125  | Mean train accuracy: 0.8911594748497009\n",
            "Epoch: 2/5  | Epoch loss: 2.578415632247925  | Mean train accuracy: 0.891171932220459\n",
            "Epoch: 2/5  | Epoch loss: 2.57794451713562  | Mean train accuracy: 0.8911835551261902\n",
            "Epoch: 2/5  | Epoch loss: 2.577821969985962  | Mean train accuracy: 0.8911948204040527\n",
            "Epoch: 2/5  | Epoch loss: 2.578427314758301  | Mean train accuracy: 0.8912104368209839\n",
            "Epoch: 2/5  | Epoch loss: 2.578213691711426  | Mean train accuracy: 0.891222357749939\n",
            "Epoch: 2/5  | Epoch loss: 2.5776731967926025  | Mean train accuracy: 0.8912365436553955\n",
            "Epoch: 2/5  | Epoch loss: 2.578653573989868  | Mean train accuracy: 0.8912501335144043\n",
            "Epoch: 2/5  | Epoch loss: 2.5796189308166504  | Mean train accuracy: 0.8912613987922668\n",
            "Epoch: 2/5  | Epoch loss: 2.579160690307617  | Mean train accuracy: 0.8912732601165771\n",
            "Epoch: 2/5  | Epoch loss: 2.5788488388061523  | Mean train accuracy: 0.8912829756736755\n",
            "Epoch: 2/5  | Epoch loss: 2.578850507736206  | Mean train accuracy: 0.8912913203239441\n",
            "Epoch: 2/5  | Epoch loss: 2.5793328285217285  | Mean train accuracy: 0.8912997841835022\n",
            "Epoch: 2/5  | Epoch loss: 2.5796754360198975  | Mean train accuracy: 0.8913065195083618\n",
            "Epoch: 2/5  | Epoch loss: 2.579840898513794  | Mean train accuracy: 0.8913158774375916\n",
            "Epoch: 2/5  | Epoch loss: 2.5798490047454834  | Mean train accuracy: 0.891320526599884\n",
            "Epoch: 2/5  | Epoch loss: 2.5791900157928467  | Mean train accuracy: 0.8913247585296631\n",
            "Epoch: 2/5  | Epoch loss: 2.57983136177063  | Mean train accuracy: 0.8913254737854004\n",
            "Epoch: 2/5  | Epoch loss: 2.580214262008667  | Mean train accuracy: 0.8913294076919556\n",
            "Epoch: 2/5  | Epoch loss: 2.580659866333008  | Mean train accuracy: 0.8913302421569824\n",
            "Epoch: 2/5  | Epoch loss: 2.5801215171813965  | Mean train accuracy: 0.8913283348083496\n",
            "Epoch: 2/5  | Epoch loss: 2.5798981189727783  | Mean train accuracy: 0.891326367855072\n",
            "Epoch: 2/5  | Epoch loss: 2.5806028842926025  | Mean train accuracy: 0.8913241028785706\n",
            "Epoch: 2/5  | Epoch loss: 2.580156087875366  | Mean train accuracy: 0.8913217186927795\n",
            "Epoch: 2/5  | Epoch loss: 2.5797500610351562  | Mean train accuracy: 0.8913193941116333\n",
            "Epoch: 2/5  | Epoch loss: 2.580172538757324  | Mean train accuracy: 0.8913174271583557\n",
            "Epoch: 2/5  | Epoch loss: 2.5803182125091553  | Mean train accuracy: 0.8913130164146423\n",
            "Epoch: 2/5  | Epoch loss: 2.580220937728882  | Mean train accuracy: 0.8913120627403259\n",
            "Epoch: 2/5  | Epoch loss: 2.5805599689483643  | Mean train accuracy: 0.8913097977638245\n",
            "Epoch: 2/5  | Epoch loss: 2.5802414417266846  | Mean train accuracy: 0.8913090825080872\n",
            "Epoch: 2/5  | Epoch loss: 2.580859422683716  | Mean train accuracy: 0.8913095593452454\n",
            "Epoch: 2/5  | Epoch loss: 2.5806736946105957  | Mean train accuracy: 0.891309916973114\n",
            "Epoch: 2/5  | Epoch loss: 2.580805540084839  | Mean train accuracy: 0.8913131356239319\n",
            "Epoch: 2/5  | Epoch loss: 2.5808987617492676  | Mean train accuracy: 0.8913165926933289\n",
            "Epoch: 2/5  | Epoch loss: 2.5813233852386475  | Mean train accuracy: 0.8913199305534363\n",
            "Epoch: 2/5  | Epoch loss: 2.5811707973480225  | Mean train accuracy: 0.8913224339485168\n",
            "Epoch: 2/5  | Epoch loss: 2.582028865814209  | Mean train accuracy: 0.8913255929946899\n",
            "Epoch: 2/5  | Epoch loss: 2.582122325897217  | Mean train accuracy: 0.8913267850875854\n",
            "Epoch: 2/5  | Epoch loss: 2.582239866256714  | Mean train accuracy: 0.8913290500640869\n",
            "Epoch: 2/5  | Epoch loss: 2.582399368286133  | Mean train accuracy: 0.8913329839706421\n",
            "Epoch: 2/5  | Epoch loss: 2.582257032394409  | Mean train accuracy: 0.8913368582725525\n",
            "Epoch: 2/5  | Epoch loss: 2.5821399688720703  | Mean train accuracy: 0.8913410305976868\n",
            "Epoch: 2/5  | Epoch loss: 2.5826756954193115  | Mean train accuracy: 0.89134281873703\n",
            "Epoch: 2/5  | Epoch loss: 2.583127975463867  | Mean train accuracy: 0.8913421034812927\n",
            "Epoch: 2/5  | Epoch loss: 2.5832772254943848  | Mean train accuracy: 0.8913443684577942\n",
            "Epoch: 2/5  | Epoch loss: 2.5833919048309326  | Mean train accuracy: 0.891346275806427\n",
            "Epoch: 2/5  | Epoch loss: 2.584003448486328  | Mean train accuracy: 0.8913483619689941\n",
            "Epoch: 2/5  | Epoch loss: 2.5839951038360596  | Mean train accuracy: 0.891351580619812\n",
            "Epoch: 2/5  | Epoch loss: 2.5837771892547607  | Mean train accuracy: 0.8913549780845642\n",
            "Epoch: 2/5  | Epoch loss: 2.5837292671203613  | Mean train accuracy: 0.8913581371307373\n",
            "Epoch: 2/5  | Epoch loss: 2.5833592414855957  | Mean train accuracy: 0.8913612961769104\n",
            "Epoch: 2/5  | Epoch loss: 2.5839738845825195  | Mean train accuracy: 0.8913627862930298\n",
            "Epoch: 2/5  | Epoch loss: 2.5840682983398438  | Mean train accuracy: 0.891361653804779\n",
            "Epoch: 2/5  | Epoch loss: 2.5840115547180176  | Mean train accuracy: 0.8913650512695312\n",
            "Epoch: 2/5  | Epoch loss: 2.5843379497528076  | Mean train accuracy: 0.8913672566413879\n",
            "Epoch: 2/5  | Epoch loss: 2.584714889526367  | Mean train accuracy: 0.8913695812225342\n",
            "Epoch: 2/5  | Epoch loss: 2.584984302520752  | Mean train accuracy: 0.8913747668266296\n",
            "Epoch: 2/5  | Epoch loss: 2.5850043296813965  | Mean train accuracy: 0.8913776278495789\n",
            "Epoch: 2/5  | Epoch loss: 2.584517002105713  | Mean train accuracy: 0.8913825750350952\n",
            "Epoch: 2/5  | Epoch loss: 2.5840909481048584  | Mean train accuracy: 0.891385555267334\n",
            "Epoch: 2/5  | Epoch loss: 2.5840306282043457  | Mean train accuracy: 0.8913878202438354\n",
            "Epoch: 2/5  | Epoch loss: 2.5837132930755615  | Mean train accuracy: 0.8913903832435608\n",
            "Epoch: 2/5  | Epoch loss: 2.583688974380493  | Mean train accuracy: 0.8913944959640503\n",
            "Epoch: 2/5  | Epoch loss: 2.583892822265625  | Mean train accuracy: 0.8913987874984741\n",
            "Epoch: 2/5  | Epoch loss: 2.583418846130371  | Mean train accuracy: 0.8914042711257935\n",
            "Epoch: 2/5  | Epoch loss: 2.5837137699127197  | Mean train accuracy: 0.8914095163345337\n",
            "Epoch: 2/5  | Epoch loss: 2.5833964347839355  | Mean train accuracy: 0.8914146423339844\n",
            "Epoch: 2/5  | Epoch loss: 2.583489179611206  | Mean train accuracy: 0.891420304775238\n",
            "Epoch: 2/5  | Epoch loss: 2.583327054977417  | Mean train accuracy: 0.8914265036582947\n",
            "Epoch: 2/5  | Epoch loss: 2.582670211791992  | Mean train accuracy: 0.8914312720298767\n",
            "Epoch: 2/5  | Epoch loss: 2.5827949047088623  | Mean train accuracy: 0.8914375305175781\n",
            "Epoch: 2/5  | Epoch loss: 2.5829596519470215  | Mean train accuracy: 0.8914444446563721\n",
            "Epoch: 2/5  | Epoch loss: 2.582585334777832  | Mean train accuracy: 0.8914535045623779\n",
            "Epoch: 2/5  | Epoch loss: 2.5827910900115967  | Mean train accuracy: 0.8914608955383301\n",
            "Epoch: 2/5  | Epoch loss: 2.582454204559326  | Mean train accuracy: 0.8914710879325867\n",
            "Epoch: 2/5  | Epoch loss: 2.5826520919799805  | Mean train accuracy: 0.8914780020713806\n",
            "Epoch: 2/5  | Epoch loss: 2.581803321838379  | Mean train accuracy: 0.8914861679077148\n",
            "Epoch: 2/5  | Epoch loss: 2.581855535507202  | Mean train accuracy: 0.8914953470230103\n",
            "Epoch: 2/5  | Epoch loss: 2.58151912689209  | Mean train accuracy: 0.8915027379989624\n",
            "Epoch: 2/5  | Epoch loss: 2.581752300262451  | Mean train accuracy: 0.891510546207428\n",
            "Epoch: 2/5  | Epoch loss: 2.581346273422241  | Mean train accuracy: 0.8915174603462219\n",
            "Epoch: 2/5  | Epoch loss: 2.58073353767395  | Mean train accuracy: 0.8915244936943054\n",
            "Epoch: 2/5  | Epoch loss: 2.580399751663208  | Mean train accuracy: 0.8915319442749023\n",
            "Epoch: 2/5  | Epoch loss: 2.581125020980835  | Mean train accuracy: 0.8915389180183411\n",
            "Epoch: 2/5  | Epoch loss: 2.581104278564453  | Mean train accuracy: 0.8915476202964783\n",
            "Epoch: 2/5  | Epoch loss: 2.5811853408813477  | Mean train accuracy: 0.8915544152259827\n",
            "Epoch: 2/5  | Epoch loss: 2.581559896469116  | Mean train accuracy: 0.8915623426437378\n",
            "Epoch: 2/5  | Epoch loss: 2.5813729763031006  | Mean train accuracy: 0.891571044921875\n",
            "Epoch: 2/5  | Epoch loss: 2.5812532901763916  | Mean train accuracy: 0.8915782570838928\n",
            "Epoch: 2/5  | Epoch loss: 2.5811705589294434  | Mean train accuracy: 0.8915847539901733\n",
            "Epoch: 2/5  | Epoch loss: 2.5814340114593506  | Mean train accuracy: 0.8915913701057434\n",
            "Epoch: 2/5  | Epoch loss: 2.5821080207824707  | Mean train accuracy: 0.8915984034538269\n",
            "Epoch: 2/5  | Epoch loss: 2.582115650177002  | Mean train accuracy: 0.8916059136390686\n",
            "Epoch: 2/5  | Epoch loss: 2.582047939300537  | Mean train accuracy: 0.8916130065917969\n",
            "Epoch: 2/5  | Epoch loss: 2.5819785594940186  | Mean train accuracy: 0.8916189074516296\n",
            "Epoch: 2/5  | Epoch loss: 2.581543445587158  | Mean train accuracy: 0.8916247487068176\n",
            "Epoch: 2/5  | Epoch loss: 2.5817956924438477  | Mean train accuracy: 0.8916307091712952\n",
            "Epoch: 2/5  | Epoch loss: 2.58136248588562  | Mean train accuracy: 0.8916354775428772\n",
            "Epoch: 2/5  | Epoch loss: 2.5813677310943604  | Mean train accuracy: 0.8916392922401428\n",
            "Epoch: 2/5  | Epoch loss: 2.5816802978515625  | Mean train accuracy: 0.8916445970535278\n",
            "Epoch: 2/5  | Epoch loss: 2.58193302154541  | Mean train accuracy: 0.8916504383087158\n",
            "Epoch: 2/5  | Epoch loss: 2.5819296836853027  | Mean train accuracy: 0.8916581869125366\n",
            "Epoch: 2/5  | Epoch loss: 2.582050085067749  | Mean train accuracy: 0.8916651010513306\n",
            "Epoch: 2/5  | Epoch loss: 2.5817902088165283  | Mean train accuracy: 0.8916727304458618\n",
            "Epoch: 2/5  | Epoch loss: 2.5817089080810547  | Mean train accuracy: 0.8916804194450378\n",
            "Epoch: 2/5  | Epoch loss: 2.5820233821868896  | Mean train accuracy: 0.8916879892349243\n",
            "Epoch: 2/5  | Epoch loss: 2.582502603530884  | Mean train accuracy: 0.8916963338851929\n",
            "Epoch: 2/5  | Epoch loss: 2.582545518875122  | Mean train accuracy: 0.8917041420936584\n",
            "Epoch: 2/5  | Epoch loss: 2.5826547145843506  | Mean train accuracy: 0.8917123079299927\n",
            "Epoch: 2/5  | Epoch loss: 2.5823686122894287  | Mean train accuracy: 0.8917202353477478\n",
            "Epoch: 2/5  | Epoch loss: 2.582728385925293  | Mean train accuracy: 0.8917267322540283\n",
            "Epoch: 2/5  | Epoch loss: 2.58274245262146  | Mean train accuracy: 0.8917338848114014\n",
            "Epoch: 2/5  | Epoch loss: 2.5824532508850098  | Mean train accuracy: 0.8917400240898132\n",
            "Epoch: 2/5  | Epoch loss: 2.582350015640259  | Mean train accuracy: 0.8917459845542908\n",
            "Epoch: 2/5  | Epoch loss: 2.5825247764587402  | Mean train accuracy: 0.8917524218559265\n",
            "Epoch: 2/5  | Epoch loss: 2.582071542739868  | Mean train accuracy: 0.8917571902275085\n",
            "Epoch: 2/5  | Epoch loss: 2.5818936824798584  | Mean train accuracy: 0.8917609453201294\n",
            "Epoch: 2/5  | Epoch loss: 2.581493377685547  | Mean train accuracy: 0.8917679190635681\n",
            "Epoch: 2/5  | Epoch loss: 2.581326723098755  | Mean train accuracy: 0.8917731642723083\n",
            "Epoch: 2/5  | Epoch loss: 2.5815374851226807  | Mean train accuracy: 0.8917760848999023\n",
            "Epoch: 2/5  | Epoch loss: 2.581991195678711  | Mean train accuracy: 0.8917807340621948\n",
            "Epoch: 2/5  | Epoch loss: 2.581721782684326  | Mean train accuracy: 0.89178466796875\n",
            "Epoch: 2/5  | Epoch loss: 2.5813305377960205  | Mean train accuracy: 0.8917891979217529\n",
            "Epoch: 2/5  | Epoch loss: 2.581727981567383  | Mean train accuracy: 0.8917924761772156\n",
            "Epoch: 2/5  | Epoch loss: 2.581209182739258  | Mean train accuracy: 0.8917962312698364\n",
            "Epoch: 2/5  | Epoch loss: 2.5810534954071045  | Mean train accuracy: 0.8917994499206543\n",
            "Epoch: 2/5  | Epoch loss: 2.5808863639831543  | Mean train accuracy: 0.8918021321296692\n",
            "Epoch: 2/5  | Epoch loss: 2.581094741821289  | Mean train accuracy: 0.891803503036499\n",
            "Epoch: 2/5  | Epoch loss: 2.580773115158081  | Mean train accuracy: 0.891806960105896\n",
            "Epoch: 2/5  | Epoch loss: 2.5807816982269287  | Mean train accuracy: 0.8918116688728333\n",
            "Epoch: 2/5  | Epoch loss: 2.5807883739471436  | Mean train accuracy: 0.8918137550354004\n",
            "Epoch: 2/5  | Epoch loss: 2.581326961517334  | Mean train accuracy: 0.8918144106864929\n",
            "Epoch: 2/5  | Epoch loss: 2.5816831588745117  | Mean train accuracy: 0.8918163180351257\n",
            "Epoch: 2/5  | Epoch loss: 2.5811843872070312  | Mean train accuracy: 0.8918194770812988\n",
            "Epoch: 2/5  | Epoch loss: 2.5816659927368164  | Mean train accuracy: 0.8918219804763794\n",
            "Epoch: 2/5  | Epoch loss: 2.5815927982330322  | Mean train accuracy: 0.8918241262435913\n",
            "Epoch: 2/5  | Epoch loss: 2.581756353378296  | Mean train accuracy: 0.8918243646621704\n",
            "Epoch: 2/5  | Epoch loss: 2.5816099643707275  | Mean train accuracy: 0.8918245434761047\n",
            "Epoch: 2/5  | Epoch loss: 2.5819175243377686  | Mean train accuracy: 0.8918255567550659\n",
            "Epoch: 2/5  | Epoch loss: 2.5818958282470703  | Mean train accuracy: 0.8918278217315674\n",
            "Epoch: 2/5  | Epoch loss: 2.5821738243103027  | Mean train accuracy: 0.891828715801239\n",
            "Epoch: 2/5  | Epoch loss: 2.581991672515869  | Mean train accuracy: 0.8918305039405823\n",
            "Epoch: 2/5  | Epoch loss: 2.5822768211364746  | Mean train accuracy: 0.8918324112892151\n",
            "Epoch: 2/5  | Epoch loss: 2.582552671432495  | Mean train accuracy: 0.8918333649635315\n",
            "Epoch: 2/5  | Epoch loss: 2.5833051204681396  | Mean train accuracy: 0.8918319940567017\n",
            "Epoch: 2/5  | Epoch loss: 2.5834178924560547  | Mean train accuracy: 0.8918334245681763\n",
            "Epoch: 2/5  | Epoch loss: 2.5830776691436768  | Mean train accuracy: 0.8918342590332031\n",
            "Epoch: 2/5  | Epoch loss: 2.5835087299346924  | Mean train accuracy: 0.8918346166610718\n",
            "Epoch: 2/5  | Epoch loss: 2.5832388401031494  | Mean train accuracy: 0.891835629940033\n",
            "Epoch: 2/5  | Epoch loss: 2.583348035812378  | Mean train accuracy: 0.8918361067771912\n",
            "Epoch: 2/5  | Epoch loss: 2.5837478637695312  | Mean train accuracy: 0.8918371200561523\n",
            "Epoch: 2/5  | Epoch loss: 2.583855628967285  | Mean train accuracy: 0.8918375372886658\n",
            "Epoch: 2/5  | Epoch loss: 2.583742618560791  | Mean train accuracy: 0.8918372988700867\n",
            "Epoch: 2/5  | Epoch loss: 2.5835514068603516  | Mean train accuracy: 0.8918386101722717\n",
            "Epoch: 2/5  | Epoch loss: 2.583355665206909  | Mean train accuracy: 0.8918403387069702\n",
            "Epoch: 2/5  | Epoch loss: 2.5833044052124023  | Mean train accuracy: 0.891842246055603\n",
            "Epoch: 2/5  | Epoch loss: 2.583324909210205  | Mean train accuracy: 0.8918442130088806\n",
            "Epoch: 2/5  | Epoch loss: 2.583453893661499  | Mean train accuracy: 0.8918463587760925\n",
            "Epoch: 2/5  | Epoch loss: 2.583120822906494  | Mean train accuracy: 0.8918491005897522\n",
            "Epoch: 2/5  | Epoch loss: 2.583078384399414  | Mean train accuracy: 0.8918520212173462\n",
            "Epoch: 2/5  | Epoch loss: 2.582951307296753  | Mean train accuracy: 0.8918560743331909\n",
            "Epoch: 2/5  | Epoch loss: 2.5825531482696533  | Mean train accuracy: 0.8918603658676147\n",
            "Epoch: 2/5  | Epoch loss: 2.582465410232544  | Mean train accuracy: 0.8918634653091431\n",
            "Epoch: 2/5  | Epoch loss: 2.5826680660247803  | Mean train accuracy: 0.8918667435646057\n",
            "Epoch: 2/5  | Epoch loss: 2.5824356079101562  | Mean train accuracy: 0.8918694853782654\n",
            "Epoch: 2/5  | Epoch loss: 2.58250093460083  | Mean train accuracy: 0.8918723464012146\n",
            "Epoch: 2/5  | Epoch loss: 2.5831100940704346  | Mean train accuracy: 0.8918763995170593\n",
            "Epoch: 2/5  | Epoch loss: 2.583249807357788  | Mean train accuracy: 0.8918811082839966\n",
            "Epoch: 2/5  | Epoch loss: 2.5829105377197266  | Mean train accuracy: 0.8918864727020264\n",
            "Epoch: 2/5  | Epoch loss: 2.5828473567962646  | Mean train accuracy: 0.8918920755386353\n",
            "Epoch: 2/5  | Epoch loss: 2.5829949378967285  | Mean train accuracy: 0.8918968439102173\n",
            "Epoch: 2/5  | Epoch loss: 2.582876205444336  | Mean train accuracy: 0.8919008374214172\n",
            "Epoch: 2/5  | Epoch loss: 2.582963228225708  | Mean train accuracy: 0.8919060826301575\n",
            "Epoch: 2/5  | Epoch loss: 2.5832784175872803  | Mean train accuracy: 0.8919112086296082\n",
            "Epoch: 2/5  | Epoch loss: 2.5837149620056152  | Mean train accuracy: 0.8919169306755066\n",
            "Epoch: 2/5  | Epoch loss: 2.5840070247650146  | Mean train accuracy: 0.8919222950935364\n",
            "Epoch: 2/5  | Epoch loss: 2.583793878555298  | Mean train accuracy: 0.8919280767440796\n",
            "Epoch: 2/5  | Epoch loss: 2.5838968753814697  | Mean train accuracy: 0.8919313549995422\n",
            "Epoch: 2/5  | Epoch loss: 2.5839083194732666  | Mean train accuracy: 0.891934871673584\n",
            "Epoch: 2/5  | Epoch loss: 2.583308458328247  | Mean train accuracy: 0.891938328742981\n",
            "Epoch: 2/5  | Epoch loss: 2.5833816528320312  | Mean train accuracy: 0.8919424414634705\n",
            "Epoch: 2/5  | Epoch loss: 2.583566188812256  | Mean train accuracy: 0.8919476270675659\n",
            "Epoch: 2/5  | Epoch loss: 2.5837316513061523  | Mean train accuracy: 0.8919526934623718\n",
            "Epoch: 2/5  | Epoch loss: 2.583540201187134  | Mean train accuracy: 0.8919567465782166\n",
            "Epoch: 2/5  | Epoch loss: 2.5828962326049805  | Mean train accuracy: 0.8919612169265747\n",
            "Epoch: 2/5  | Epoch loss: 2.582789182662964  | Mean train accuracy: 0.891966700553894\n",
            "Epoch: 2/5  | Epoch loss: 2.5826895236968994  | Mean train accuracy: 0.8919714689254761\n",
            "Epoch: 2/5  | Epoch loss: 2.5827982425689697  | Mean train accuracy: 0.8919769525527954\n",
            "Epoch: 2/5  | Epoch loss: 2.5827627182006836  | Mean train accuracy: 0.8919832110404968\n",
            "Epoch: 2/5  | Epoch loss: 2.5828535556793213  | Mean train accuracy: 0.8919902443885803\n",
            "Epoch: 2/5  | Epoch loss: 2.582852602005005  | Mean train accuracy: 0.8919965624809265\n",
            "Epoch: 2/5  | Epoch loss: 2.5829837322235107  | Mean train accuracy: 0.8920020461082458\n",
            "Epoch: 2/5  | Epoch loss: 2.5827410221099854  | Mean train accuracy: 0.8920080065727234\n",
            "Epoch: 2/5  | Epoch loss: 2.5825695991516113  | Mean train accuracy: 0.8920142650604248\n",
            "Epoch: 2/5  | Epoch loss: 2.582221508026123  | Mean train accuracy: 0.8920218348503113\n",
            "Epoch: 2/5  | Epoch loss: 2.5825178623199463  | Mean train accuracy: 0.8920272588729858\n",
            "Epoch: 2/5  | Epoch loss: 2.5824222564697266  | Mean train accuracy: 0.892033040523529\n",
            "Epoch: 2/5  | Epoch loss: 2.5824382305145264  | Mean train accuracy: 0.8920393586158752\n",
            "Epoch: 2/5  | Epoch loss: 2.5827584266662598  | Mean train accuracy: 0.892045259475708\n",
            "Epoch: 2/5  | Epoch loss: 2.5829453468322754  | Mean train accuracy: 0.8920498490333557\n",
            "Epoch: 2/5  | Epoch loss: 2.5833005905151367  | Mean train accuracy: 0.89205402135849\n",
            "Epoch: 2/5  | Epoch loss: 2.583158254623413  | Mean train accuracy: 0.8920590877532959\n",
            "Epoch: 2/5  | Epoch loss: 2.583080291748047  | Mean train accuracy: 0.8920621275901794\n",
            "Epoch: 2/5  | Epoch loss: 2.583096981048584  | Mean train accuracy: 0.8920666575431824\n",
            "Epoch: 2/5  | Epoch loss: 2.582690954208374  | Mean train accuracy: 0.8920690417289734\n",
            "Epoch: 2/5  | Epoch loss: 2.58251953125  | Mean train accuracy: 0.8920723795890808\n",
            "Epoch: 2/5  | Epoch loss: 2.5822832584381104  | Mean train accuracy: 0.8920763731002808\n",
            "Epoch: 2/5  | Epoch loss: 2.582672595977783  | Mean train accuracy: 0.8920798897743225\n",
            "Epoch: 2/5  | Epoch loss: 2.5824942588806152  | Mean train accuracy: 0.8920846581459045\n",
            "Epoch: 2/5  | Epoch loss: 2.5818939208984375  | Mean train accuracy: 0.8920895457267761\n",
            "Epoch: 2/5  | Epoch loss: 2.581353187561035  | Mean train accuracy: 0.8920947909355164\n",
            "Epoch: 2/5  | Epoch loss: 2.5812041759490967  | Mean train accuracy: 0.8921008706092834\n",
            "Epoch: 2/5  | Epoch loss: 2.5813703536987305  | Mean train accuracy: 0.8921054005622864\n",
            "Epoch: 2/5  | Epoch loss: 2.581270933151245  | Mean train accuracy: 0.8921108245849609\n",
            "Epoch: 2/5  | Epoch loss: 2.581327438354492  | Mean train accuracy: 0.8921160101890564\n",
            "Epoch: 2/5  | Epoch loss: 2.5813705921173096  | Mean train accuracy: 0.892121434211731\n",
            "Epoch: 2/5  | Epoch loss: 2.5816805362701416  | Mean train accuracy: 0.8921266198158264\n",
            "Epoch: 2/5  | Epoch loss: 2.5817155838012695  | Mean train accuracy: 0.8921318650245667\n",
            "Epoch: 2/5  | Epoch loss: 2.5822455883026123  | Mean train accuracy: 0.8921356201171875\n",
            "Epoch: 2/5  | Epoch loss: 2.5822153091430664  | Mean train accuracy: 0.8921413421630859\n",
            "Epoch: 2/5  | Epoch loss: 2.5824427604675293  | Mean train accuracy: 0.8921477794647217\n",
            "Epoch: 2/5  | Epoch loss: 2.5823729038238525  | Mean train accuracy: 0.8921541571617126\n",
            "Epoch: 2/5  | Epoch loss: 2.5821774005889893  | Mean train accuracy: 0.8921604752540588\n",
            "Epoch: 2/5  | Epoch loss: 2.5821213722229004  | Mean train accuracy: 0.8921668529510498\n",
            "Epoch: 2/5  | Epoch loss: 2.582087278366089  | Mean train accuracy: 0.892172634601593\n",
            "Epoch: 2/5  | Epoch loss: 2.5816738605499268  | Mean train accuracy: 0.8921784162521362\n",
            "Epoch: 2/5  | Epoch loss: 2.5817222595214844  | Mean train accuracy: 0.8921834826469421\n",
            "Epoch: 2/5  | Epoch loss: 2.5815603733062744  | Mean train accuracy: 0.8921893835067749\n",
            "Epoch: 2/5  | Epoch loss: 2.5817840099334717  | Mean train accuracy: 0.8921953439712524\n",
            "Epoch: 2/5  | Epoch loss: 2.581836462020874  | Mean train accuracy: 0.8922006487846375\n",
            "Epoch: 2/5  | Epoch loss: 2.5815181732177734  | Mean train accuracy: 0.8922053575515747\n",
            "Epoch: 2/5  | Epoch loss: 2.5815200805664062  | Mean train accuracy: 0.892209529876709\n",
            "Epoch: 2/5  | Epoch loss: 2.5816895961761475  | Mean train accuracy: 0.8922135233879089\n",
            "Epoch: 2/5  | Epoch loss: 2.5816805362701416  | Mean train accuracy: 0.8922181129455566\n",
            "Epoch: 2/5  | Epoch loss: 2.581287384033203  | Mean train accuracy: 0.8922215700149536\n",
            "Epoch: 2/5  | Epoch loss: 2.5808377265930176  | Mean train accuracy: 0.8922268152236938\n",
            "Epoch: 2/5  | Epoch loss: 2.5808629989624023  | Mean train accuracy: 0.8922286629676819\n",
            "Epoch: 2/5  | Epoch loss: 2.5809884071350098  | Mean train accuracy: 0.8922301530838013\n",
            "Epoch: 2/5  | Epoch loss: 2.5813004970550537  | Mean train accuracy: 0.8922315239906311\n",
            "Epoch: 2/5  | Epoch loss: 2.581467866897583  | Mean train accuracy: 0.8922316431999207\n",
            "Epoch: 2/5  | Epoch loss: 2.5816242694854736  | Mean train accuracy: 0.8922314643859863\n",
            "Epoch: 2/5  | Epoch loss: 2.5811071395874023  | Mean train accuracy: 0.8922306895256042\n",
            "Epoch: 2/5  | Epoch loss: 2.580766201019287  | Mean train accuracy: 0.8922285437583923\n",
            "Epoch: 2/5  | Epoch loss: 2.580350875854492  | Mean train accuracy: 0.8922279477119446\n",
            "Epoch: 2/5  | Epoch loss: 2.5799906253814697  | Mean train accuracy: 0.8922266364097595\n",
            "Epoch: 2/5  | Epoch loss: 2.5804593563079834  | Mean train accuracy: 0.8922251462936401\n",
            "Epoch: 2/5  | Epoch loss: 2.580490827560425  | Mean train accuracy: 0.8922240734100342\n",
            "Epoch: 2/5  | Epoch loss: 2.5806691646575928  | Mean train accuracy: 0.8922230005264282\n",
            "Epoch: 2/5  | Epoch loss: 2.580965280532837  | Mean train accuracy: 0.8922206163406372\n",
            "Epoch: 2/5  | Epoch loss: 2.5807840824127197  | Mean train accuracy: 0.8922179341316223\n",
            "Epoch: 2/5  | Epoch loss: 2.5808494091033936  | Mean train accuracy: 0.8922173976898193\n",
            "Epoch: 2/5  | Epoch loss: 2.5807406902313232  | Mean train accuracy: 0.8922167420387268\n",
            "Epoch: 2/5  | Epoch loss: 2.5808982849121094  | Mean train accuracy: 0.8922166228294373\n",
            "Epoch: 2/5  | Epoch loss: 2.580758571624756  | Mean train accuracy: 0.8922175168991089\n",
            "Epoch: 2/5  | Epoch loss: 2.5808281898498535  | Mean train accuracy: 0.892217218875885\n",
            "Epoch: 2/5  | Epoch loss: 2.580998420715332  | Mean train accuracy: 0.892217755317688\n",
            "Epoch: 2/5  | Epoch loss: 2.5811221599578857  | Mean train accuracy: 0.8922186493873596\n",
            "Epoch: 2/5  | Epoch loss: 2.5809614658355713  | Mean train accuracy: 0.8922195434570312\n",
            "Epoch: 2/5  | Epoch loss: 2.5807876586914062  | Mean train accuracy: 0.8922227025032043\n",
            "Epoch: 2/5  | Epoch loss: 2.5807089805603027  | Mean train accuracy: 0.8922259211540222\n",
            "Epoch: 2/5  | Epoch loss: 2.580490827560425  | Mean train accuracy: 0.8922303915023804\n",
            "Epoch: 2/5  | Epoch loss: 2.57997727394104  | Mean train accuracy: 0.8922352194786072\n",
            "Epoch: 2/5  | Epoch loss: 2.5798983573913574  | Mean train accuracy: 0.8922386765480042\n",
            "Epoch: 2/5  | Epoch loss: 2.5798187255859375  | Mean train accuracy: 0.8922421336174011\n",
            "Epoch: 2/5  | Epoch loss: 2.5801939964294434  | Mean train accuracy: 0.8922471404075623\n",
            "Epoch: 2/5  | Epoch loss: 2.580191135406494  | Mean train accuracy: 0.8922532796859741\n",
            "Epoch: 2/5  | Epoch loss: 2.580404043197632  | Mean train accuracy: 0.8922585844993591\n",
            "Epoch: 2/5  | Epoch loss: 2.5803585052490234  | Mean train accuracy: 0.8922646641731262\n",
            "Epoch: 2/5  | Epoch loss: 2.580281972885132  | Mean train accuracy: 0.8922702670097351\n",
            "Epoch: 2/5  | Epoch loss: 2.580519199371338  | Mean train accuracy: 0.8922761678695679\n",
            "Epoch: 2/5  | Epoch loss: 2.580355167388916  | Mean train accuracy: 0.8922832012176514\n",
            "Epoch: 2/5  | Epoch loss: 2.580551862716675  | Mean train accuracy: 0.8922911286354065\n",
            "Epoch: 2/5  | Epoch loss: 2.5805346965789795  | Mean train accuracy: 0.8922984600067139\n",
            "Epoch: 2/5  | Epoch loss: 2.580470085144043  | Mean train accuracy: 0.892306387424469\n",
            "Epoch: 2/5  | Epoch loss: 2.580758571624756  | Mean train accuracy: 0.8923137784004211\n",
            "Epoch: 2/5  | Epoch loss: 2.580782413482666  | Mean train accuracy: 0.8923202157020569\n",
            "Epoch: 2/5  | Epoch loss: 2.580476760864258  | Mean train accuracy: 0.8923274874687195\n",
            "Epoch: 2/5  | Epoch loss: 2.580571413040161  | Mean train accuracy: 0.8923344016075134\n",
            "Epoch: 2/5  | Epoch loss: 2.580509662628174  | Mean train accuracy: 0.892340898513794\n",
            "Epoch: 2/5  | Epoch loss: 2.5807175636291504  | Mean train accuracy: 0.8923472166061401\n",
            "Epoch: 2/5  | Epoch loss: 2.5805060863494873  | Mean train accuracy: 0.8923530578613281\n",
            "Epoch: 2/5  | Epoch loss: 2.58097505569458  | Mean train accuracy: 0.8923594951629639\n",
            "Epoch: 2/5  | Epoch loss: 2.5808818340301514  | Mean train accuracy: 0.8923633098602295\n",
            "Epoch: 2/5  | Epoch loss: 2.5809333324432373  | Mean train accuracy: 0.8923677802085876\n",
            "Epoch: 2/5  | Epoch loss: 2.5808069705963135  | Mean train accuracy: 0.8923729658126831\n",
            "Epoch: 2/5  | Epoch loss: 2.581070899963379  | Mean train accuracy: 0.8923760056495667\n",
            "Epoch: 2/5  | Epoch loss: 2.580779790878296  | Mean train accuracy: 0.8923792839050293\n",
            "Epoch: 2/5  | Epoch loss: 2.581035852432251  | Mean train accuracy: 0.8923842310905457\n",
            "Epoch: 2/5  | Epoch loss: 2.581040620803833  | Mean train accuracy: 0.8923889398574829\n",
            "Epoch: 2/5  | Epoch loss: 2.581052541732788  | Mean train accuracy: 0.8923925757408142\n",
            "Epoch: 2/5  | Epoch loss: 2.5812227725982666  | Mean train accuracy: 0.892396867275238\n",
            "Epoch: 2/5  | Epoch loss: 2.5814263820648193  | Mean train accuracy: 0.8924015760421753\n",
            "Epoch: 2/5  | Epoch loss: 2.5815792083740234  | Mean train accuracy: 0.8924044966697693\n",
            "Epoch: 2/5  | Epoch loss: 2.58141827583313  | Mean train accuracy: 0.8924078345298767\n",
            "Epoch: 2/5  | Epoch loss: 2.581636428833008  | Mean train accuracy: 0.8924122452735901\n",
            "Epoch: 2/5  | Epoch loss: 2.581655740737915  | Mean train accuracy: 0.8924153447151184\n",
            "Epoch: 2/5  | Epoch loss: 2.581756114959717  | Mean train accuracy: 0.8924176692962646\n",
            "Epoch: 2/5  | Epoch loss: 2.5817158222198486  | Mean train accuracy: 0.8924211263656616\n",
            "Epoch: 2/5  | Epoch loss: 2.5815751552581787  | Mean train accuracy: 0.892423152923584\n",
            "Epoch: 2/5  | Epoch loss: 2.5812172889709473  | Mean train accuracy: 0.8924265503883362\n",
            "Epoch: 2/5  | Epoch loss: 2.581380605697632  | Mean train accuracy: 0.892429530620575\n",
            "Epoch: 2/5  | Epoch loss: 2.581207752227783  | Mean train accuracy: 0.8924320936203003\n",
            "Epoch: 2/5  | Epoch loss: 2.5811893939971924  | Mean train accuracy: 0.8924356698989868\n",
            "Epoch: 2/5  | Epoch loss: 2.5811007022857666  | Mean train accuracy: 0.8924376964569092\n",
            "Epoch: 2/5  | Epoch loss: 2.5813331604003906  | Mean train accuracy: 0.8924403190612793\n",
            "Epoch: 2/5  | Epoch loss: 2.5814807415008545  | Mean train accuracy: 0.8924431800842285\n",
            "Epoch: 2/5  | Epoch loss: 2.5815377235412598  | Mean train accuracy: 0.8924453258514404\n",
            "Epoch: 2/5  | Epoch loss: 2.5815484523773193  | Mean train accuracy: 0.8924471735954285\n",
            "Epoch: 2/5  | Epoch loss: 2.5815601348876953  | Mean train accuracy: 0.8924493193626404\n",
            "Epoch: 2/5  | Epoch loss: 2.5813581943511963  | Mean train accuracy: 0.892451286315918\n",
            "Epoch: 2/5  | Epoch loss: 2.5809571743011475  | Mean train accuracy: 0.8924528956413269\n",
            "Epoch: 2/5  | Epoch loss: 2.580881357192993  | Mean train accuracy: 0.8924555778503418\n",
            "Epoch: 2/5  | Epoch loss: 2.5809452533721924  | Mean train accuracy: 0.8924581408500671\n",
            "Epoch: 2/5  | Epoch loss: 2.5808186531066895  | Mean train accuracy: 0.8924607634544373\n",
            "Epoch: 2/5  | Epoch loss: 2.5806987285614014  | Mean train accuracy: 0.8924633264541626\n",
            "Epoch: 2/5  | Epoch loss: 2.5804734230041504  | Mean train accuracy: 0.8924660682678223\n",
            "Epoch: 2/5  | Epoch loss: 2.580636501312256  | Mean train accuracy: 0.8924687504768372\n",
            "Epoch: 2/5  | Epoch loss: 2.58056640625  | Mean train accuracy: 0.8924720883369446\n",
            "Epoch: 2/5  | Epoch loss: 2.580958366394043  | Mean train accuracy: 0.8924747705459595\n",
            "Epoch: 2/5  | Epoch loss: 2.5806474685668945  | Mean train accuracy: 0.8924768567085266\n",
            "Epoch: 2/5  | Epoch loss: 2.580434799194336  | Mean train accuracy: 0.8924780488014221\n",
            "Epoch: 2/5  | Epoch loss: 2.5802602767944336  | Mean train accuracy: 0.892480731010437\n",
            "Epoch: 2/5  | Epoch loss: 2.5802810192108154  | Mean train accuracy: 0.8924834728240967\n",
            "Epoch: 2/5  | Epoch loss: 2.5804431438446045  | Mean train accuracy: 0.8924862146377563\n",
            "Epoch: 2/5  | Epoch loss: 2.580375909805298  | Mean train accuracy: 0.892489492893219\n",
            "Epoch: 2/5  | Epoch loss: 2.5804009437561035  | Mean train accuracy: 0.8924928307533264\n",
            "Epoch: 2/5  | Epoch loss: 2.580366849899292  | Mean train accuracy: 0.8924974799156189\n",
            "Epoch: 2/5  | Epoch loss: 2.5806784629821777  | Mean train accuracy: 0.8925020098686218\n",
            "Epoch: 2/5  | Epoch loss: 2.58076548576355  | Mean train accuracy: 0.8925056457519531\n",
            "Epoch: 2/5  | Epoch loss: 2.5805935859680176  | Mean train accuracy: 0.8925091624259949\n",
            "Epoch: 2/5  | Epoch loss: 2.580467700958252  | Mean train accuracy: 0.8925122618675232\n",
            "Epoch: 2/5  | Epoch loss: 2.5809686183929443  | Mean train accuracy: 0.8925163149833679\n",
            "Epoch: 2/5  | Epoch loss: 2.5813393592834473  | Mean train accuracy: 0.8925198912620544\n",
            "Epoch: 2/5  | Epoch loss: 2.581618547439575  | Mean train accuracy: 0.892524003982544\n",
            "Epoch: 2/5  | Epoch loss: 2.581704616546631  | Mean train accuracy: 0.8925291895866394\n",
            "Epoch: 2/5  | Epoch loss: 2.5817885398864746  | Mean train accuracy: 0.8925343751907349\n",
            "Epoch: 2/5  | Epoch loss: 2.581861734390259  | Mean train accuracy: 0.8925390839576721\n",
            "Epoch: 2/5  | Epoch loss: 2.581913948059082  | Mean train accuracy: 0.892544150352478\n",
            "Epoch: 2/5  | Epoch loss: 2.581974506378174  | Mean train accuracy: 0.8925493955612183\n",
            "Epoch: 2/5  | Epoch loss: 2.5823330879211426  | Mean train accuracy: 0.8925547003746033\n",
            "Epoch: 2/5  | Epoch loss: 2.5825281143188477  | Mean train accuracy: 0.8925590515136719\n",
            "Epoch: 2/5  | Epoch loss: 2.5830130577087402  | Mean train accuracy: 0.8925624489784241\n",
            "Epoch: 2/5  | Epoch loss: 2.5829882621765137  | Mean train accuracy: 0.8925660848617554\n",
            "Epoch: 2/5  | Epoch loss: 2.5829241275787354  | Mean train accuracy: 0.8925696015357971\n",
            "Epoch: 2/5  | Epoch loss: 2.583246946334839  | Mean train accuracy: 0.8925727009773254\n",
            "Epoch: 2/5  | Epoch loss: 2.5832109451293945  | Mean train accuracy: 0.8925743103027344\n",
            "Epoch: 2/5  | Epoch loss: 2.5829365253448486  | Mean train accuracy: 0.8925756216049194\n",
            "Epoch: 2/5  | Epoch loss: 2.5828328132629395  | Mean train accuracy: 0.892576277256012\n",
            "Epoch: 2/5  | Epoch loss: 2.583073616027832  | Mean train accuracy: 0.8925777673721313\n",
            "Epoch: 2/5  | Epoch loss: 2.5830800533294678  | Mean train accuracy: 0.8925789594650269\n",
            "Epoch: 2/5  | Epoch loss: 2.58304762840271  | Mean train accuracy: 0.8925791382789612\n",
            "Epoch: 2/5  | Epoch loss: 2.5829837322235107  | Mean train accuracy: 0.8925789594650269\n",
            "Epoch: 2/5  | Epoch loss: 2.582841157913208  | Mean train accuracy: 0.8925788402557373\n",
            "Epoch: 2/5  | Epoch loss: 2.5827369689941406  | Mean train accuracy: 0.8925790190696716\n",
            "Epoch: 2/5  | Epoch loss: 2.5827243328094482  | Mean train accuracy: 0.8925797343254089\n",
            "Epoch: 2/5  | Epoch loss: 2.582933187484741  | Mean train accuracy: 0.8925788998603821\n",
            "Epoch: 2/5  | Epoch loss: 2.582536220550537  | Mean train accuracy: 0.892578125\n",
            "Epoch: 2/5  | Epoch loss: 2.582176923751831  | Mean train accuracy: 0.8925780057907104\n",
            "Epoch: 2/5  | Epoch loss: 2.5820231437683105  | Mean train accuracy: 0.8925785422325134\n",
            "Epoch: 2/5  | Epoch loss: 2.582108497619629  | Mean train accuracy: 0.892579197883606\n",
            "Epoch: 2/5  | Epoch loss: 2.5821001529693604  | Mean train accuracy: 0.8925797343254089\n",
            "Epoch: 2/5  | Epoch loss: 2.5818142890930176  | Mean train accuracy: 0.8925796151161194\n",
            "Epoch: 2/5  | Epoch loss: 2.581658363342285  | Mean train accuracy: 0.8925800323486328\n",
            "Epoch: 2/5  | Epoch loss: 2.5817041397094727  | Mean train accuracy: 0.8925806283950806\n",
            "Epoch: 2/5  | Epoch loss: 2.5817322731018066  | Mean train accuracy: 0.8925819396972656\n",
            "Epoch: 2/5  | Epoch loss: 2.5815556049346924  | Mean train accuracy: 0.8925842046737671\n",
            "Epoch: 2/5  | Epoch loss: 2.5817813873291016  | Mean train accuracy: 0.892585277557373\n",
            "Epoch: 2/5  | Epoch loss: 2.582113742828369  | Mean train accuracy: 0.8925869464874268\n",
            "Epoch: 2/5  | Epoch loss: 2.581892251968384  | Mean train accuracy: 0.8925889730453491\n",
            "Epoch: 2/5  | Epoch loss: 2.5818605422973633  | Mean train accuracy: 0.8925912976264954\n",
            "Epoch: 2/5  | Epoch loss: 2.5817131996154785  | Mean train accuracy: 0.8925939798355103\n",
            "Epoch: 2/5  | Epoch loss: 2.581545352935791  | Mean train accuracy: 0.8925962448120117\n",
            "Epoch: 2/5  | Epoch loss: 2.5816690921783447  | Mean train accuracy: 0.8926001191139221\n",
            "Epoch: 2/5  | Epoch loss: 2.581549644470215  | Mean train accuracy: 0.8926032185554504\n",
            "Epoch: 2/5  | Epoch loss: 2.581352949142456  | Mean train accuracy: 0.8926060795783997\n",
            "Epoch: 2/5  | Epoch loss: 2.581294536590576  | Mean train accuracy: 0.8926111459732056\n",
            "Epoch: 2/5  | Epoch loss: 2.5814735889434814  | Mean train accuracy: 0.8926154375076294\n",
            "Epoch: 2/5  | Epoch loss: 2.5813827514648438  | Mean train accuracy: 0.8926213383674622\n",
            "Epoch: 2/5  | Epoch loss: 2.581263780593872  | Mean train accuracy: 0.8926270604133606\n",
            "Epoch: 2/5  | Epoch loss: 2.581159830093384  | Mean train accuracy: 0.8926330804824829\n",
            "Epoch: 2/5  | Epoch loss: 2.5813677310943604  | Mean train accuracy: 0.8926388621330261\n",
            "Epoch: 2/5  | Epoch loss: 2.5811381340026855  | Mean train accuracy: 0.8926443457603455\n",
            "Epoch: 2/5  | Epoch loss: 2.5812571048736572  | Mean train accuracy: 0.8926498889923096\n",
            "Epoch: 2/5  | Epoch loss: 2.5809521675109863  | Mean train accuracy: 0.8926563858985901\n",
            "Epoch: 2/5  | Epoch loss: 2.5806920528411865  | Mean train accuracy: 0.8926631808280945\n",
            "Epoch: 2/5  | Epoch loss: 2.581027030944824  | Mean train accuracy: 0.8926693201065063\n",
            "Epoch: 2/5  | Epoch loss: 2.5809783935546875  | Mean train accuracy: 0.8926752805709839\n",
            "Epoch: 2/5  | Epoch loss: 2.5808987617492676  | Mean train accuracy: 0.8926807641983032\n",
            "Epoch: 2/5  | Epoch loss: 2.5809106826782227  | Mean train accuracy: 0.8926865458488464\n",
            "Epoch: 2/5  | Epoch loss: 2.5807690620422363  | Mean train accuracy: 0.8926917314529419\n",
            "Epoch: 2/5  | Epoch loss: 2.5808563232421875  | Mean train accuracy: 0.8926977515220642\n",
            "Epoch: 2/5  | Epoch loss: 2.580925464630127  | Mean train accuracy: 0.8927029371261597\n",
            "Epoch: 2/5  | Epoch loss: 2.580874443054199  | Mean train accuracy: 0.8927076458930969\n",
            "Epoch: 2/5  | Epoch loss: 2.581132411956787  | Mean train accuracy: 0.8927119970321655\n",
            "Epoch: 2/5  | Epoch loss: 2.5810091495513916  | Mean train accuracy: 0.8927156329154968\n",
            "Epoch: 2/5  | Epoch loss: 2.581446647644043  | Mean train accuracy: 0.8927186727523804\n",
            "Epoch: 2/5  | Epoch loss: 2.5811915397644043  | Mean train accuracy: 0.8927215337753296\n",
            "Epoch: 2/5  | Epoch loss: 2.5810446739196777  | Mean train accuracy: 0.8927239179611206\n",
            "Epoch: 2/5  | Epoch loss: 2.581139087677002  | Mean train accuracy: 0.8927250504493713\n",
            "Epoch: 2/5  | Epoch loss: 2.5811989307403564  | Mean train accuracy: 0.8927267789840698\n",
            "Epoch: 2/5  | Epoch loss: 2.581256866455078  | Mean train accuracy: 0.8927278518676758\n",
            "Epoch: 2/5  | Epoch loss: 2.581230401992798  | Mean train accuracy: 0.8927285075187683\n",
            "Epoch: 2/5  | Epoch loss: 2.5815188884735107  | Mean train accuracy: 0.8927276730537415\n",
            "Epoch: 2/5  | Epoch loss: 2.5816490650177  | Mean train accuracy: 0.8927270174026489\n",
            "Epoch: 2/5  | Epoch loss: 2.5817410945892334  | Mean train accuracy: 0.8927252292633057\n",
            "Epoch: 2/5  | Epoch loss: 2.5817246437072754  | Mean train accuracy: 0.8927238583564758\n",
            "Epoch: 2/5  | Epoch loss: 2.5820038318634033  | Mean train accuracy: 0.8927225470542908\n",
            "Epoch: 2/5  | Epoch loss: 2.5821585655212402  | Mean train accuracy: 0.8927211761474609\n",
            "Epoch: 2/5  | Epoch loss: 2.5821597576141357  | Mean train accuracy: 0.8927189707756042\n",
            "Epoch: 2/5  | Epoch loss: 2.5819170475006104  | Mean train accuracy: 0.8927183747291565\n",
            "Epoch: 2/5  | Epoch loss: 2.582118511199951  | Mean train accuracy: 0.8927180171012878\n",
            "Epoch: 2/5  | Epoch loss: 2.5817975997924805  | Mean train accuracy: 0.8927181363105774\n",
            "Epoch: 2/5  | Epoch loss: 2.5819971561431885  | Mean train accuracy: 0.8927182555198669\n",
            "Epoch: 2/5  | Epoch loss: 2.5818381309509277  | Mean train accuracy: 0.8927174806594849\n",
            "Epoch: 2/5  | Epoch loss: 2.581963539123535  | Mean train accuracy: 0.8927174210548401\n",
            "Epoch: 2/5  | Epoch loss: 2.5819950103759766  | Mean train accuracy: 0.8927183747291565\n",
            "Epoch: 2/5  | Epoch loss: 2.581951856613159  | Mean train accuracy: 0.8927185535430908\n",
            "Epoch: 2/5  | Epoch loss: 2.581843852996826  | Mean train accuracy: 0.8927182555198669\n",
            "Epoch: 2/5  | Epoch loss: 2.5818932056427  | Mean train accuracy: 0.8927185535430908\n",
            "Epoch: 2/5  | Epoch loss: 2.5820484161376953  | Mean train accuracy: 0.8927173018455505\n",
            "Epoch: 2/5  | Epoch loss: 2.582167863845825  | Mean train accuracy: 0.8927157521247864\n",
            "Epoch: 2/5  | Epoch loss: 2.5819029808044434  | Mean train accuracy: 0.892715334892273\n",
            "Epoch: 2/5  | Epoch loss: 2.5819835662841797  | Mean train accuracy: 0.892714262008667\n",
            "Epoch: 2/5  | Epoch loss: 2.5820729732513428  | Mean train accuracy: 0.892711877822876\n",
            "Epoch: 2/5  | Epoch loss: 2.582137107849121  | Mean train accuracy: 0.8927105069160461\n",
            "Epoch: 2/5  | Epoch loss: 2.582329750061035  | Mean train accuracy: 0.8927083611488342\n",
            "Epoch: 2/5  | Epoch loss: 2.5823214054107666  | Mean train accuracy: 0.8927070498466492\n",
            "Epoch: 2/5  | Epoch loss: 2.5820624828338623  | Mean train accuracy: 0.8927040100097656\n",
            "Epoch: 2/5  | Epoch loss: 2.58235502243042  | Mean train accuracy: 0.8927001953125\n",
            "Epoch: 2/5  | Epoch loss: 2.582477331161499  | Mean train accuracy: 0.8926977515220642\n",
            "Epoch: 2/5  | Epoch loss: 2.5821428298950195  | Mean train accuracy: 0.8926942348480225\n",
            "Epoch: 2/5  | Epoch loss: 2.582246780395508  | Mean train accuracy: 0.8926905989646912\n",
            "Epoch: 2/5  | Epoch loss: 2.5823941230773926  | Mean train accuracy: 0.8926869034767151\n",
            "Epoch: 2/5  | Epoch loss: 2.5824763774871826  | Mean train accuracy: 0.8926836252212524\n",
            "Epoch: 2/5  | Epoch loss: 2.5828609466552734  | Mean train accuracy: 0.8926793336868286\n",
            "Epoch: 2/5  | Epoch loss: 2.5829482078552246  | Mean train accuracy: 0.8926752805709839\n",
            "Epoch: 2/5  | Epoch loss: 2.5829944610595703  | Mean train accuracy: 0.8926698565483093\n",
            "Epoch: 2/5  | Epoch loss: 2.5830137729644775  | Mean train accuracy: 0.892665445804596\n",
            "Epoch: 2/5  | Epoch loss: 2.5830395221710205  | Mean train accuracy: 0.8926615715026855\n",
            "Epoch: 2/5  | Epoch loss: 2.5831151008605957  | Mean train accuracy: 0.8926575183868408\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8da54a56185544a598322cae476470bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=655), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 3/5  | Epoch loss: 2.5725748538970947  | Mean train accuracy: 0.8904445171356201\n",
            "Epoch: 3/5  | Epoch loss: 2.5344176292419434  | Mean train accuracy: 0.8903027176856995\n",
            "Epoch: 3/5  | Epoch loss: 2.5723114013671875  | Mean train accuracy: 0.8900835514068604\n",
            "Epoch: 3/5  | Epoch loss: 2.5912444591522217  | Mean train accuracy: 0.8901898860931396\n",
            "Epoch: 3/5  | Epoch loss: 2.596583843231201  | Mean train accuracy: 0.8901815414428711\n",
            "Epoch: 3/5  | Epoch loss: 2.5928404331207275  | Mean train accuracy: 0.8899653553962708\n",
            "Epoch: 3/5  | Epoch loss: 2.5940988063812256  | Mean train accuracy: 0.889892041683197\n",
            "Epoch: 3/5  | Epoch loss: 2.597268581390381  | Mean train accuracy: 0.8898676037788391\n",
            "Epoch: 3/5  | Epoch loss: 2.6098403930664062  | Mean train accuracy: 0.8899160027503967\n",
            "Epoch: 3/5  | Epoch loss: 2.587486982345581  | Mean train accuracy: 0.8898811340332031\n",
            "Epoch: 3/5  | Epoch loss: 2.59639573097229  | Mean train accuracy: 0.8899160027503967\n",
            "Epoch: 3/5  | Epoch loss: 2.602292776107788  | Mean train accuracy: 0.8898999094963074\n",
            "Epoch: 3/5  | Epoch loss: 2.6042909622192383  | Mean train accuracy: 0.8899298310279846\n",
            "Epoch: 3/5  | Epoch loss: 2.613537311553955  | Mean train accuracy: 0.8899711966514587\n",
            "Epoch: 3/5  | Epoch loss: 2.5993006229400635  | Mean train accuracy: 0.889973521232605\n",
            "Epoch: 3/5  | Epoch loss: 2.5947177410125732  | Mean train accuracy: 0.8900109529495239\n",
            "Epoch: 3/5  | Epoch loss: 2.5942063331604004  | Mean train accuracy: 0.8900227546691895\n",
            "Epoch: 3/5  | Epoch loss: 2.5876078605651855  | Mean train accuracy: 0.8900189995765686\n",
            "Epoch: 3/5  | Epoch loss: 2.5995516777038574  | Mean train accuracy: 0.8899979591369629\n",
            "Epoch: 3/5  | Epoch loss: 2.5853207111358643  | Mean train accuracy: 0.8900087475776672\n",
            "Epoch: 3/5  | Epoch loss: 2.5883145332336426  | Mean train accuracy: 0.8899957537651062\n",
            "Epoch: 3/5  | Epoch loss: 2.592090606689453  | Mean train accuracy: 0.8899816274642944\n",
            "Epoch: 3/5  | Epoch loss: 2.596632480621338  | Mean train accuracy: 0.8899781703948975\n",
            "Epoch: 3/5  | Epoch loss: 2.5977632999420166  | Mean train accuracy: 0.8899471163749695\n",
            "Epoch: 3/5  | Epoch loss: 2.597641944885254  | Mean train accuracy: 0.8898947834968567\n",
            "Epoch: 3/5  | Epoch loss: 2.5982308387756348  | Mean train accuracy: 0.8898648619651794\n",
            "Epoch: 3/5  | Epoch loss: 2.6013882160186768  | Mean train accuracy: 0.8898342847824097\n",
            "Epoch: 3/5  | Epoch loss: 2.6052896976470947  | Mean train accuracy: 0.8898054361343384\n",
            "Epoch: 3/5  | Epoch loss: 2.6060757637023926  | Mean train accuracy: 0.8898048400878906\n",
            "Epoch: 3/5  | Epoch loss: 2.608139753341675  | Mean train accuracy: 0.8897742033004761\n",
            "Epoch: 3/5  | Epoch loss: 2.6086854934692383  | Mean train accuracy: 0.8897696733474731\n",
            "Epoch: 3/5  | Epoch loss: 2.6076338291168213  | Mean train accuracy: 0.8897535800933838\n",
            "Epoch: 3/5  | Epoch loss: 2.6096785068511963  | Mean train accuracy: 0.8897444605827332\n",
            "Epoch: 3/5  | Epoch loss: 2.606003999710083  | Mean train accuracy: 0.8897297978401184\n",
            "Epoch: 3/5  | Epoch loss: 2.6067492961883545  | Mean train accuracy: 0.8897053003311157\n",
            "Epoch: 3/5  | Epoch loss: 2.600125551223755  | Mean train accuracy: 0.8896824717521667\n",
            "Epoch: 3/5  | Epoch loss: 2.5976040363311768  | Mean train accuracy: 0.8896723389625549\n",
            "Epoch: 3/5  | Epoch loss: 2.5942254066467285  | Mean train accuracy: 0.8896797299385071\n",
            "Epoch: 3/5  | Epoch loss: 2.5970544815063477  | Mean train accuracy: 0.8896815180778503\n",
            "Epoch: 3/5  | Epoch loss: 2.599527597427368  | Mean train accuracy: 0.889682948589325\n",
            "Epoch: 3/5  | Epoch loss: 2.5967986583709717  | Mean train accuracy: 0.8897066116333008\n",
            "Epoch: 3/5  | Epoch loss: 2.595503091812134  | Mean train accuracy: 0.8897238373756409\n",
            "Epoch: 3/5  | Epoch loss: 2.595259666442871  | Mean train accuracy: 0.889735221862793\n",
            "Epoch: 3/5  | Epoch loss: 2.59389328956604  | Mean train accuracy: 0.8897618651390076\n",
            "Epoch: 3/5  | Epoch loss: 2.594130516052246  | Mean train accuracy: 0.8897899389266968\n",
            "Epoch: 3/5  | Epoch loss: 2.5972135066986084  | Mean train accuracy: 0.8898119926452637\n",
            "Epoch: 3/5  | Epoch loss: 2.597238063812256  | Mean train accuracy: 0.8898292779922485\n",
            "Epoch: 3/5  | Epoch loss: 2.5997607707977295  | Mean train accuracy: 0.8898361325263977\n",
            "Epoch: 3/5  | Epoch loss: 2.5991454124450684  | Mean train accuracy: 0.8898667097091675\n",
            "Epoch: 3/5  | Epoch loss: 2.5980544090270996  | Mean train accuracy: 0.8898909091949463\n",
            "Epoch: 3/5  | Epoch loss: 2.600681781768799  | Mean train accuracy: 0.8899146318435669\n",
            "Epoch: 3/5  | Epoch loss: 2.6048593521118164  | Mean train accuracy: 0.8899268507957458\n",
            "Epoch: 3/5  | Epoch loss: 2.606731414794922  | Mean train accuracy: 0.889947772026062\n",
            "Epoch: 3/5  | Epoch loss: 2.6062381267547607  | Mean train accuracy: 0.8899746537208557\n",
            "Epoch: 3/5  | Epoch loss: 2.6027615070343018  | Mean train accuracy: 0.8899998664855957\n",
            "Epoch: 3/5  | Epoch loss: 2.6034257411956787  | Mean train accuracy: 0.8900232315063477\n",
            "Epoch: 3/5  | Epoch loss: 2.6011483669281006  | Mean train accuracy: 0.8900534510612488\n",
            "Epoch: 3/5  | Epoch loss: 2.60129714012146  | Mean train accuracy: 0.8900786638259888\n",
            "Epoch: 3/5  | Epoch loss: 2.602388858795166  | Mean train accuracy: 0.8900948762893677\n",
            "Epoch: 3/5  | Epoch loss: 2.599850654602051  | Mean train accuracy: 0.8901155591011047\n",
            "Epoch: 3/5  | Epoch loss: 2.5970559120178223  | Mean train accuracy: 0.8901438117027283\n",
            "Epoch: 3/5  | Epoch loss: 2.595325231552124  | Mean train accuracy: 0.8901731371879578\n",
            "Epoch: 3/5  | Epoch loss: 2.5968434810638428  | Mean train accuracy: 0.8902062773704529\n",
            "Epoch: 3/5  | Epoch loss: 2.5967350006103516  | Mean train accuracy: 0.8902374505996704\n",
            "Epoch: 3/5  | Epoch loss: 2.598433494567871  | Mean train accuracy: 0.890275776386261\n",
            "Epoch: 3/5  | Epoch loss: 2.600909948348999  | Mean train accuracy: 0.8903107643127441\n",
            "Epoch: 3/5  | Epoch loss: 2.598475456237793  | Mean train accuracy: 0.8903434872627258\n",
            "Epoch: 3/5  | Epoch loss: 2.599175214767456  | Mean train accuracy: 0.8903681039810181\n",
            "Epoch: 3/5  | Epoch loss: 2.5976040363311768  | Mean train accuracy: 0.8904082775115967\n",
            "Epoch: 3/5  | Epoch loss: 2.597071886062622  | Mean train accuracy: 0.8904500603675842\n",
            "Epoch: 3/5  | Epoch loss: 2.597350597381592  | Mean train accuracy: 0.8904981017112732\n",
            "Epoch: 3/5  | Epoch loss: 2.59688138961792  | Mean train accuracy: 0.8905423283576965\n",
            "Epoch: 3/5  | Epoch loss: 2.5954818725585938  | Mean train accuracy: 0.8905904293060303\n",
            "Epoch: 3/5  | Epoch loss: 2.593506336212158  | Mean train accuracy: 0.8906340599060059\n",
            "Epoch: 3/5  | Epoch loss: 2.5915162563323975  | Mean train accuracy: 0.8906767964363098\n",
            "Epoch: 3/5  | Epoch loss: 2.5904295444488525  | Mean train accuracy: 0.890720546245575\n",
            "Epoch: 3/5  | Epoch loss: 2.592212677001953  | Mean train accuracy: 0.8907542824745178\n",
            "Epoch: 3/5  | Epoch loss: 2.591824769973755  | Mean train accuracy: 0.8907949924468994\n",
            "Epoch: 3/5  | Epoch loss: 2.589911460876465  | Mean train accuracy: 0.8908252716064453\n",
            "Epoch: 3/5  | Epoch loss: 2.592937707901001  | Mean train accuracy: 0.8908542394638062\n",
            "Epoch: 3/5  | Epoch loss: 2.5932140350341797  | Mean train accuracy: 0.8908860683441162\n",
            "Epoch: 3/5  | Epoch loss: 2.5925960540771484  | Mean train accuracy: 0.8909171223640442\n",
            "Epoch: 3/5  | Epoch loss: 2.5929360389709473  | Mean train accuracy: 0.8909485340118408\n",
            "Epoch: 3/5  | Epoch loss: 2.5913937091827393  | Mean train accuracy: 0.8909794092178345\n",
            "Epoch: 3/5  | Epoch loss: 2.5908806324005127  | Mean train accuracy: 0.891008198261261\n",
            "Epoch: 3/5  | Epoch loss: 2.590226650238037  | Mean train accuracy: 0.8910369873046875\n",
            "Epoch: 3/5  | Epoch loss: 2.5908381938934326  | Mean train accuracy: 0.8910587430000305\n",
            "Epoch: 3/5  | Epoch loss: 2.5893216133117676  | Mean train accuracy: 0.8910853862762451\n",
            "Epoch: 3/5  | Epoch loss: 2.5906004905700684  | Mean train accuracy: 0.8911062479019165\n",
            "Epoch: 3/5  | Epoch loss: 2.591066598892212  | Mean train accuracy: 0.8911283612251282\n",
            "Epoch: 3/5  | Epoch loss: 2.591095209121704  | Mean train accuracy: 0.8911522030830383\n",
            "Epoch: 3/5  | Epoch loss: 2.590833902359009  | Mean train accuracy: 0.8911709785461426\n",
            "Epoch: 3/5  | Epoch loss: 2.5900001525878906  | Mean train accuracy: 0.8911916017532349\n",
            "Epoch: 3/5  | Epoch loss: 2.5911054611206055  | Mean train accuracy: 0.8912124633789062\n",
            "Epoch: 3/5  | Epoch loss: 2.5936880111694336  | Mean train accuracy: 0.8912301063537598\n",
            "Epoch: 3/5  | Epoch loss: 2.595184326171875  | Mean train accuracy: 0.8912515044212341\n",
            "Epoch: 3/5  | Epoch loss: 2.5951757431030273  | Mean train accuracy: 0.8912714719772339\n",
            "Epoch: 3/5  | Epoch loss: 2.595214605331421  | Mean train accuracy: 0.8912854790687561\n",
            "Epoch: 3/5  | Epoch loss: 2.5953965187072754  | Mean train accuracy: 0.8913024067878723\n",
            "Epoch: 3/5  | Epoch loss: 2.5931479930877686  | Mean train accuracy: 0.8913159966468811\n",
            "Epoch: 3/5  | Epoch loss: 2.5946109294891357  | Mean train accuracy: 0.8913300633430481\n",
            "Epoch: 3/5  | Epoch loss: 2.595797300338745  | Mean train accuracy: 0.8913357853889465\n",
            "Epoch: 3/5  | Epoch loss: 2.5955798625946045  | Mean train accuracy: 0.891342282295227\n",
            "Epoch: 3/5  | Epoch loss: 2.596583366394043  | Mean train accuracy: 0.8913484811782837\n",
            "Epoch: 3/5  | Epoch loss: 2.595889091491699  | Mean train accuracy: 0.891352653503418\n",
            "Epoch: 3/5  | Epoch loss: 2.5933797359466553  | Mean train accuracy: 0.8913601040840149\n",
            "Epoch: 3/5  | Epoch loss: 2.592517852783203  | Mean train accuracy: 0.8913660049438477\n",
            "Epoch: 3/5  | Epoch loss: 2.5915844440460205  | Mean train accuracy: 0.8913750052452087\n",
            "Epoch: 3/5  | Epoch loss: 2.590514898300171  | Mean train accuracy: 0.8913858532905579\n",
            "Epoch: 3/5  | Epoch loss: 2.590409517288208  | Mean train accuracy: 0.8913965821266174\n",
            "Epoch: 3/5  | Epoch loss: 2.5894134044647217  | Mean train accuracy: 0.8914085626602173\n",
            "Epoch: 3/5  | Epoch loss: 2.5882649421691895  | Mean train accuracy: 0.8914246559143066\n",
            "Epoch: 3/5  | Epoch loss: 2.589232921600342  | Mean train accuracy: 0.8914410471916199\n",
            "Epoch: 3/5  | Epoch loss: 2.590423822402954  | Mean train accuracy: 0.8914605975151062\n",
            "Epoch: 3/5  | Epoch loss: 2.589277505874634  | Mean train accuracy: 0.8914808034896851\n",
            "Epoch: 3/5  | Epoch loss: 2.589533805847168  | Mean train accuracy: 0.8915019631385803\n",
            "Epoch: 3/5  | Epoch loss: 2.5895867347717285  | Mean train accuracy: 0.8915255665779114\n",
            "Epoch: 3/5  | Epoch loss: 2.5901811122894287  | Mean train accuracy: 0.8915512561798096\n",
            "Epoch: 3/5  | Epoch loss: 2.589298725128174  | Mean train accuracy: 0.8915761709213257\n",
            "Epoch: 3/5  | Epoch loss: 2.5894999504089355  | Mean train accuracy: 0.8915999531745911\n",
            "Epoch: 3/5  | Epoch loss: 2.589210033416748  | Mean train accuracy: 0.8916212320327759\n",
            "Epoch: 3/5  | Epoch loss: 2.5886480808258057  | Mean train accuracy: 0.8916440010070801\n",
            "Epoch: 3/5  | Epoch loss: 2.5884902477264404  | Mean train accuracy: 0.8916646242141724\n",
            "Epoch: 3/5  | Epoch loss: 2.5883870124816895  | Mean train accuracy: 0.8916870355606079\n",
            "Epoch: 3/5  | Epoch loss: 2.5896403789520264  | Mean train accuracy: 0.8917074203491211\n",
            "Epoch: 3/5  | Epoch loss: 2.588658332824707  | Mean train accuracy: 0.8917267322540283\n",
            "Epoch: 3/5  | Epoch loss: 2.588689088821411  | Mean train accuracy: 0.8917466998100281\n",
            "Epoch: 3/5  | Epoch loss: 2.5878052711486816  | Mean train accuracy: 0.8917698860168457\n",
            "Epoch: 3/5  | Epoch loss: 2.5882482528686523  | Mean train accuracy: 0.8917911648750305\n",
            "Epoch: 3/5  | Epoch loss: 2.588519334793091  | Mean train accuracy: 0.8918159008026123\n",
            "Epoch: 3/5  | Epoch loss: 2.5875444412231445  | Mean train accuracy: 0.8918418884277344\n",
            "Epoch: 3/5  | Epoch loss: 2.5872702598571777  | Mean train accuracy: 0.8918635249137878\n",
            "Epoch: 3/5  | Epoch loss: 2.58713436126709  | Mean train accuracy: 0.8918843269348145\n",
            "Epoch: 3/5  | Epoch loss: 2.586954116821289  | Mean train accuracy: 0.8919060230255127\n",
            "Epoch: 3/5  | Epoch loss: 2.5866587162017822  | Mean train accuracy: 0.8919273614883423\n",
            "Epoch: 3/5  | Epoch loss: 2.5862159729003906  | Mean train accuracy: 0.8919496536254883\n",
            "Epoch: 3/5  | Epoch loss: 2.586479902267456  | Mean train accuracy: 0.8919714093208313\n",
            "Epoch: 3/5  | Epoch loss: 2.586263656616211  | Mean train accuracy: 0.8919949531555176\n",
            "Epoch: 3/5  | Epoch loss: 2.586313486099243  | Mean train accuracy: 0.892021656036377\n",
            "Epoch: 3/5  | Epoch loss: 2.5859858989715576  | Mean train accuracy: 0.8920391798019409\n",
            "Epoch: 3/5  | Epoch loss: 2.5854852199554443  | Mean train accuracy: 0.8920606970787048\n",
            "Epoch: 3/5  | Epoch loss: 2.5853302478790283  | Mean train accuracy: 0.8920771479606628\n",
            "Epoch: 3/5  | Epoch loss: 2.5847795009613037  | Mean train accuracy: 0.8920981287956238\n",
            "Epoch: 3/5  | Epoch loss: 2.5846645832061768  | Mean train accuracy: 0.8921205997467041\n",
            "Epoch: 3/5  | Epoch loss: 2.584170341491699  | Mean train accuracy: 0.8921405673027039\n",
            "Epoch: 3/5  | Epoch loss: 2.584092140197754  | Mean train accuracy: 0.8921612501144409\n",
            "Epoch: 3/5  | Epoch loss: 2.584397077560425  | Mean train accuracy: 0.892179548740387\n",
            "Epoch: 3/5  | Epoch loss: 2.5849602222442627  | Mean train accuracy: 0.892196536064148\n",
            "Epoch: 3/5  | Epoch loss: 2.5857057571411133  | Mean train accuracy: 0.8922129273414612\n",
            "Epoch: 3/5  | Epoch loss: 2.5849413871765137  | Mean train accuracy: 0.8922263383865356\n",
            "Epoch: 3/5  | Epoch loss: 2.5853700637817383  | Mean train accuracy: 0.8922418355941772\n",
            "Epoch: 3/5  | Epoch loss: 2.5854904651641846  | Mean train accuracy: 0.8922526836395264\n",
            "Epoch: 3/5  | Epoch loss: 2.585231065750122  | Mean train accuracy: 0.8922597765922546\n",
            "Epoch: 3/5  | Epoch loss: 2.5847065448760986  | Mean train accuracy: 0.8922644257545471\n",
            "Epoch: 3/5  | Epoch loss: 2.583977699279785  | Mean train accuracy: 0.8922685980796814\n",
            "Epoch: 3/5  | Epoch loss: 2.58294677734375  | Mean train accuracy: 0.8922756910324097\n",
            "Epoch: 3/5  | Epoch loss: 2.5820791721343994  | Mean train accuracy: 0.8922817707061768\n",
            "Epoch: 3/5  | Epoch loss: 2.5817110538482666  | Mean train accuracy: 0.8922858834266663\n",
            "Epoch: 3/5  | Epoch loss: 2.582381010055542  | Mean train accuracy: 0.8922902941703796\n",
            "Epoch: 3/5  | Epoch loss: 2.5817947387695312  | Mean train accuracy: 0.8922914266586304\n",
            "Epoch: 3/5  | Epoch loss: 2.5818679332733154  | Mean train accuracy: 0.8922924995422363\n",
            "Epoch: 3/5  | Epoch loss: 2.5822255611419678  | Mean train accuracy: 0.8922953009605408\n",
            "Epoch: 3/5  | Epoch loss: 2.581878423690796  | Mean train accuracy: 0.8922995328903198\n",
            "Epoch: 3/5  | Epoch loss: 2.581474542617798  | Mean train accuracy: 0.8922999501228333\n",
            "Epoch: 3/5  | Epoch loss: 2.5816595554351807  | Mean train accuracy: 0.8923032879829407\n",
            "Epoch: 3/5  | Epoch loss: 2.582002639770508  | Mean train accuracy: 0.892306387424469\n",
            "Epoch: 3/5  | Epoch loss: 2.5814621448516846  | Mean train accuracy: 0.8923072814941406\n",
            "Epoch: 3/5  | Epoch loss: 2.580986738204956  | Mean train accuracy: 0.8923087120056152\n",
            "Epoch: 3/5  | Epoch loss: 2.5806806087493896  | Mean train accuracy: 0.8923096060752869\n",
            "Epoch: 3/5  | Epoch loss: 2.5807318687438965  | Mean train accuracy: 0.8923138976097107\n",
            "Epoch: 3/5  | Epoch loss: 2.5801167488098145  | Mean train accuracy: 0.8923191428184509\n",
            "Epoch: 3/5  | Epoch loss: 2.578991174697876  | Mean train accuracy: 0.8923236131668091\n",
            "Epoch: 3/5  | Epoch loss: 2.579019069671631  | Mean train accuracy: 0.8923301696777344\n",
            "Epoch: 3/5  | Epoch loss: 2.579076051712036  | Mean train accuracy: 0.8923347592353821\n",
            "Epoch: 3/5  | Epoch loss: 2.578580379486084  | Mean train accuracy: 0.8923402428627014\n",
            "Epoch: 3/5  | Epoch loss: 2.5775556564331055  | Mean train accuracy: 0.8923493027687073\n",
            "Epoch: 3/5  | Epoch loss: 2.577472686767578  | Mean train accuracy: 0.8923572897911072\n",
            "Epoch: 3/5  | Epoch loss: 2.5764310359954834  | Mean train accuracy: 0.8923652172088623\n",
            "Epoch: 3/5  | Epoch loss: 2.5760695934295654  | Mean train accuracy: 0.8923741579055786\n",
            "Epoch: 3/5  | Epoch loss: 2.57446026802063  | Mean train accuracy: 0.8923839926719666\n",
            "Epoch: 3/5  | Epoch loss: 2.5747740268707275  | Mean train accuracy: 0.8923940062522888\n",
            "Epoch: 3/5  | Epoch loss: 2.5749473571777344  | Mean train accuracy: 0.8924081921577454\n",
            "Epoch: 3/5  | Epoch loss: 2.574899435043335  | Mean train accuracy: 0.8924230337142944\n",
            "Epoch: 3/5  | Epoch loss: 2.5759570598602295  | Mean train accuracy: 0.8924384713172913\n",
            "Epoch: 3/5  | Epoch loss: 2.575613021850586  | Mean train accuracy: 0.8924555778503418\n",
            "Epoch: 3/5  | Epoch loss: 2.5753695964813232  | Mean train accuracy: 0.8924728631973267\n",
            "Epoch: 3/5  | Epoch loss: 2.577127456665039  | Mean train accuracy: 0.8924899697303772\n",
            "Epoch: 3/5  | Epoch loss: 2.5759437084198  | Mean train accuracy: 0.8925072550773621\n",
            "Epoch: 3/5  | Epoch loss: 2.575038433074951  | Mean train accuracy: 0.8925264477729797\n",
            "Epoch: 3/5  | Epoch loss: 2.5752007961273193  | Mean train accuracy: 0.8925462365150452\n",
            "Epoch: 3/5  | Epoch loss: 2.5759379863739014  | Mean train accuracy: 0.8925638794898987\n",
            "Epoch: 3/5  | Epoch loss: 2.576080799102783  | Mean train accuracy: 0.892585277557373\n",
            "Epoch: 3/5  | Epoch loss: 2.576364040374756  | Mean train accuracy: 0.8926047682762146\n",
            "Epoch: 3/5  | Epoch loss: 2.5767674446105957  | Mean train accuracy: 0.8926246762275696\n",
            "Epoch: 3/5  | Epoch loss: 2.5763204097747803  | Mean train accuracy: 0.8926429152488708\n",
            "Epoch: 3/5  | Epoch loss: 2.576749801635742  | Mean train accuracy: 0.8926558494567871\n",
            "Epoch: 3/5  | Epoch loss: 2.576838731765747  | Mean train accuracy: 0.8926742672920227\n",
            "Epoch: 3/5  | Epoch loss: 2.5773775577545166  | Mean train accuracy: 0.8926849365234375\n",
            "Epoch: 3/5  | Epoch loss: 2.5769612789154053  | Mean train accuracy: 0.8926993608474731\n",
            "Epoch: 3/5  | Epoch loss: 2.5771872997283936  | Mean train accuracy: 0.8927105665206909\n",
            "Epoch: 3/5  | Epoch loss: 2.5776469707489014  | Mean train accuracy: 0.8927217721939087\n",
            "Epoch: 3/5  | Epoch loss: 2.5777926445007324  | Mean train accuracy: 0.8927293419837952\n",
            "Epoch: 3/5  | Epoch loss: 2.577638626098633  | Mean train accuracy: 0.8927379250526428\n",
            "Epoch: 3/5  | Epoch loss: 2.5777056217193604  | Mean train accuracy: 0.8927450180053711\n",
            "Epoch: 3/5  | Epoch loss: 2.578099489212036  | Mean train accuracy: 0.892752468585968\n",
            "Epoch: 3/5  | Epoch loss: 2.5776498317718506  | Mean train accuracy: 0.89276123046875\n",
            "Epoch: 3/5  | Epoch loss: 2.5772294998168945  | Mean train accuracy: 0.8927706480026245\n",
            "Epoch: 3/5  | Epoch loss: 2.5780351161956787  | Mean train accuracy: 0.89278244972229\n",
            "Epoch: 3/5  | Epoch loss: 2.577646255493164  | Mean train accuracy: 0.8927944302558899\n",
            "Epoch: 3/5  | Epoch loss: 2.5769271850585938  | Mean train accuracy: 0.8928048014640808\n",
            "Epoch: 3/5  | Epoch loss: 2.576374053955078  | Mean train accuracy: 0.892817497253418\n",
            "Epoch: 3/5  | Epoch loss: 2.5766000747680664  | Mean train accuracy: 0.892831027507782\n",
            "Epoch: 3/5  | Epoch loss: 2.5766124725341797  | Mean train accuracy: 0.8928454518318176\n",
            "Epoch: 3/5  | Epoch loss: 2.576133966445923  | Mean train accuracy: 0.8928579092025757\n",
            "Epoch: 3/5  | Epoch loss: 2.576050281524658  | Mean train accuracy: 0.8928746581077576\n",
            "Epoch: 3/5  | Epoch loss: 2.575686454772949  | Mean train accuracy: 0.8928918242454529\n",
            "Epoch: 3/5  | Epoch loss: 2.5761449337005615  | Mean train accuracy: 0.8929088711738586\n",
            "Epoch: 3/5  | Epoch loss: 2.57550311088562  | Mean train accuracy: 0.8929260969161987\n",
            "Epoch: 3/5  | Epoch loss: 2.5753653049468994  | Mean train accuracy: 0.8929422497749329\n",
            "Epoch: 3/5  | Epoch loss: 2.5759644508361816  | Mean train accuracy: 0.8929576277732849\n",
            "Epoch: 3/5  | Epoch loss: 2.575727701187134  | Mean train accuracy: 0.8929729461669922\n",
            "Epoch: 3/5  | Epoch loss: 2.575160026550293  | Mean train accuracy: 0.8929896950721741\n",
            "Epoch: 3/5  | Epoch loss: 2.5758795738220215  | Mean train accuracy: 0.8930047154426575\n",
            "Epoch: 3/5  | Epoch loss: 2.576814889907837  | Mean train accuracy: 0.89301598072052\n",
            "Epoch: 3/5  | Epoch loss: 2.5762381553649902  | Mean train accuracy: 0.8930296301841736\n",
            "Epoch: 3/5  | Epoch loss: 2.576002359390259  | Mean train accuracy: 0.8930419683456421\n",
            "Epoch: 3/5  | Epoch loss: 2.575965166091919  | Mean train accuracy: 0.89305180311203\n",
            "Epoch: 3/5  | Epoch loss: 2.576463222503662  | Mean train accuracy: 0.8930613398551941\n",
            "Epoch: 3/5  | Epoch loss: 2.5767886638641357  | Mean train accuracy: 0.8930705189704895\n",
            "Epoch: 3/5  | Epoch loss: 2.576890468597412  | Mean train accuracy: 0.8930792808532715\n",
            "Epoch: 3/5  | Epoch loss: 2.576836585998535  | Mean train accuracy: 0.8930855393409729\n",
            "Epoch: 3/5  | Epoch loss: 2.576158285140991  | Mean train accuracy: 0.8930906057357788\n",
            "Epoch: 3/5  | Epoch loss: 2.5768885612487793  | Mean train accuracy: 0.8930968046188354\n",
            "Epoch: 3/5  | Epoch loss: 2.577298402786255  | Mean train accuracy: 0.8931012153625488\n",
            "Epoch: 3/5  | Epoch loss: 2.5776519775390625  | Mean train accuracy: 0.8931052088737488\n",
            "Epoch: 3/5  | Epoch loss: 2.577143669128418  | Mean train accuracy: 0.893106997013092\n",
            "Epoch: 3/5  | Epoch loss: 2.5769028663635254  | Mean train accuracy: 0.8931076526641846\n",
            "Epoch: 3/5  | Epoch loss: 2.577531576156616  | Mean train accuracy: 0.8931089639663696\n",
            "Epoch: 3/5  | Epoch loss: 2.577059030532837  | Mean train accuracy: 0.8931100964546204\n",
            "Epoch: 3/5  | Epoch loss: 2.576735019683838  | Mean train accuracy: 0.8931118249893188\n",
            "Epoch: 3/5  | Epoch loss: 2.5771305561065674  | Mean train accuracy: 0.8931161761283875\n",
            "Epoch: 3/5  | Epoch loss: 2.5772452354431152  | Mean train accuracy: 0.8931196331977844\n",
            "Epoch: 3/5  | Epoch loss: 2.5771076679229736  | Mean train accuracy: 0.893121063709259\n",
            "Epoch: 3/5  | Epoch loss: 2.577561616897583  | Mean train accuracy: 0.8931266069412231\n",
            "Epoch: 3/5  | Epoch loss: 2.5772411823272705  | Mean train accuracy: 0.893129825592041\n",
            "Epoch: 3/5  | Epoch loss: 2.5777266025543213  | Mean train accuracy: 0.8931321501731873\n",
            "Epoch: 3/5  | Epoch loss: 2.577666759490967  | Mean train accuracy: 0.8931386470794678\n",
            "Epoch: 3/5  | Epoch loss: 2.5776681900024414  | Mean train accuracy: 0.8931445479393005\n",
            "Epoch: 3/5  | Epoch loss: 2.5777053833007812  | Mean train accuracy: 0.8931513428688049\n",
            "Epoch: 3/5  | Epoch loss: 2.5780537128448486  | Mean train accuracy: 0.8931583166122437\n",
            "Epoch: 3/5  | Epoch loss: 2.577871322631836  | Mean train accuracy: 0.8931668400764465\n",
            "Epoch: 3/5  | Epoch loss: 2.578756093978882  | Mean train accuracy: 0.8931739330291748\n",
            "Epoch: 3/5  | Epoch loss: 2.578791856765747  | Mean train accuracy: 0.8931825757026672\n",
            "Epoch: 3/5  | Epoch loss: 2.578944683074951  | Mean train accuracy: 0.8931879997253418\n",
            "Epoch: 3/5  | Epoch loss: 2.579105854034424  | Mean train accuracy: 0.8931949138641357\n",
            "Epoch: 3/5  | Epoch loss: 2.578913688659668  | Mean train accuracy: 0.8932022452354431\n",
            "Epoch: 3/5  | Epoch loss: 2.57881760597229  | Mean train accuracy: 0.8932073712348938\n",
            "Epoch: 3/5  | Epoch loss: 2.5792975425720215  | Mean train accuracy: 0.8932124972343445\n",
            "Epoch: 3/5  | Epoch loss: 2.5797200202941895  | Mean train accuracy: 0.893217921257019\n",
            "Epoch: 3/5  | Epoch loss: 2.579881191253662  | Mean train accuracy: 0.89322429895401\n",
            "Epoch: 3/5  | Epoch loss: 2.579944372177124  | Mean train accuracy: 0.8932295441627502\n",
            "Epoch: 3/5  | Epoch loss: 2.580502986907959  | Mean train accuracy: 0.8932350277900696\n",
            "Epoch: 3/5  | Epoch loss: 2.580557107925415  | Mean train accuracy: 0.8932391405105591\n",
            "Epoch: 3/5  | Epoch loss: 2.5802805423736572  | Mean train accuracy: 0.8932451009750366\n",
            "Epoch: 3/5  | Epoch loss: 2.5803983211517334  | Mean train accuracy: 0.8932486772537231\n",
            "Epoch: 3/5  | Epoch loss: 2.5801939964294434  | Mean train accuracy: 0.8932536840438843\n",
            "Epoch: 3/5  | Epoch loss: 2.5807812213897705  | Mean train accuracy: 0.8932584524154663\n",
            "Epoch: 3/5  | Epoch loss: 2.580869674682617  | Mean train accuracy: 0.8932633996009827\n",
            "Epoch: 3/5  | Epoch loss: 2.5808043479919434  | Mean train accuracy: 0.8932691216468811\n",
            "Epoch: 3/5  | Epoch loss: 2.5811843872070312  | Mean train accuracy: 0.8932738900184631\n",
            "Epoch: 3/5  | Epoch loss: 2.581646680831909  | Mean train accuracy: 0.8932784795761108\n",
            "Epoch: 3/5  | Epoch loss: 2.581984043121338  | Mean train accuracy: 0.8932840824127197\n",
            "Epoch: 3/5  | Epoch loss: 2.5820395946502686  | Mean train accuracy: 0.8932896852493286\n",
            "Epoch: 3/5  | Epoch loss: 2.5816476345062256  | Mean train accuracy: 0.8932965993881226\n",
            "Epoch: 3/5  | Epoch loss: 2.5812830924987793  | Mean train accuracy: 0.8933019638061523\n",
            "Epoch: 3/5  | Epoch loss: 2.5812838077545166  | Mean train accuracy: 0.8933084607124329\n",
            "Epoch: 3/5  | Epoch loss: 2.5809404850006104  | Mean train accuracy: 0.8933169841766357\n",
            "Epoch: 3/5  | Epoch loss: 2.580965995788574  | Mean train accuracy: 0.8933233022689819\n",
            "Epoch: 3/5  | Epoch loss: 2.581052541732788  | Mean train accuracy: 0.8933287262916565\n",
            "Epoch: 3/5  | Epoch loss: 2.5804567337036133  | Mean train accuracy: 0.8933346271514893\n",
            "Epoch: 3/5  | Epoch loss: 2.5808870792388916  | Mean train accuracy: 0.8933419585227966\n",
            "Epoch: 3/5  | Epoch loss: 2.5805628299713135  | Mean train accuracy: 0.8933500647544861\n",
            "Epoch: 3/5  | Epoch loss: 2.5806665420532227  | Mean train accuracy: 0.8933571577072144\n",
            "Epoch: 3/5  | Epoch loss: 2.580528497695923  | Mean train accuracy: 0.8933652639389038\n",
            "Epoch: 3/5  | Epoch loss: 2.579899549484253  | Mean train accuracy: 0.8933753371238708\n",
            "Epoch: 3/5  | Epoch loss: 2.579974412918091  | Mean train accuracy: 0.8933849334716797\n",
            "Epoch: 3/5  | Epoch loss: 2.580258846282959  | Mean train accuracy: 0.8933942317962646\n",
            "Epoch: 3/5  | Epoch loss: 2.5798561573028564  | Mean train accuracy: 0.8934025168418884\n",
            "Epoch: 3/5  | Epoch loss: 2.5801327228546143  | Mean train accuracy: 0.893410861492157\n",
            "Epoch: 3/5  | Epoch loss: 2.579826593399048  | Mean train accuracy: 0.8934199810028076\n",
            "Epoch: 3/5  | Epoch loss: 2.579979181289673  | Mean train accuracy: 0.8934280276298523\n",
            "Epoch: 3/5  | Epoch loss: 2.579162120819092  | Mean train accuracy: 0.8934366703033447\n",
            "Epoch: 3/5  | Epoch loss: 2.5791828632354736  | Mean train accuracy: 0.8934439420700073\n",
            "Epoch: 3/5  | Epoch loss: 2.5788958072662354  | Mean train accuracy: 0.8934499621391296\n",
            "Epoch: 3/5  | Epoch loss: 2.579073429107666  | Mean train accuracy: 0.8934550881385803\n",
            "Epoch: 3/5  | Epoch loss: 2.578666925430298  | Mean train accuracy: 0.8934603333473206\n",
            "Epoch: 3/5  | Epoch loss: 2.578089475631714  | Mean train accuracy: 0.8934661149978638\n",
            "Epoch: 3/5  | Epoch loss: 2.5777242183685303  | Mean train accuracy: 0.8934710025787354\n",
            "Epoch: 3/5  | Epoch loss: 2.5784053802490234  | Mean train accuracy: 0.8934751749038696\n",
            "Epoch: 3/5  | Epoch loss: 2.578327178955078  | Mean train accuracy: 0.8934803009033203\n",
            "Epoch: 3/5  | Epoch loss: 2.5783965587615967  | Mean train accuracy: 0.8934866786003113\n",
            "Epoch: 3/5  | Epoch loss: 2.578756332397461  | Mean train accuracy: 0.8934927582740784\n",
            "Epoch: 3/5  | Epoch loss: 2.578629493713379  | Mean train accuracy: 0.8934968709945679\n",
            "Epoch: 3/5  | Epoch loss: 2.5785605907440186  | Mean train accuracy: 0.8935030698776245\n",
            "Epoch: 3/5  | Epoch loss: 2.578542470932007  | Mean train accuracy: 0.8935098648071289\n",
            "Epoch: 3/5  | Epoch loss: 2.578881025314331  | Mean train accuracy: 0.893515408039093\n",
            "Epoch: 3/5  | Epoch loss: 2.579620599746704  | Mean train accuracy: 0.8935198187828064\n",
            "Epoch: 3/5  | Epoch loss: 2.579598903656006  | Mean train accuracy: 0.893524169921875\n",
            "Epoch: 3/5  | Epoch loss: 2.579439401626587  | Mean train accuracy: 0.8935281038284302\n",
            "Epoch: 3/5  | Epoch loss: 2.5792689323425293  | Mean train accuracy: 0.8935315608978271\n",
            "Epoch: 3/5  | Epoch loss: 2.5787160396575928  | Mean train accuracy: 0.8935349583625793\n",
            "Epoch: 3/5  | Epoch loss: 2.5789968967437744  | Mean train accuracy: 0.8935384750366211\n",
            "Epoch: 3/5  | Epoch loss: 2.578538179397583  | Mean train accuracy: 0.8935416340827942\n",
            "Epoch: 3/5  | Epoch loss: 2.5785930156707764  | Mean train accuracy: 0.893545925617218\n",
            "Epoch: 3/5  | Epoch loss: 2.578946352005005  | Mean train accuracy: 0.8935512900352478\n",
            "Epoch: 3/5  | Epoch loss: 2.5792317390441895  | Mean train accuracy: 0.893557608127594\n",
            "Epoch: 3/5  | Epoch loss: 2.5792253017425537  | Mean train accuracy: 0.8935631513595581\n",
            "Epoch: 3/5  | Epoch loss: 2.5793955326080322  | Mean train accuracy: 0.8935702443122864\n",
            "Epoch: 3/5  | Epoch loss: 2.579162120819092  | Mean train accuracy: 0.8935778141021729\n",
            "Epoch: 3/5  | Epoch loss: 2.579090118408203  | Mean train accuracy: 0.8935872912406921\n",
            "Epoch: 3/5  | Epoch loss: 2.579422950744629  | Mean train accuracy: 0.8935959339141846\n",
            "Epoch: 3/5  | Epoch loss: 2.57983136177063  | Mean train accuracy: 0.8936036825180054\n",
            "Epoch: 3/5  | Epoch loss: 2.5798611640930176  | Mean train accuracy: 0.89361172914505\n",
            "Epoch: 3/5  | Epoch loss: 2.5800046920776367  | Mean train accuracy: 0.8936192989349365\n",
            "Epoch: 3/5  | Epoch loss: 2.5798234939575195  | Mean train accuracy: 0.893627941608429\n",
            "Epoch: 3/5  | Epoch loss: 2.580195426940918  | Mean train accuracy: 0.8936363458633423\n",
            "Epoch: 3/5  | Epoch loss: 2.580108404159546  | Mean train accuracy: 0.8936448693275452\n",
            "Epoch: 3/5  | Epoch loss: 2.5798308849334717  | Mean train accuracy: 0.8936540484428406\n",
            "Epoch: 3/5  | Epoch loss: 2.579737901687622  | Mean train accuracy: 0.8936620950698853\n",
            "Epoch: 3/5  | Epoch loss: 2.5798492431640625  | Mean train accuracy: 0.8936710357666016\n",
            "Epoch: 3/5  | Epoch loss: 2.5794694423675537  | Mean train accuracy: 0.8936778903007507\n",
            "Epoch: 3/5  | Epoch loss: 2.5792243480682373  | Mean train accuracy: 0.8936854004859924\n",
            "Epoch: 3/5  | Epoch loss: 2.578781843185425  | Mean train accuracy: 0.8936917781829834\n",
            "Epoch: 3/5  | Epoch loss: 2.5786056518554688  | Mean train accuracy: 0.8936999440193176\n",
            "Epoch: 3/5  | Epoch loss: 2.5787551403045654  | Mean train accuracy: 0.8937067985534668\n",
            "Epoch: 3/5  | Epoch loss: 2.5792245864868164  | Mean train accuracy: 0.8937150835990906\n",
            "Epoch: 3/5  | Epoch loss: 2.5789854526519775  | Mean train accuracy: 0.8937231302261353\n",
            "Epoch: 3/5  | Epoch loss: 2.578601837158203  | Mean train accuracy: 0.893729567527771\n",
            "Epoch: 3/5  | Epoch loss: 2.5789971351623535  | Mean train accuracy: 0.8937345743179321\n",
            "Epoch: 3/5  | Epoch loss: 2.5784993171691895  | Mean train accuracy: 0.8937411308288574\n",
            "Epoch: 3/5  | Epoch loss: 2.5783209800720215  | Mean train accuracy: 0.8937467336654663\n",
            "Epoch: 3/5  | Epoch loss: 2.578157424926758  | Mean train accuracy: 0.8937531113624573\n",
            "Epoch: 3/5  | Epoch loss: 2.578350067138672  | Mean train accuracy: 0.8937600255012512\n",
            "Epoch: 3/5  | Epoch loss: 2.577970266342163  | Mean train accuracy: 0.8937674164772034\n",
            "Epoch: 3/5  | Epoch loss: 2.5779972076416016  | Mean train accuracy: 0.8937742114067078\n",
            "Epoch: 3/5  | Epoch loss: 2.577993154525757  | Mean train accuracy: 0.8937802910804749\n",
            "Epoch: 3/5  | Epoch loss: 2.5785560607910156  | Mean train accuracy: 0.8937861919403076\n",
            "Epoch: 3/5  | Epoch loss: 2.578885316848755  | Mean train accuracy: 0.8937925100326538\n",
            "Epoch: 3/5  | Epoch loss: 2.5784382820129395  | Mean train accuracy: 0.8937985897064209\n",
            "Epoch: 3/5  | Epoch loss: 2.578768253326416  | Mean train accuracy: 0.8938043117523193\n",
            "Epoch: 3/5  | Epoch loss: 2.578723907470703  | Mean train accuracy: 0.8938094973564148\n",
            "Epoch: 3/5  | Epoch loss: 2.5788748264312744  | Mean train accuracy: 0.8938146233558655\n",
            "Epoch: 3/5  | Epoch loss: 2.5787391662597656  | Mean train accuracy: 0.8938196897506714\n",
            "Epoch: 3/5  | Epoch loss: 2.5790867805480957  | Mean train accuracy: 0.8938233256340027\n",
            "Epoch: 3/5  | Epoch loss: 2.579146385192871  | Mean train accuracy: 0.8938277363777161\n",
            "Epoch: 3/5  | Epoch loss: 2.5794718265533447  | Mean train accuracy: 0.8938303589820862\n",
            "Epoch: 3/5  | Epoch loss: 2.5792765617370605  | Mean train accuracy: 0.8938340544700623\n",
            "Epoch: 3/5  | Epoch loss: 2.5795819759368896  | Mean train accuracy: 0.8938361406326294\n",
            "Epoch: 3/5  | Epoch loss: 2.5798537731170654  | Mean train accuracy: 0.893837571144104\n",
            "Epoch: 3/5  | Epoch loss: 2.5805981159210205  | Mean train accuracy: 0.8938380479812622\n",
            "Epoch: 3/5  | Epoch loss: 2.580709457397461  | Mean train accuracy: 0.893837571144104\n",
            "Epoch: 3/5  | Epoch loss: 2.58038592338562  | Mean train accuracy: 0.8938367366790771\n",
            "Epoch: 3/5  | Epoch loss: 2.580777645111084  | Mean train accuracy: 0.8938356637954712\n",
            "Epoch: 3/5  | Epoch loss: 2.580549716949463  | Mean train accuracy: 0.8938342332839966\n",
            "Epoch: 3/5  | Epoch loss: 2.5805840492248535  | Mean train accuracy: 0.8938322067260742\n",
            "Epoch: 3/5  | Epoch loss: 2.5810089111328125  | Mean train accuracy: 0.8938306570053101\n",
            "Epoch: 3/5  | Epoch loss: 2.5811305046081543  | Mean train accuracy: 0.8938297629356384\n",
            "Epoch: 3/5  | Epoch loss: 2.58100962638855  | Mean train accuracy: 0.8938276767730713\n",
            "Epoch: 3/5  | Epoch loss: 2.5808608531951904  | Mean train accuracy: 0.8938265442848206\n",
            "Epoch: 3/5  | Epoch loss: 2.580693483352661  | Mean train accuracy: 0.8938271403312683\n",
            "Epoch: 3/5  | Epoch loss: 2.580634117126465  | Mean train accuracy: 0.8938256502151489\n",
            "Epoch: 3/5  | Epoch loss: 2.5806169509887695  | Mean train accuracy: 0.8938251733779907\n",
            "Epoch: 3/5  | Epoch loss: 2.580705404281616  | Mean train accuracy: 0.8938248753547668\n",
            "Epoch: 3/5  | Epoch loss: 2.5803799629211426  | Mean train accuracy: 0.893825113773346\n",
            "Epoch: 3/5  | Epoch loss: 2.580359935760498  | Mean train accuracy: 0.8938277959823608\n",
            "Epoch: 3/5  | Epoch loss: 2.580233573913574  | Mean train accuracy: 0.8938304781913757\n",
            "Epoch: 3/5  | Epoch loss: 2.5798122882843018  | Mean train accuracy: 0.8938313126564026\n",
            "Epoch: 3/5  | Epoch loss: 2.579697370529175  | Mean train accuracy: 0.8938327431678772\n",
            "Epoch: 3/5  | Epoch loss: 2.5798962116241455  | Mean train accuracy: 0.8938353657722473\n",
            "Epoch: 3/5  | Epoch loss: 2.579679489135742  | Mean train accuracy: 0.8938381671905518\n",
            "Epoch: 3/5  | Epoch loss: 2.5797619819641113  | Mean train accuracy: 0.8938409686088562\n",
            "Epoch: 3/5  | Epoch loss: 2.5803511142730713  | Mean train accuracy: 0.8938441872596741\n",
            "Epoch: 3/5  | Epoch loss: 2.580504894256592  | Mean train accuracy: 0.8938487768173218\n",
            "Epoch: 3/5  | Epoch loss: 2.5801727771759033  | Mean train accuracy: 0.8938530087471008\n",
            "Epoch: 3/5  | Epoch loss: 2.5800983905792236  | Mean train accuracy: 0.8938581347465515\n",
            "Epoch: 3/5  | Epoch loss: 2.580242872238159  | Mean train accuracy: 0.893863320350647\n",
            "Epoch: 3/5  | Epoch loss: 2.5800695419311523  | Mean train accuracy: 0.8938691020011902\n",
            "Epoch: 3/5  | Epoch loss: 2.580148458480835  | Mean train accuracy: 0.8938743472099304\n",
            "Epoch: 3/5  | Epoch loss: 2.5804603099823  | Mean train accuracy: 0.8938797116279602\n",
            "Epoch: 3/5  | Epoch loss: 2.5808768272399902  | Mean train accuracy: 0.8938855528831482\n",
            "Epoch: 3/5  | Epoch loss: 2.5811920166015625  | Mean train accuracy: 0.8938918709754944\n",
            "Epoch: 3/5  | Epoch loss: 2.5809640884399414  | Mean train accuracy: 0.8938962817192078\n",
            "Epoch: 3/5  | Epoch loss: 2.581098794937134  | Mean train accuracy: 0.8939021229743958\n",
            "Epoch: 3/5  | Epoch loss: 2.5811734199523926  | Mean train accuracy: 0.8939056992530823\n",
            "Epoch: 3/5  | Epoch loss: 2.5805482864379883  | Mean train accuracy: 0.8939109444618225\n",
            "Epoch: 3/5  | Epoch loss: 2.5806236267089844  | Mean train accuracy: 0.8939151167869568\n",
            "Epoch: 3/5  | Epoch loss: 2.5808229446411133  | Mean train accuracy: 0.8939193487167358\n",
            "Epoch: 3/5  | Epoch loss: 2.580970048904419  | Mean train accuracy: 0.8939237594604492\n",
            "Epoch: 3/5  | Epoch loss: 2.5808417797088623  | Mean train accuracy: 0.8939285278320312\n",
            "Epoch: 3/5  | Epoch loss: 2.5802013874053955  | Mean train accuracy: 0.8939346075057983\n",
            "Epoch: 3/5  | Epoch loss: 2.5801305770874023  | Mean train accuracy: 0.8939397931098938\n",
            "Epoch: 3/5  | Epoch loss: 2.580026388168335  | Mean train accuracy: 0.8939458727836609\n",
            "Epoch: 3/5  | Epoch loss: 2.5801198482513428  | Mean train accuracy: 0.893951416015625\n",
            "Epoch: 3/5  | Epoch loss: 2.5800890922546387  | Mean train accuracy: 0.8939582109451294\n",
            "Epoch: 3/5  | Epoch loss: 2.5802078247070312  | Mean train accuracy: 0.8939656615257263\n",
            "Epoch: 3/5  | Epoch loss: 2.580221176147461  | Mean train accuracy: 0.8939720392227173\n",
            "Epoch: 3/5  | Epoch loss: 2.5803780555725098  | Mean train accuracy: 0.8939783573150635\n",
            "Epoch: 3/5  | Epoch loss: 2.5799901485443115  | Mean train accuracy: 0.8939844965934753\n",
            "Epoch: 3/5  | Epoch loss: 2.579824447631836  | Mean train accuracy: 0.8939899802207947\n",
            "Epoch: 3/5  | Epoch loss: 2.579451084136963  | Mean train accuracy: 0.8939964771270752\n",
            "Epoch: 3/5  | Epoch loss: 2.5796937942504883  | Mean train accuracy: 0.8940020799636841\n",
            "Epoch: 3/5  | Epoch loss: 2.5795986652374268  | Mean train accuracy: 0.8940072655677795\n",
            "Epoch: 3/5  | Epoch loss: 2.5795857906341553  | Mean train accuracy: 0.8940123915672302\n",
            "Epoch: 3/5  | Epoch loss: 2.579949378967285  | Mean train accuracy: 0.8940168619155884\n",
            "Epoch: 3/5  | Epoch loss: 2.5801258087158203  | Mean train accuracy: 0.8940215706825256\n",
            "Epoch: 3/5  | Epoch loss: 2.580477476119995  | Mean train accuracy: 0.894026517868042\n",
            "Epoch: 3/5  | Epoch loss: 2.580305576324463  | Mean train accuracy: 0.8940294981002808\n",
            "Epoch: 3/5  | Epoch loss: 2.5802807807922363  | Mean train accuracy: 0.8940322995185852\n",
            "Epoch: 3/5  | Epoch loss: 2.5804014205932617  | Mean train accuracy: 0.8940339088439941\n",
            "Epoch: 3/5  | Epoch loss: 2.5800528526306152  | Mean train accuracy: 0.8940368890762329\n",
            "Epoch: 3/5  | Epoch loss: 2.5798962116241455  | Mean train accuracy: 0.8940388560295105\n",
            "Epoch: 3/5  | Epoch loss: 2.5796689987182617  | Mean train accuracy: 0.8940408229827881\n",
            "Epoch: 3/5  | Epoch loss: 2.5799505710601807  | Mean train accuracy: 0.8940432667732239\n",
            "Epoch: 3/5  | Epoch loss: 2.579721212387085  | Mean train accuracy: 0.8940469026565552\n",
            "Epoch: 3/5  | Epoch loss: 2.5791237354278564  | Mean train accuracy: 0.8940494060516357\n",
            "Epoch: 3/5  | Epoch loss: 2.578561305999756  | Mean train accuracy: 0.8940524458885193\n",
            "Epoch: 3/5  | Epoch loss: 2.578444242477417  | Mean train accuracy: 0.8940570950508118\n",
            "Epoch: 3/5  | Epoch loss: 2.5785653591156006  | Mean train accuracy: 0.894061803817749\n",
            "Epoch: 3/5  | Epoch loss: 2.5784482955932617  | Mean train accuracy: 0.8940654993057251\n",
            "Epoch: 3/5  | Epoch loss: 2.5784571170806885  | Mean train accuracy: 0.8940697908401489\n",
            "Epoch: 3/5  | Epoch loss: 2.5784761905670166  | Mean train accuracy: 0.8940739631652832\n",
            "Epoch: 3/5  | Epoch loss: 2.5787007808685303  | Mean train accuracy: 0.8940795660018921\n",
            "Epoch: 3/5  | Epoch loss: 2.5787911415100098  | Mean train accuracy: 0.8940841555595398\n",
            "Epoch: 3/5  | Epoch loss: 2.579378128051758  | Mean train accuracy: 0.8940895199775696\n",
            "Epoch: 3/5  | Epoch loss: 2.5793614387512207  | Mean train accuracy: 0.8940951824188232\n",
            "Epoch: 3/5  | Epoch loss: 2.5795958042144775  | Mean train accuracy: 0.8941015005111694\n",
            "Epoch: 3/5  | Epoch loss: 2.579507350921631  | Mean train accuracy: 0.8941066861152649\n",
            "Epoch: 3/5  | Epoch loss: 2.5792999267578125  | Mean train accuracy: 0.8941119313240051\n",
            "Epoch: 3/5  | Epoch loss: 2.5792388916015625  | Mean train accuracy: 0.8941187262535095\n",
            "Epoch: 3/5  | Epoch loss: 2.5792253017425537  | Mean train accuracy: 0.8941242694854736\n",
            "Epoch: 3/5  | Epoch loss: 2.578826427459717  | Mean train accuracy: 0.8941296339035034\n",
            "Epoch: 3/5  | Epoch loss: 2.5788352489471436  | Mean train accuracy: 0.8941358923912048\n",
            "Epoch: 3/5  | Epoch loss: 2.578692674636841  | Mean train accuracy: 0.8941413164138794\n",
            "Epoch: 3/5  | Epoch loss: 2.579017400741577  | Mean train accuracy: 0.8941460847854614\n",
            "Epoch: 3/5  | Epoch loss: 2.5790226459503174  | Mean train accuracy: 0.8941507935523987\n",
            "Epoch: 3/5  | Epoch loss: 2.578691244125366  | Mean train accuracy: 0.8941550254821777\n",
            "Epoch: 3/5  | Epoch loss: 2.578690528869629  | Mean train accuracy: 0.8941600322723389\n",
            "Epoch: 3/5  | Epoch loss: 2.5788607597351074  | Mean train accuracy: 0.8941642642021179\n",
            "Epoch: 3/5  | Epoch loss: 2.578892707824707  | Mean train accuracy: 0.894166886806488\n",
            "Epoch: 3/5  | Epoch loss: 2.578476667404175  | Mean train accuracy: 0.8941705226898193\n",
            "Epoch: 3/5  | Epoch loss: 2.578089714050293  | Mean train accuracy: 0.8941736817359924\n",
            "Epoch: 3/5  | Epoch loss: 2.5780959129333496  | Mean train accuracy: 0.8941773772239685\n",
            "Epoch: 3/5  | Epoch loss: 2.578176259994507  | Mean train accuracy: 0.8941788077354431\n",
            "Epoch: 3/5  | Epoch loss: 2.5784401893615723  | Mean train accuracy: 0.8941801190376282\n",
            "Epoch: 3/5  | Epoch loss: 2.578629493713379  | Mean train accuracy: 0.8941806554794312\n",
            "Epoch: 3/5  | Epoch loss: 2.5788135528564453  | Mean train accuracy: 0.894180178642273\n",
            "Epoch: 3/5  | Epoch loss: 2.5783090591430664  | Mean train accuracy: 0.8941804766654968\n",
            "Epoch: 3/5  | Epoch loss: 2.5780959129333496  | Mean train accuracy: 0.8941797018051147\n",
            "Epoch: 3/5  | Epoch loss: 2.577711820602417  | Mean train accuracy: 0.8941792249679565\n",
            "Epoch: 3/5  | Epoch loss: 2.5773682594299316  | Mean train accuracy: 0.8941783308982849\n",
            "Epoch: 3/5  | Epoch loss: 2.577789783477783  | Mean train accuracy: 0.8941762447357178\n",
            "Epoch: 3/5  | Epoch loss: 2.5777554512023926  | Mean train accuracy: 0.8941760659217834\n",
            "Epoch: 3/5  | Epoch loss: 2.5779056549072266  | Mean train accuracy: 0.8941748738288879\n",
            "Epoch: 3/5  | Epoch loss: 2.578173875808716  | Mean train accuracy: 0.8941754698753357\n",
            "Epoch: 3/5  | Epoch loss: 2.577977418899536  | Mean train accuracy: 0.8941747546195984\n",
            "Epoch: 3/5  | Epoch loss: 2.5780019760131836  | Mean train accuracy: 0.8941748738288879\n",
            "Epoch: 3/5  | Epoch loss: 2.5779480934143066  | Mean train accuracy: 0.8941745758056641\n",
            "Epoch: 3/5  | Epoch loss: 2.578129529953003  | Mean train accuracy: 0.8941757082939148\n",
            "Epoch: 3/5  | Epoch loss: 2.5779995918273926  | Mean train accuracy: 0.8941766023635864\n",
            "Epoch: 3/5  | Epoch loss: 2.578062057495117  | Mean train accuracy: 0.8941784501075745\n",
            "Epoch: 3/5  | Epoch loss: 2.578274965286255  | Mean train accuracy: 0.8941808938980103\n",
            "Epoch: 3/5  | Epoch loss: 2.5783653259277344  | Mean train accuracy: 0.8941831588745117\n",
            "Epoch: 3/5  | Epoch loss: 2.5782432556152344  | Mean train accuracy: 0.8941869139671326\n",
            "Epoch: 3/5  | Epoch loss: 2.5780696868896484  | Mean train accuracy: 0.8941900134086609\n",
            "Epoch: 3/5  | Epoch loss: 2.5779755115509033  | Mean train accuracy: 0.8941945433616638\n",
            "Epoch: 3/5  | Epoch loss: 2.5777645111083984  | Mean train accuracy: 0.8942000269889832\n",
            "Epoch: 3/5  | Epoch loss: 2.577237129211426  | Mean train accuracy: 0.8942052125930786\n",
            "Epoch: 3/5  | Epoch loss: 2.5771570205688477  | Mean train accuracy: 0.8942112326622009\n",
            "Epoch: 3/5  | Epoch loss: 2.5770926475524902  | Mean train accuracy: 0.8942174911499023\n",
            "Epoch: 3/5  | Epoch loss: 2.5774621963500977  | Mean train accuracy: 0.8942245244979858\n",
            "Epoch: 3/5  | Epoch loss: 2.5774621963500977  | Mean train accuracy: 0.8942318558692932\n",
            "Epoch: 3/5  | Epoch loss: 2.577638626098633  | Mean train accuracy: 0.8942390084266663\n",
            "Epoch: 3/5  | Epoch loss: 2.5776352882385254  | Mean train accuracy: 0.8942469954490662\n",
            "Epoch: 3/5  | Epoch loss: 2.5775954723358154  | Mean train accuracy: 0.8942537307739258\n",
            "Epoch: 3/5  | Epoch loss: 2.5778756141662598  | Mean train accuracy: 0.8942604064941406\n",
            "Epoch: 3/5  | Epoch loss: 2.5777125358581543  | Mean train accuracy: 0.8942680358886719\n",
            "Epoch: 3/5  | Epoch loss: 2.5779244899749756  | Mean train accuracy: 0.894274890422821\n",
            "Epoch: 3/5  | Epoch loss: 2.57796573638916  | Mean train accuracy: 0.8942837119102478\n",
            "Epoch: 3/5  | Epoch loss: 2.5778615474700928  | Mean train accuracy: 0.8942919373512268\n",
            "Epoch: 3/5  | Epoch loss: 2.5780768394470215  | Mean train accuracy: 0.8943005800247192\n",
            "Epoch: 3/5  | Epoch loss: 2.5781328678131104  | Mean train accuracy: 0.8943095207214355\n",
            "Epoch: 3/5  | Epoch loss: 2.5778286457061768  | Mean train accuracy: 0.8943172693252563\n",
            "Epoch: 3/5  | Epoch loss: 2.5779287815093994  | Mean train accuracy: 0.8943250775337219\n",
            "Epoch: 3/5  | Epoch loss: 2.577881336212158  | Mean train accuracy: 0.8943328261375427\n",
            "Epoch: 3/5  | Epoch loss: 2.5780272483825684  | Mean train accuracy: 0.894339382648468\n",
            "Epoch: 3/5  | Epoch loss: 2.5777735710144043  | Mean train accuracy: 0.8943451642990112\n",
            "Epoch: 3/5  | Epoch loss: 2.5782370567321777  | Mean train accuracy: 0.8943513035774231\n",
            "Epoch: 3/5  | Epoch loss: 2.5781610012054443  | Mean train accuracy: 0.8943576812744141\n",
            "Epoch: 3/5  | Epoch loss: 2.578232765197754  | Mean train accuracy: 0.8943639397621155\n",
            "Epoch: 3/5  | Epoch loss: 2.5781147480010986  | Mean train accuracy: 0.8943708539009094\n",
            "Epoch: 3/5  | Epoch loss: 2.578457832336426  | Mean train accuracy: 0.8943770527839661\n",
            "Epoch: 3/5  | Epoch loss: 2.578193187713623  | Mean train accuracy: 0.8943840265274048\n",
            "Epoch: 3/5  | Epoch loss: 2.578486204147339  | Mean train accuracy: 0.8943902254104614\n",
            "Epoch: 3/5  | Epoch loss: 2.578486919403076  | Mean train accuracy: 0.8943971991539001\n",
            "Epoch: 3/5  | Epoch loss: 2.5785250663757324  | Mean train accuracy: 0.8944045901298523\n",
            "Epoch: 3/5  | Epoch loss: 2.5787010192871094  | Mean train accuracy: 0.8944107294082642\n",
            "Epoch: 3/5  | Epoch loss: 2.5788729190826416  | Mean train accuracy: 0.8944181203842163\n",
            "Epoch: 3/5  | Epoch loss: 2.5790035724639893  | Mean train accuracy: 0.8944251537322998\n",
            "Epoch: 3/5  | Epoch loss: 2.57885479927063  | Mean train accuracy: 0.8944315910339355\n",
            "Epoch: 3/5  | Epoch loss: 2.579094409942627  | Mean train accuracy: 0.8944385051727295\n",
            "Epoch: 3/5  | Epoch loss: 2.579131603240967  | Mean train accuracy: 0.8944457173347473\n",
            "Epoch: 3/5  | Epoch loss: 2.5792360305786133  | Mean train accuracy: 0.8944526314735413\n",
            "Epoch: 3/5  | Epoch loss: 2.579197883605957  | Mean train accuracy: 0.8944599628448486\n",
            "Epoch: 3/5  | Epoch loss: 2.5790627002716064  | Mean train accuracy: 0.894466757774353\n",
            "Epoch: 3/5  | Epoch loss: 2.57869291305542  | Mean train accuracy: 0.894473671913147\n",
            "Epoch: 3/5  | Epoch loss: 2.578754186630249  | Mean train accuracy: 0.8944799304008484\n",
            "Epoch: 3/5  | Epoch loss: 2.578589916229248  | Mean train accuracy: 0.8944864869117737\n",
            "Epoch: 3/5  | Epoch loss: 2.5786147117614746  | Mean train accuracy: 0.8944920301437378\n",
            "Epoch: 3/5  | Epoch loss: 2.5785696506500244  | Mean train accuracy: 0.8944973349571228\n",
            "Epoch: 3/5  | Epoch loss: 2.5788447856903076  | Mean train accuracy: 0.8945029377937317\n",
            "Epoch: 3/5  | Epoch loss: 2.5790209770202637  | Mean train accuracy: 0.8945101499557495\n",
            "Epoch: 3/5  | Epoch loss: 2.579106330871582  | Mean train accuracy: 0.8945159316062927\n",
            "Epoch: 3/5  | Epoch loss: 2.579057455062866  | Mean train accuracy: 0.8945217728614807\n",
            "Epoch: 3/5  | Epoch loss: 2.5790717601776123  | Mean train accuracy: 0.8945270776748657\n",
            "Epoch: 3/5  | Epoch loss: 2.5789172649383545  | Mean train accuracy: 0.8945333361625671\n",
            "Epoch: 3/5  | Epoch loss: 2.578505754470825  | Mean train accuracy: 0.8945384621620178\n",
            "Epoch: 3/5  | Epoch loss: 2.5784289836883545  | Mean train accuracy: 0.8945437669754028\n",
            "Epoch: 3/5  | Epoch loss: 2.5784804821014404  | Mean train accuracy: 0.894550085067749\n",
            "Epoch: 3/5  | Epoch loss: 2.5783438682556152  | Mean train accuracy: 0.8945552706718445\n",
            "Epoch: 3/5  | Epoch loss: 2.578263521194458  | Mean train accuracy: 0.8945600390434265\n",
            "Epoch: 3/5  | Epoch loss: 2.578056573867798  | Mean train accuracy: 0.894565761089325\n",
            "Epoch: 3/5  | Epoch loss: 2.578239679336548  | Mean train accuracy: 0.8945704102516174\n",
            "Epoch: 3/5  | Epoch loss: 2.5781760215759277  | Mean train accuracy: 0.8945764303207397\n",
            "Epoch: 3/5  | Epoch loss: 2.5785462856292725  | Mean train accuracy: 0.8945807218551636\n",
            "Epoch: 3/5  | Epoch loss: 2.5782668590545654  | Mean train accuracy: 0.8945855498313904\n",
            "Epoch: 3/5  | Epoch loss: 2.5781004428863525  | Mean train accuracy: 0.8945903778076172\n",
            "Epoch: 3/5  | Epoch loss: 2.5779590606689453  | Mean train accuracy: 0.8945956230163574\n",
            "Epoch: 3/5  | Epoch loss: 2.5779836177825928  | Mean train accuracy: 0.8946014642715454\n",
            "Epoch: 3/5  | Epoch loss: 2.578162431716919  | Mean train accuracy: 0.894607663154602\n",
            "Epoch: 3/5  | Epoch loss: 2.5780646800994873  | Mean train accuracy: 0.894614040851593\n",
            "Epoch: 3/5  | Epoch loss: 2.578120470046997  | Mean train accuracy: 0.8946214318275452\n",
            "Epoch: 3/5  | Epoch loss: 2.578083038330078  | Mean train accuracy: 0.8946287035942078\n",
            "Epoch: 3/5  | Epoch loss: 2.5783331394195557  | Mean train accuracy: 0.8946353197097778\n",
            "Epoch: 3/5  | Epoch loss: 2.578418016433716  | Mean train accuracy: 0.8946413993835449\n",
            "Epoch: 3/5  | Epoch loss: 2.5782241821289062  | Mean train accuracy: 0.8946480751037598\n",
            "Epoch: 3/5  | Epoch loss: 2.578089714050293  | Mean train accuracy: 0.8946549892425537\n",
            "Epoch: 3/5  | Epoch loss: 2.578646183013916  | Mean train accuracy: 0.8946611285209656\n",
            "Epoch: 3/5  | Epoch loss: 2.5789999961853027  | Mean train accuracy: 0.8946678042411804\n",
            "Epoch: 3/5  | Epoch loss: 2.5792484283447266  | Mean train accuracy: 0.8946743607521057\n",
            "Epoch: 3/5  | Epoch loss: 2.5793344974517822  | Mean train accuracy: 0.8946821689605713\n",
            "Epoch: 3/5  | Epoch loss: 2.5793838500976562  | Mean train accuracy: 0.8946900963783264\n",
            "Epoch: 3/5  | Epoch loss: 2.5794594287872314  | Mean train accuracy: 0.8946973085403442\n",
            "Epoch: 3/5  | Epoch loss: 2.5795350074768066  | Mean train accuracy: 0.8947043418884277\n",
            "Epoch: 3/5  | Epoch loss: 2.5796079635620117  | Mean train accuracy: 0.8947123885154724\n",
            "Epoch: 3/5  | Epoch loss: 2.5798866748809814  | Mean train accuracy: 0.8947203755378723\n",
            "Epoch: 3/5  | Epoch loss: 2.5800890922546387  | Mean train accuracy: 0.8947271704673767\n",
            "Epoch: 3/5  | Epoch loss: 2.580597400665283  | Mean train accuracy: 0.8947330117225647\n",
            "Epoch: 3/5  | Epoch loss: 2.5805752277374268  | Mean train accuracy: 0.8947385549545288\n",
            "Epoch: 3/5  | Epoch loss: 2.5804738998413086  | Mean train accuracy: 0.8947441577911377\n",
            "Epoch: 3/5  | Epoch loss: 2.5807347297668457  | Mean train accuracy: 0.8947482705116272\n",
            "Epoch: 3/5  | Epoch loss: 2.580690383911133  | Mean train accuracy: 0.8947522044181824\n",
            "Epoch: 3/5  | Epoch loss: 2.5804295539855957  | Mean train accuracy: 0.8947557806968689\n",
            "Epoch: 3/5  | Epoch loss: 2.5803070068359375  | Mean train accuracy: 0.894759476184845\n",
            "Epoch: 3/5  | Epoch loss: 2.580613136291504  | Mean train accuracy: 0.8947617411613464\n",
            "Epoch: 3/5  | Epoch loss: 2.5806918144226074  | Mean train accuracy: 0.8947650194168091\n",
            "Epoch: 3/5  | Epoch loss: 2.5806691646575928  | Mean train accuracy: 0.894766628742218\n",
            "Epoch: 3/5  | Epoch loss: 2.580583095550537  | Mean train accuracy: 0.8947694301605225\n",
            "Epoch: 3/5  | Epoch loss: 2.5804603099823  | Mean train accuracy: 0.8947714567184448\n",
            "Epoch: 3/5  | Epoch loss: 2.5803768634796143  | Mean train accuracy: 0.8947728872299194\n",
            "Epoch: 3/5  | Epoch loss: 2.580371379852295  | Mean train accuracy: 0.8947743773460388\n",
            "Epoch: 3/5  | Epoch loss: 2.580594539642334  | Mean train accuracy: 0.8947755098342896\n",
            "Epoch: 3/5  | Epoch loss: 2.580206871032715  | Mean train accuracy: 0.8947765231132507\n",
            "Epoch: 3/5  | Epoch loss: 2.5798189640045166  | Mean train accuracy: 0.8947777152061462\n",
            "Epoch: 3/5  | Epoch loss: 2.579655885696411  | Mean train accuracy: 0.8947790265083313\n",
            "Epoch: 3/5  | Epoch loss: 2.5797030925750732  | Mean train accuracy: 0.894780695438385\n",
            "Epoch: 3/5  | Epoch loss: 2.5796895027160645  | Mean train accuracy: 0.8947821855545044\n",
            "Epoch: 3/5  | Epoch loss: 2.579389810562134  | Mean train accuracy: 0.8947831988334656\n",
            "Epoch: 3/5  | Epoch loss: 2.579275131225586  | Mean train accuracy: 0.894784688949585\n",
            "Epoch: 3/5  | Epoch loss: 2.579298496246338  | Mean train accuracy: 0.8947863578796387\n",
            "Epoch: 3/5  | Epoch loss: 2.5793187618255615  | Mean train accuracy: 0.8947874903678894\n",
            "Epoch: 3/5  | Epoch loss: 2.5791189670562744  | Mean train accuracy: 0.8947904109954834\n",
            "Epoch: 3/5  | Epoch loss: 2.579319715499878  | Mean train accuracy: 0.8947930335998535\n",
            "Epoch: 3/5  | Epoch loss: 2.5796990394592285  | Mean train accuracy: 0.8947948813438416\n",
            "Epoch: 3/5  | Epoch loss: 2.579508066177368  | Mean train accuracy: 0.8947976231575012\n",
            "Epoch: 3/5  | Epoch loss: 2.5794758796691895  | Mean train accuracy: 0.8948003053665161\n",
            "Epoch: 3/5  | Epoch loss: 2.57930850982666  | Mean train accuracy: 0.8948044776916504\n",
            "Epoch: 3/5  | Epoch loss: 2.579134464263916  | Mean train accuracy: 0.894807755947113\n",
            "Epoch: 3/5  | Epoch loss: 2.5792365074157715  | Mean train accuracy: 0.8948116302490234\n",
            "Epoch: 3/5  | Epoch loss: 2.5791244506835938  | Mean train accuracy: 0.8948153853416443\n",
            "Epoch: 3/5  | Epoch loss: 2.5789334774017334  | Mean train accuracy: 0.8948192596435547\n",
            "Epoch: 3/5  | Epoch loss: 2.5788755416870117  | Mean train accuracy: 0.8948245644569397\n",
            "Epoch: 3/5  | Epoch loss: 2.5791008472442627  | Mean train accuracy: 0.894830048084259\n",
            "Epoch: 3/5  | Epoch loss: 2.5789942741394043  | Mean train accuracy: 0.8948354721069336\n",
            "Epoch: 3/5  | Epoch loss: 2.578887939453125  | Mean train accuracy: 0.8948420882225037\n",
            "Epoch: 3/5  | Epoch loss: 2.578780174255371  | Mean train accuracy: 0.8948492407798767\n",
            "Epoch: 3/5  | Epoch loss: 2.578990936279297  | Mean train accuracy: 0.8948562741279602\n",
            "Epoch: 3/5  | Epoch loss: 2.5787506103515625  | Mean train accuracy: 0.8948628902435303\n",
            "Epoch: 3/5  | Epoch loss: 2.578885078430176  | Mean train accuracy: 0.8948699235916138\n",
            "Epoch: 3/5  | Epoch loss: 2.5786325931549072  | Mean train accuracy: 0.894878089427948\n",
            "Epoch: 3/5  | Epoch loss: 2.5783791542053223  | Mean train accuracy: 0.8948858380317688\n",
            "Epoch: 3/5  | Epoch loss: 2.578683376312256  | Mean train accuracy: 0.8948941826820374\n",
            "Epoch: 3/5  | Epoch loss: 2.5786631107330322  | Mean train accuracy: 0.8949020504951477\n",
            "Epoch: 3/5  | Epoch loss: 2.578585147857666  | Mean train accuracy: 0.8949093818664551\n",
            "Epoch: 3/5  | Epoch loss: 2.578577756881714  | Mean train accuracy: 0.8949168920516968\n",
            "Epoch: 3/5  | Epoch loss: 2.5784449577331543  | Mean train accuracy: 0.8949248790740967\n",
            "Epoch: 3/5  | Epoch loss: 2.5785109996795654  | Mean train accuracy: 0.8949314951896667\n",
            "Epoch: 3/5  | Epoch loss: 2.578596830368042  | Mean train accuracy: 0.8949388861656189\n",
            "Epoch: 3/5  | Epoch loss: 2.5785491466522217  | Mean train accuracy: 0.8949453234672546\n",
            "Epoch: 3/5  | Epoch loss: 2.5788276195526123  | Mean train accuracy: 0.8949514031410217\n",
            "Epoch: 3/5  | Epoch loss: 2.5786924362182617  | Mean train accuracy: 0.8949568867683411\n",
            "Epoch: 3/5  | Epoch loss: 2.579164981842041  | Mean train accuracy: 0.8949617743492126\n",
            "Epoch: 3/5  | Epoch loss: 2.578916072845459  | Mean train accuracy: 0.8949669003486633\n",
            "Epoch: 3/5  | Epoch loss: 2.578761339187622  | Mean train accuracy: 0.8949715495109558\n",
            "Epoch: 3/5  | Epoch loss: 2.5788838863372803  | Mean train accuracy: 0.8949751853942871\n",
            "Epoch: 3/5  | Epoch loss: 2.5789361000061035  | Mean train accuracy: 0.8949787616729736\n",
            "Epoch: 3/5  | Epoch loss: 2.5789954662323  | Mean train accuracy: 0.8949821591377258\n",
            "Epoch: 3/5  | Epoch loss: 2.578913688659668  | Mean train accuracy: 0.8949854969978333\n",
            "Epoch: 3/5  | Epoch loss: 2.579239845275879  | Mean train accuracy: 0.8949875235557556\n",
            "Epoch: 3/5  | Epoch loss: 2.5793817043304443  | Mean train accuracy: 0.8949888944625854\n",
            "Epoch: 3/5  | Epoch loss: 2.579493761062622  | Mean train accuracy: 0.8949897289276123\n",
            "Epoch: 3/5  | Epoch loss: 2.5795021057128906  | Mean train accuracy: 0.8949904441833496\n",
            "Epoch: 3/5  | Epoch loss: 2.5797581672668457  | Mean train accuracy: 0.8949908018112183\n",
            "Epoch: 3/5  | Epoch loss: 2.579925298690796  | Mean train accuracy: 0.8949908018112183\n",
            "Epoch: 3/5  | Epoch loss: 2.5799567699432373  | Mean train accuracy: 0.8949910402297974\n",
            "Epoch: 3/5  | Epoch loss: 2.5796847343444824  | Mean train accuracy: 0.8949918150901794\n",
            "Epoch: 3/5  | Epoch loss: 2.579883575439453  | Mean train accuracy: 0.8949925303459167\n",
            "Epoch: 3/5  | Epoch loss: 2.5795786380767822  | Mean train accuracy: 0.8949938416481018\n",
            "Epoch: 3/5  | Epoch loss: 2.5797479152679443  | Mean train accuracy: 0.8949953317642212\n",
            "Epoch: 3/5  | Epoch loss: 2.5796077251434326  | Mean train accuracy: 0.8949969410896301\n",
            "Epoch: 3/5  | Epoch loss: 2.579751491546631  | Mean train accuracy: 0.8949987292289734\n",
            "Epoch: 3/5  | Epoch loss: 2.5797698497772217  | Mean train accuracy: 0.8950003981590271\n",
            "Epoch: 3/5  | Epoch loss: 2.579742908477783  | Mean train accuracy: 0.8950028419494629\n",
            "Epoch: 3/5  | Epoch loss: 2.5796241760253906  | Mean train accuracy: 0.895005464553833\n",
            "Epoch: 3/5  | Epoch loss: 2.5796291828155518  | Mean train accuracy: 0.8950076103210449\n",
            "Epoch: 3/5  | Epoch loss: 2.579746723175049  | Mean train accuracy: 0.8950095176696777\n",
            "Epoch: 3/5  | Epoch loss: 2.5798983573913574  | Mean train accuracy: 0.8950102925300598\n",
            "Epoch: 3/5  | Epoch loss: 2.5796618461608887  | Mean train accuracy: 0.8950130939483643\n",
            "Epoch: 3/5  | Epoch loss: 2.579749822616577  | Mean train accuracy: 0.895015299320221\n",
            "Epoch: 3/5  | Epoch loss: 2.5798442363739014  | Mean train accuracy: 0.8950174450874329\n",
            "Epoch: 3/5  | Epoch loss: 2.5799431800842285  | Mean train accuracy: 0.8950185775756836\n",
            "Epoch: 3/5  | Epoch loss: 2.5801632404327393  | Mean train accuracy: 0.8950200080871582\n",
            "Epoch: 3/5  | Epoch loss: 2.580152750015259  | Mean train accuracy: 0.8950210809707642\n",
            "Epoch: 3/5  | Epoch loss: 2.579895496368408  | Mean train accuracy: 0.8950216174125671\n",
            "Epoch: 3/5  | Epoch loss: 2.580193519592285  | Mean train accuracy: 0.8950226306915283\n",
            "Epoch: 3/5  | Epoch loss: 2.5803253650665283  | Mean train accuracy: 0.8950230479240417\n",
            "Epoch: 3/5  | Epoch loss: 2.5800185203552246  | Mean train accuracy: 0.8950238823890686\n",
            "Epoch: 3/5  | Epoch loss: 2.580124855041504  | Mean train accuracy: 0.8950240612030029\n",
            "Epoch: 3/5  | Epoch loss: 2.580270767211914  | Mean train accuracy: 0.8950244188308716\n",
            "Epoch: 3/5  | Epoch loss: 2.5803797245025635  | Mean train accuracy: 0.8950248956680298\n",
            "Epoch: 3/5  | Epoch loss: 2.5807862281799316  | Mean train accuracy: 0.8950247764587402\n",
            "Epoch: 3/5  | Epoch loss: 2.580904960632324  | Mean train accuracy: 0.8950247764587402\n",
            "Epoch: 3/5  | Epoch loss: 2.5809922218322754  | Mean train accuracy: 0.8950244188308716\n",
            "Epoch: 3/5  | Epoch loss: 2.580993890762329  | Mean train accuracy: 0.895023763179779\n",
            "Epoch: 3/5  | Epoch loss: 2.5810344219207764  | Mean train accuracy: 0.8950235247612\n",
            "Epoch: 3/5  | Epoch loss: 2.581117630004883  | Mean train accuracy: 0.8950234651565552\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d3d52c9be91406f8da44794eaaf6f81",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=655), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4/5  | Epoch loss: 2.5640530586242676  | Mean train accuracy: 0.8942347168922424\n",
            "Epoch: 4/5  | Epoch loss: 2.5288472175598145  | Mean train accuracy: 0.8942991495132446\n",
            "Epoch: 4/5  | Epoch loss: 2.569382905960083  | Mean train accuracy: 0.8944066166877747\n",
            "Epoch: 4/5  | Epoch loss: 2.592202663421631  | Mean train accuracy: 0.8942508697509766\n",
            "Epoch: 4/5  | Epoch loss: 2.597085475921631  | Mean train accuracy: 0.8943069577217102\n",
            "Epoch: 4/5  | Epoch loss: 2.5900962352752686  | Mean train accuracy: 0.8942390084266663\n",
            "Epoch: 4/5  | Epoch loss: 2.591796398162842  | Mean train accuracy: 0.8941941857337952\n",
            "Epoch: 4/5  | Epoch loss: 2.592994213104248  | Mean train accuracy: 0.8940961360931396\n",
            "Epoch: 4/5  | Epoch loss: 2.6062417030334473  | Mean train accuracy: 0.894124448299408\n",
            "Epoch: 4/5  | Epoch loss: 2.5843050479888916  | Mean train accuracy: 0.8941109776496887\n",
            "Epoch: 4/5  | Epoch loss: 2.595726490020752  | Mean train accuracy: 0.8941268920898438\n",
            "Epoch: 4/5  | Epoch loss: 2.5989272594451904  | Mean train accuracy: 0.8941261768341064\n",
            "Epoch: 4/5  | Epoch loss: 2.600163459777832  | Mean train accuracy: 0.8941048383712769\n",
            "Epoch: 4/5  | Epoch loss: 2.6073930263519287  | Mean train accuracy: 0.8941002488136292\n",
            "Epoch: 4/5  | Epoch loss: 2.593912363052368  | Mean train accuracy: 0.8941075205802917\n",
            "Epoch: 4/5  | Epoch loss: 2.589540481567383  | Mean train accuracy: 0.8940654993057251\n",
            "Epoch: 4/5  | Epoch loss: 2.5887110233306885  | Mean train accuracy: 0.8940337896347046\n",
            "Epoch: 4/5  | Epoch loss: 2.5807595252990723  | Mean train accuracy: 0.8940277099609375\n",
            "Epoch: 4/5  | Epoch loss: 2.5928070545196533  | Mean train accuracy: 0.8939788937568665\n",
            "Epoch: 4/5  | Epoch loss: 2.5785555839538574  | Mean train accuracy: 0.8939536809921265\n",
            "Epoch: 4/5  | Epoch loss: 2.5837600231170654  | Mean train accuracy: 0.8939167261123657\n",
            "Epoch: 4/5  | Epoch loss: 2.5876338481903076  | Mean train accuracy: 0.8938720226287842\n",
            "Epoch: 4/5  | Epoch loss: 2.592726469039917  | Mean train accuracy: 0.893811047077179\n",
            "Epoch: 4/5  | Epoch loss: 2.5931777954101562  | Mean train accuracy: 0.8937641978263855\n",
            "Epoch: 4/5  | Epoch loss: 2.5934994220733643  | Mean train accuracy: 0.8937283158302307\n",
            "Epoch: 4/5  | Epoch loss: 2.594527006149292  | Mean train accuracy: 0.8936833739280701\n",
            "Epoch: 4/5  | Epoch loss: 2.59743070602417  | Mean train accuracy: 0.8936245441436768\n",
            "Epoch: 4/5  | Epoch loss: 2.601255178451538  | Mean train accuracy: 0.8935837149620056\n",
            "Epoch: 4/5  | Epoch loss: 2.602571487426758  | Mean train accuracy: 0.893528401851654\n",
            "Epoch: 4/5  | Epoch loss: 2.605609893798828  | Mean train accuracy: 0.8934857845306396\n",
            "Epoch: 4/5  | Epoch loss: 2.605802536010742  | Mean train accuracy: 0.8934296369552612\n",
            "Epoch: 4/5  | Epoch loss: 2.6039388179779053  | Mean train accuracy: 0.8933919072151184\n",
            "Epoch: 4/5  | Epoch loss: 2.6058475971221924  | Mean train accuracy: 0.8933658599853516\n",
            "Epoch: 4/5  | Epoch loss: 2.6019623279571533  | Mean train accuracy: 0.8933405876159668\n",
            "Epoch: 4/5  | Epoch loss: 2.6028833389282227  | Mean train accuracy: 0.8933164477348328\n",
            "Epoch: 4/5  | Epoch loss: 2.596602439880371  | Mean train accuracy: 0.8933129906654358\n",
            "Epoch: 4/5  | Epoch loss: 2.5946249961853027  | Mean train accuracy: 0.8933107256889343\n",
            "Epoch: 4/5  | Epoch loss: 2.59110951423645  | Mean train accuracy: 0.8933123350143433\n",
            "Epoch: 4/5  | Epoch loss: 2.5941965579986572  | Mean train accuracy: 0.8933175206184387\n",
            "Epoch: 4/5  | Epoch loss: 2.596111297607422  | Mean train accuracy: 0.8933316469192505\n",
            "Epoch: 4/5  | Epoch loss: 2.5931522846221924  | Mean train accuracy: 0.8933636546134949\n",
            "Epoch: 4/5  | Epoch loss: 2.5920040607452393  | Mean train accuracy: 0.8933868408203125\n",
            "Epoch: 4/5  | Epoch loss: 2.59213924407959  | Mean train accuracy: 0.8934137225151062\n",
            "Epoch: 4/5  | Epoch loss: 2.5906317234039307  | Mean train accuracy: 0.8934189081192017\n",
            "Epoch: 4/5  | Epoch loss: 2.591447114944458  | Mean train accuracy: 0.893427848815918\n",
            "Epoch: 4/5  | Epoch loss: 2.5951027870178223  | Mean train accuracy: 0.8934462070465088\n",
            "Epoch: 4/5  | Epoch loss: 2.595005750656128  | Mean train accuracy: 0.8934643268585205\n",
            "Epoch: 4/5  | Epoch loss: 2.5973052978515625  | Mean train accuracy: 0.893466055393219\n",
            "Epoch: 4/5  | Epoch loss: 2.5966029167175293  | Mean train accuracy: 0.8934838175773621\n",
            "Epoch: 4/5  | Epoch loss: 2.595094919204712  | Mean train accuracy: 0.8934952616691589\n",
            "Epoch: 4/5  | Epoch loss: 2.598227024078369  | Mean train accuracy: 0.8935003876686096\n",
            "Epoch: 4/5  | Epoch loss: 2.6025173664093018  | Mean train accuracy: 0.8935132622718811\n",
            "Epoch: 4/5  | Epoch loss: 2.6048033237457275  | Mean train accuracy: 0.8935144543647766\n",
            "Epoch: 4/5  | Epoch loss: 2.604130983352661  | Mean train accuracy: 0.8935256600379944\n",
            "Epoch: 4/5  | Epoch loss: 2.6007492542266846  | Mean train accuracy: 0.8935310244560242\n",
            "Epoch: 4/5  | Epoch loss: 2.6020681858062744  | Mean train accuracy: 0.8935372233390808\n",
            "Epoch: 4/5  | Epoch loss: 2.5994112491607666  | Mean train accuracy: 0.8935388326644897\n",
            "Epoch: 4/5  | Epoch loss: 2.600004196166992  | Mean train accuracy: 0.8935517072677612\n",
            "Epoch: 4/5  | Epoch loss: 2.601466655731201  | Mean train accuracy: 0.8935685157775879\n",
            "Epoch: 4/5  | Epoch loss: 2.599191427230835  | Mean train accuracy: 0.8935794234275818\n",
            "Epoch: 4/5  | Epoch loss: 2.5963730812072754  | Mean train accuracy: 0.8935961127281189\n",
            "Epoch: 4/5  | Epoch loss: 2.5946695804595947  | Mean train accuracy: 0.8936020135879517\n",
            "Epoch: 4/5  | Epoch loss: 2.5961313247680664  | Mean train accuracy: 0.8936268091201782\n",
            "Epoch: 4/5  | Epoch loss: 2.5963282585144043  | Mean train accuracy: 0.8936523795127869\n",
            "Epoch: 4/5  | Epoch loss: 2.59749436378479  | Mean train accuracy: 0.8936694860458374\n",
            "Epoch: 4/5  | Epoch loss: 2.5996971130371094  | Mean train accuracy: 0.8936991095542908\n",
            "Epoch: 4/5  | Epoch loss: 2.597076892852783  | Mean train accuracy: 0.8937313556671143\n",
            "Epoch: 4/5  | Epoch loss: 2.597658395767212  | Mean train accuracy: 0.8937681317329407\n",
            "Epoch: 4/5  | Epoch loss: 2.5959949493408203  | Mean train accuracy: 0.8937963843345642\n",
            "Epoch: 4/5  | Epoch loss: 2.595539093017578  | Mean train accuracy: 0.8938304781913757\n",
            "Epoch: 4/5  | Epoch loss: 2.5957553386688232  | Mean train accuracy: 0.8938668370246887\n",
            "Epoch: 4/5  | Epoch loss: 2.59566068649292  | Mean train accuracy: 0.8939135074615479\n",
            "Epoch: 4/5  | Epoch loss: 2.59407114982605  | Mean train accuracy: 0.8939588069915771\n",
            "Epoch: 4/5  | Epoch loss: 2.5917603969573975  | Mean train accuracy: 0.8940044045448303\n",
            "Epoch: 4/5  | Epoch loss: 2.5896222591400146  | Mean train accuracy: 0.8940521478652954\n",
            "Epoch: 4/5  | Epoch loss: 2.5886220932006836  | Mean train accuracy: 0.8940933346748352\n",
            "Epoch: 4/5  | Epoch loss: 2.5902345180511475  | Mean train accuracy: 0.8941305875778198\n",
            "Epoch: 4/5  | Epoch loss: 2.5902292728424072  | Mean train accuracy: 0.8941619992256165\n",
            "Epoch: 4/5  | Epoch loss: 2.5883960723876953  | Mean train accuracy: 0.8942024111747742\n",
            "Epoch: 4/5  | Epoch loss: 2.591594934463501  | Mean train accuracy: 0.8942352533340454\n",
            "Epoch: 4/5  | Epoch loss: 2.5918424129486084  | Mean train accuracy: 0.8942764401435852\n",
            "Epoch: 4/5  | Epoch loss: 2.5915708541870117  | Mean train accuracy: 0.894317626953125\n",
            "Epoch: 4/5  | Epoch loss: 2.591787576675415  | Mean train accuracy: 0.8943524360656738\n",
            "Epoch: 4/5  | Epoch loss: 2.59031343460083  | Mean train accuracy: 0.8943948149681091\n",
            "Epoch: 4/5  | Epoch loss: 2.5892655849456787  | Mean train accuracy: 0.8944358229637146\n",
            "Epoch: 4/5  | Epoch loss: 2.5885744094848633  | Mean train accuracy: 0.8944769501686096\n",
            "Epoch: 4/5  | Epoch loss: 2.5890936851501465  | Mean train accuracy: 0.8945236802101135\n",
            "Epoch: 4/5  | Epoch loss: 2.587421178817749  | Mean train accuracy: 0.8945607542991638\n",
            "Epoch: 4/5  | Epoch loss: 2.5884017944335938  | Mean train accuracy: 0.8945996761322021\n",
            "Epoch: 4/5  | Epoch loss: 2.5889322757720947  | Mean train accuracy: 0.894636869430542\n",
            "Epoch: 4/5  | Epoch loss: 2.588763475418091  | Mean train accuracy: 0.8946778178215027\n",
            "Epoch: 4/5  | Epoch loss: 2.5885045528411865  | Mean train accuracy: 0.8947107195854187\n",
            "Epoch: 4/5  | Epoch loss: 2.5874125957489014  | Mean train accuracy: 0.8947367668151855\n",
            "Epoch: 4/5  | Epoch loss: 2.5883376598358154  | Mean train accuracy: 0.8947673439979553\n",
            "Epoch: 4/5  | Epoch loss: 2.5913097858428955  | Mean train accuracy: 0.8947977423667908\n",
            "Epoch: 4/5  | Epoch loss: 2.5923261642456055  | Mean train accuracy: 0.8948151469230652\n",
            "Epoch: 4/5  | Epoch loss: 2.5925986766815186  | Mean train accuracy: 0.8948416709899902\n",
            "Epoch: 4/5  | Epoch loss: 2.592742443084717  | Mean train accuracy: 0.8948550820350647\n",
            "Epoch: 4/5  | Epoch loss: 2.5928282737731934  | Mean train accuracy: 0.8948635458946228\n",
            "Epoch: 4/5  | Epoch loss: 2.590838670730591  | Mean train accuracy: 0.8948768377304077\n",
            "Epoch: 4/5  | Epoch loss: 2.5925629138946533  | Mean train accuracy: 0.8948881030082703\n",
            "Epoch: 4/5  | Epoch loss: 2.594062328338623  | Mean train accuracy: 0.8948954343795776\n",
            "Epoch: 4/5  | Epoch loss: 2.59393048286438  | Mean train accuracy: 0.8948976993560791\n",
            "Epoch: 4/5  | Epoch loss: 2.595183849334717  | Mean train accuracy: 0.8948934078216553\n",
            "Epoch: 4/5  | Epoch loss: 2.594491720199585  | Mean train accuracy: 0.8948925137519836\n",
            "Epoch: 4/5  | Epoch loss: 2.591952323913574  | Mean train accuracy: 0.8948898315429688\n",
            "Epoch: 4/5  | Epoch loss: 2.5911402702331543  | Mean train accuracy: 0.8948811888694763\n",
            "Epoch: 4/5  | Epoch loss: 2.5900027751922607  | Mean train accuracy: 0.8948769569396973\n",
            "Epoch: 4/5  | Epoch loss: 2.589109182357788  | Mean train accuracy: 0.8948740363121033\n",
            "Epoch: 4/5  | Epoch loss: 2.589076042175293  | Mean train accuracy: 0.894870400428772\n",
            "Epoch: 4/5  | Epoch loss: 2.588128089904785  | Mean train accuracy: 0.8948761820793152\n",
            "Epoch: 4/5  | Epoch loss: 2.5869908332824707  | Mean train accuracy: 0.8948805928230286\n",
            "Epoch: 4/5  | Epoch loss: 2.588191032409668  | Mean train accuracy: 0.8948884010314941\n",
            "Epoch: 4/5  | Epoch loss: 2.589232921600342  | Mean train accuracy: 0.8948988914489746\n",
            "Epoch: 4/5  | Epoch loss: 2.588149070739746  | Mean train accuracy: 0.8949127197265625\n",
            "Epoch: 4/5  | Epoch loss: 2.5883543491363525  | Mean train accuracy: 0.8949292302131653\n",
            "Epoch: 4/5  | Epoch loss: 2.588406801223755  | Mean train accuracy: 0.8949444890022278\n",
            "Epoch: 4/5  | Epoch loss: 2.5892117023468018  | Mean train accuracy: 0.8949607610702515\n",
            "Epoch: 4/5  | Epoch loss: 2.588202714920044  | Mean train accuracy: 0.8949769735336304\n",
            "Epoch: 4/5  | Epoch loss: 2.5887410640716553  | Mean train accuracy: 0.8949922919273376\n",
            "Epoch: 4/5  | Epoch loss: 2.5883712768554688  | Mean train accuracy: 0.8950048685073853\n",
            "Epoch: 4/5  | Epoch loss: 2.5879130363464355  | Mean train accuracy: 0.8950162529945374\n",
            "Epoch: 4/5  | Epoch loss: 2.587570905685425  | Mean train accuracy: 0.8950293064117432\n",
            "Epoch: 4/5  | Epoch loss: 2.587446451187134  | Mean train accuracy: 0.8950416445732117\n",
            "Epoch: 4/5  | Epoch loss: 2.5886294841766357  | Mean train accuracy: 0.8950550556182861\n",
            "Epoch: 4/5  | Epoch loss: 2.587773323059082  | Mean train accuracy: 0.8950691223144531\n",
            "Epoch: 4/5  | Epoch loss: 2.587737798690796  | Mean train accuracy: 0.8950832486152649\n",
            "Epoch: 4/5  | Epoch loss: 2.5866827964782715  | Mean train accuracy: 0.8950982093811035\n",
            "Epoch: 4/5  | Epoch loss: 2.5872650146484375  | Mean train accuracy: 0.895114004611969\n",
            "Epoch: 4/5  | Epoch loss: 2.5874862670898438  | Mean train accuracy: 0.8951300382614136\n",
            "Epoch: 4/5  | Epoch loss: 2.5866973400115967  | Mean train accuracy: 0.8951454162597656\n",
            "Epoch: 4/5  | Epoch loss: 2.5865261554718018  | Mean train accuracy: 0.895162045955658\n",
            "Epoch: 4/5  | Epoch loss: 2.5863349437713623  | Mean train accuracy: 0.8951764702796936\n",
            "Epoch: 4/5  | Epoch loss: 2.5859627723693848  | Mean train accuracy: 0.8951876163482666\n",
            "Epoch: 4/5  | Epoch loss: 2.585599422454834  | Mean train accuracy: 0.8952008485794067\n",
            "Epoch: 4/5  | Epoch loss: 2.5851399898529053  | Mean train accuracy: 0.8952192664146423\n",
            "Epoch: 4/5  | Epoch loss: 2.5852749347686768  | Mean train accuracy: 0.8952290415763855\n",
            "Epoch: 4/5  | Epoch loss: 2.5848281383514404  | Mean train accuracy: 0.8952459692955017\n",
            "Epoch: 4/5  | Epoch loss: 2.584869146347046  | Mean train accuracy: 0.8952603936195374\n",
            "Epoch: 4/5  | Epoch loss: 2.5846548080444336  | Mean train accuracy: 0.8952722549438477\n",
            "Epoch: 4/5  | Epoch loss: 2.5841546058654785  | Mean train accuracy: 0.8952821493148804\n",
            "Epoch: 4/5  | Epoch loss: 2.5841193199157715  | Mean train accuracy: 0.8952939510345459\n",
            "Epoch: 4/5  | Epoch loss: 2.583683490753174  | Mean train accuracy: 0.8953008055686951\n",
            "Epoch: 4/5  | Epoch loss: 2.5835788249969482  | Mean train accuracy: 0.8953109383583069\n",
            "Epoch: 4/5  | Epoch loss: 2.5830531120300293  | Mean train accuracy: 0.8953250050544739\n",
            "Epoch: 4/5  | Epoch loss: 2.5830225944519043  | Mean train accuracy: 0.8953355550765991\n",
            "Epoch: 4/5  | Epoch loss: 2.58329176902771  | Mean train accuracy: 0.8953437209129333\n",
            "Epoch: 4/5  | Epoch loss: 2.5835959911346436  | Mean train accuracy: 0.8953508734703064\n",
            "Epoch: 4/5  | Epoch loss: 2.5844357013702393  | Mean train accuracy: 0.8953593373298645\n",
            "Epoch: 4/5  | Epoch loss: 2.583686113357544  | Mean train accuracy: 0.8953647613525391\n",
            "Epoch: 4/5  | Epoch loss: 2.584245204925537  | Mean train accuracy: 0.8953700661659241\n",
            "Epoch: 4/5  | Epoch loss: 2.5840203762054443  | Mean train accuracy: 0.8953759074211121\n",
            "Epoch: 4/5  | Epoch loss: 2.583807945251465  | Mean train accuracy: 0.8953789472579956\n",
            "Epoch: 4/5  | Epoch loss: 2.5832693576812744  | Mean train accuracy: 0.8953770399093628\n",
            "Epoch: 4/5  | Epoch loss: 2.582622766494751  | Mean train accuracy: 0.8953747153282166\n",
            "Epoch: 4/5  | Epoch loss: 2.5814692974090576  | Mean train accuracy: 0.8953697085380554\n",
            "Epoch: 4/5  | Epoch loss: 2.5807747840881348  | Mean train accuracy: 0.8953680396080017\n",
            "Epoch: 4/5  | Epoch loss: 2.5804290771484375  | Mean train accuracy: 0.8953631520271301\n",
            "Epoch: 4/5  | Epoch loss: 2.581106185913086  | Mean train accuracy: 0.8953555226325989\n",
            "Epoch: 4/5  | Epoch loss: 2.58069109916687  | Mean train accuracy: 0.8953495025634766\n",
            "Epoch: 4/5  | Epoch loss: 2.580899238586426  | Mean train accuracy: 0.8953419327735901\n",
            "Epoch: 4/5  | Epoch loss: 2.5813217163085938  | Mean train accuracy: 0.8953347206115723\n",
            "Epoch: 4/5  | Epoch loss: 2.581019163131714  | Mean train accuracy: 0.8953274488449097\n",
            "Epoch: 4/5  | Epoch loss: 2.5806963443756104  | Mean train accuracy: 0.8953195214271545\n",
            "Epoch: 4/5  | Epoch loss: 2.58095121383667  | Mean train accuracy: 0.8953101634979248\n",
            "Epoch: 4/5  | Epoch loss: 2.5811753273010254  | Mean train accuracy: 0.8953021168708801\n",
            "Epoch: 4/5  | Epoch loss: 2.5806543827056885  | Mean train accuracy: 0.895296573638916\n",
            "Epoch: 4/5  | Epoch loss: 2.580130100250244  | Mean train accuracy: 0.8952926397323608\n",
            "Epoch: 4/5  | Epoch loss: 2.579803705215454  | Mean train accuracy: 0.8952889442443848\n",
            "Epoch: 4/5  | Epoch loss: 2.5798494815826416  | Mean train accuracy: 0.8952866792678833\n",
            "Epoch: 4/5  | Epoch loss: 2.5791523456573486  | Mean train accuracy: 0.8952829241752625\n",
            "Epoch: 4/5  | Epoch loss: 2.577890157699585  | Mean train accuracy: 0.8952823281288147\n",
            "Epoch: 4/5  | Epoch loss: 2.5779898166656494  | Mean train accuracy: 0.8952840566635132\n",
            "Epoch: 4/5  | Epoch loss: 2.578082323074341  | Mean train accuracy: 0.895282506942749\n",
            "Epoch: 4/5  | Epoch loss: 2.577562093734741  | Mean train accuracy: 0.895286500453949\n",
            "Epoch: 4/5  | Epoch loss: 2.5764212608337402  | Mean train accuracy: 0.8952877521514893\n",
            "Epoch: 4/5  | Epoch loss: 2.576395273208618  | Mean train accuracy: 0.8952895402908325\n",
            "Epoch: 4/5  | Epoch loss: 2.5753073692321777  | Mean train accuracy: 0.8952938318252563\n",
            "Epoch: 4/5  | Epoch loss: 2.574910879135132  | Mean train accuracy: 0.8953007459640503\n",
            "Epoch: 4/5  | Epoch loss: 2.5733633041381836  | Mean train accuracy: 0.895305871963501\n",
            "Epoch: 4/5  | Epoch loss: 2.5736427307128906  | Mean train accuracy: 0.8953100442886353\n",
            "Epoch: 4/5  | Epoch loss: 2.573894500732422  | Mean train accuracy: 0.8953168392181396\n",
            "Epoch: 4/5  | Epoch loss: 2.573789119720459  | Mean train accuracy: 0.8953284025192261\n",
            "Epoch: 4/5  | Epoch loss: 2.574748992919922  | Mean train accuracy: 0.895338237285614\n",
            "Epoch: 4/5  | Epoch loss: 2.574467420578003  | Mean train accuracy: 0.8953511118888855\n",
            "Epoch: 4/5  | Epoch loss: 2.5742831230163574  | Mean train accuracy: 0.8953645825386047\n",
            "Epoch: 4/5  | Epoch loss: 2.575727939605713  | Mean train accuracy: 0.8953784108161926\n",
            "Epoch: 4/5  | Epoch loss: 2.5744194984436035  | Mean train accuracy: 0.8953940272331238\n",
            "Epoch: 4/5  | Epoch loss: 2.5734214782714844  | Mean train accuracy: 0.8954123854637146\n",
            "Epoch: 4/5  | Epoch loss: 2.573716163635254  | Mean train accuracy: 0.8954302072525024\n",
            "Epoch: 4/5  | Epoch loss: 2.5743391513824463  | Mean train accuracy: 0.8954456448554993\n",
            "Epoch: 4/5  | Epoch loss: 2.574418067932129  | Mean train accuracy: 0.8954619765281677\n",
            "Epoch: 4/5  | Epoch loss: 2.574479341506958  | Mean train accuracy: 0.8954795598983765\n",
            "Epoch: 4/5  | Epoch loss: 2.5749716758728027  | Mean train accuracy: 0.8954971432685852\n",
            "Epoch: 4/5  | Epoch loss: 2.5744740962982178  | Mean train accuracy: 0.895514726638794\n",
            "Epoch: 4/5  | Epoch loss: 2.5749123096466064  | Mean train accuracy: 0.8955315351486206\n",
            "Epoch: 4/5  | Epoch loss: 2.5749571323394775  | Mean train accuracy: 0.8955497741699219\n",
            "Epoch: 4/5  | Epoch loss: 2.5753283500671387  | Mean train accuracy: 0.8955627679824829\n",
            "Epoch: 4/5  | Epoch loss: 2.5749804973602295  | Mean train accuracy: 0.8955782055854797\n",
            "Epoch: 4/5  | Epoch loss: 2.5751049518585205  | Mean train accuracy: 0.8955935835838318\n",
            "Epoch: 4/5  | Epoch loss: 2.5755062103271484  | Mean train accuracy: 0.8956048488616943\n",
            "Epoch: 4/5  | Epoch loss: 2.575681686401367  | Mean train accuracy: 0.895615816116333\n",
            "Epoch: 4/5  | Epoch loss: 2.5755958557128906  | Mean train accuracy: 0.8956270217895508\n",
            "Epoch: 4/5  | Epoch loss: 2.5755929946899414  | Mean train accuracy: 0.8956409692764282\n",
            "Epoch: 4/5  | Epoch loss: 2.5759804248809814  | Mean train accuracy: 0.8956543207168579\n",
            "Epoch: 4/5  | Epoch loss: 2.5755088329315186  | Mean train accuracy: 0.8956661820411682\n",
            "Epoch: 4/5  | Epoch loss: 2.5751824378967285  | Mean train accuracy: 0.8956785202026367\n",
            "Epoch: 4/5  | Epoch loss: 2.5759100914001465  | Mean train accuracy: 0.8956922292709351\n",
            "Epoch: 4/5  | Epoch loss: 2.575601816177368  | Mean train accuracy: 0.8957045078277588\n",
            "Epoch: 4/5  | Epoch loss: 2.5748860836029053  | Mean train accuracy: 0.8957204222679138\n",
            "Epoch: 4/5  | Epoch loss: 2.5745363235473633  | Mean train accuracy: 0.8957346081733704\n",
            "Epoch: 4/5  | Epoch loss: 2.5747172832489014  | Mean train accuracy: 0.8957483768463135\n",
            "Epoch: 4/5  | Epoch loss: 2.574725389480591  | Mean train accuracy: 0.8957678079605103\n",
            "Epoch: 4/5  | Epoch loss: 2.5743041038513184  | Mean train accuracy: 0.8957830667495728\n",
            "Epoch: 4/5  | Epoch loss: 2.574101448059082  | Mean train accuracy: 0.8958017826080322\n",
            "Epoch: 4/5  | Epoch loss: 2.5736937522888184  | Mean train accuracy: 0.8958184123039246\n",
            "Epoch: 4/5  | Epoch loss: 2.5741426944732666  | Mean train accuracy: 0.895836353302002\n",
            "Epoch: 4/5  | Epoch loss: 2.573612928390503  | Mean train accuracy: 0.895855188369751\n",
            "Epoch: 4/5  | Epoch loss: 2.57348370552063  | Mean train accuracy: 0.8958737254142761\n",
            "Epoch: 4/5  | Epoch loss: 2.57405161857605  | Mean train accuracy: 0.8958916664123535\n",
            "Epoch: 4/5  | Epoch loss: 2.573784112930298  | Mean train accuracy: 0.8959099054336548\n",
            "Epoch: 4/5  | Epoch loss: 2.5732338428497314  | Mean train accuracy: 0.8959274291992188\n",
            "Epoch: 4/5  | Epoch loss: 2.5741705894470215  | Mean train accuracy: 0.8959423303604126\n",
            "Epoch: 4/5  | Epoch loss: 2.5751705169677734  | Mean train accuracy: 0.8959589004516602\n",
            "Epoch: 4/5  | Epoch loss: 2.574711322784424  | Mean train accuracy: 0.8959749341011047\n",
            "Epoch: 4/5  | Epoch loss: 2.5744316577911377  | Mean train accuracy: 0.8959890007972717\n",
            "Epoch: 4/5  | Epoch loss: 2.57441782951355  | Mean train accuracy: 0.8960003852844238\n",
            "Epoch: 4/5  | Epoch loss: 2.574821710586548  | Mean train accuracy: 0.8960121273994446\n",
            "Epoch: 4/5  | Epoch loss: 2.575122594833374  | Mean train accuracy: 0.8960199952125549\n",
            "Epoch: 4/5  | Epoch loss: 2.575287103652954  | Mean train accuracy: 0.8960287570953369\n",
            "Epoch: 4/5  | Epoch loss: 2.5751757621765137  | Mean train accuracy: 0.8960363268852234\n",
            "Epoch: 4/5  | Epoch loss: 2.574509382247925  | Mean train accuracy: 0.8960444331169128\n",
            "Epoch: 4/5  | Epoch loss: 2.57524037361145  | Mean train accuracy: 0.8960500955581665\n",
            "Epoch: 4/5  | Epoch loss: 2.5757718086242676  | Mean train accuracy: 0.8960556983947754\n",
            "Epoch: 4/5  | Epoch loss: 2.57616925239563  | Mean train accuracy: 0.8960587978363037\n",
            "Epoch: 4/5  | Epoch loss: 2.5755562782287598  | Mean train accuracy: 0.8960621356964111\n",
            "Epoch: 4/5  | Epoch loss: 2.5752956867218018  | Mean train accuracy: 0.8960657715797424\n",
            "Epoch: 4/5  | Epoch loss: 2.5760231018066406  | Mean train accuracy: 0.8960709571838379\n",
            "Epoch: 4/5  | Epoch loss: 2.575537919998169  | Mean train accuracy: 0.8960772156715393\n",
            "Epoch: 4/5  | Epoch loss: 2.575345277786255  | Mean train accuracy: 0.896079421043396\n",
            "Epoch: 4/5  | Epoch loss: 2.575709819793701  | Mean train accuracy: 0.8960859775543213\n",
            "Epoch: 4/5  | Epoch loss: 2.575768232345581  | Mean train accuracy: 0.8960893154144287\n",
            "Epoch: 4/5  | Epoch loss: 2.575575113296509  | Mean train accuracy: 0.8960936665534973\n",
            "Epoch: 4/5  | Epoch loss: 2.5760579109191895  | Mean train accuracy: 0.896098792552948\n",
            "Epoch: 4/5  | Epoch loss: 2.575718641281128  | Mean train accuracy: 0.8961046934127808\n",
            "Epoch: 4/5  | Epoch loss: 2.5761842727661133  | Mean train accuracy: 0.8961103558540344\n",
            "Epoch: 4/5  | Epoch loss: 2.576003074645996  | Mean train accuracy: 0.8961179852485657\n",
            "Epoch: 4/5  | Epoch loss: 2.5761559009552  | Mean train accuracy: 0.8961268663406372\n",
            "Epoch: 4/5  | Epoch loss: 2.5762696266174316  | Mean train accuracy: 0.8961358070373535\n",
            "Epoch: 4/5  | Epoch loss: 2.5766501426696777  | Mean train accuracy: 0.8961461186408997\n",
            "Epoch: 4/5  | Epoch loss: 2.5763444900512695  | Mean train accuracy: 0.8961536884307861\n",
            "Epoch: 4/5  | Epoch loss: 2.5772061347961426  | Mean train accuracy: 0.8961614966392517\n",
            "Epoch: 4/5  | Epoch loss: 2.5773231983184814  | Mean train accuracy: 0.8961713314056396\n",
            "Epoch: 4/5  | Epoch loss: 2.5773377418518066  | Mean train accuracy: 0.8961787223815918\n",
            "Epoch: 4/5  | Epoch loss: 2.5774967670440674  | Mean train accuracy: 0.8961870074272156\n",
            "Epoch: 4/5  | Epoch loss: 2.577362060546875  | Mean train accuracy: 0.8961929082870483\n",
            "Epoch: 4/5  | Epoch loss: 2.5772173404693604  | Mean train accuracy: 0.8961979150772095\n",
            "Epoch: 4/5  | Epoch loss: 2.577596426010132  | Mean train accuracy: 0.8962021470069885\n",
            "Epoch: 4/5  | Epoch loss: 2.577962875366211  | Mean train accuracy: 0.8962057828903198\n",
            "Epoch: 4/5  | Epoch loss: 2.5780832767486572  | Mean train accuracy: 0.8962096571922302\n",
            "Epoch: 4/5  | Epoch loss: 2.578129768371582  | Mean train accuracy: 0.8962130546569824\n",
            "Epoch: 4/5  | Epoch loss: 2.5786001682281494  | Mean train accuracy: 0.8962156176567078\n",
            "Epoch: 4/5  | Epoch loss: 2.5786428451538086  | Mean train accuracy: 0.8962181806564331\n",
            "Epoch: 4/5  | Epoch loss: 2.5783755779266357  | Mean train accuracy: 0.8962228894233704\n",
            "Epoch: 4/5  | Epoch loss: 2.5784809589385986  | Mean train accuracy: 0.8962253928184509\n",
            "Epoch: 4/5  | Epoch loss: 2.5782456398010254  | Mean train accuracy: 0.8962289690971375\n",
            "Epoch: 4/5  | Epoch loss: 2.5788395404815674  | Mean train accuracy: 0.8962318301200867\n",
            "Epoch: 4/5  | Epoch loss: 2.578965425491333  | Mean train accuracy: 0.8962345123291016\n",
            "Epoch: 4/5  | Epoch loss: 2.5789010524749756  | Mean train accuracy: 0.8962401747703552\n",
            "Epoch: 4/5  | Epoch loss: 2.579293727874756  | Mean train accuracy: 0.8962444067001343\n",
            "Epoch: 4/5  | Epoch loss: 2.579693555831909  | Mean train accuracy: 0.8962501287460327\n",
            "Epoch: 4/5  | Epoch loss: 2.5799880027770996  | Mean train accuracy: 0.896255373954773\n",
            "Epoch: 4/5  | Epoch loss: 2.5799903869628906  | Mean train accuracy: 0.8962597846984863\n",
            "Epoch: 4/5  | Epoch loss: 2.579479455947876  | Mean train accuracy: 0.896266520023346\n",
            "Epoch: 4/5  | Epoch loss: 2.5790462493896484  | Mean train accuracy: 0.8962719440460205\n",
            "Epoch: 4/5  | Epoch loss: 2.5790114402770996  | Mean train accuracy: 0.8962776064872742\n",
            "Epoch: 4/5  | Epoch loss: 2.5787510871887207  | Mean train accuracy: 0.8962832093238831\n",
            "Epoch: 4/5  | Epoch loss: 2.578857898712158  | Mean train accuracy: 0.8962901830673218\n",
            "Epoch: 4/5  | Epoch loss: 2.5790152549743652  | Mean train accuracy: 0.8962972164154053\n",
            "Epoch: 4/5  | Epoch loss: 2.578521728515625  | Mean train accuracy: 0.896302342414856\n",
            "Epoch: 4/5  | Epoch loss: 2.578904867172241  | Mean train accuracy: 0.8963087797164917\n",
            "Epoch: 4/5  | Epoch loss: 2.5786516666412354  | Mean train accuracy: 0.8963173031806946\n",
            "Epoch: 4/5  | Epoch loss: 2.578770399093628  | Mean train accuracy: 0.8963241577148438\n",
            "Epoch: 4/5  | Epoch loss: 2.57861328125  | Mean train accuracy: 0.8963318467140198\n",
            "Epoch: 4/5  | Epoch loss: 2.578007698059082  | Mean train accuracy: 0.8963409662246704\n",
            "Epoch: 4/5  | Epoch loss: 2.5781052112579346  | Mean train accuracy: 0.8963505625724792\n",
            "Epoch: 4/5  | Epoch loss: 2.5784196853637695  | Mean train accuracy: 0.8963576555252075\n",
            "Epoch: 4/5  | Epoch loss: 2.5780751705169678  | Mean train accuracy: 0.8963657021522522\n",
            "Epoch: 4/5  | Epoch loss: 2.5783090591430664  | Mean train accuracy: 0.8963741064071655\n",
            "Epoch: 4/5  | Epoch loss: 2.5780222415924072  | Mean train accuracy: 0.8963820934295654\n",
            "Epoch: 4/5  | Epoch loss: 2.5782201290130615  | Mean train accuracy: 0.8963884115219116\n",
            "Epoch: 4/5  | Epoch loss: 2.577383518218994  | Mean train accuracy: 0.8963944911956787\n",
            "Epoch: 4/5  | Epoch loss: 2.57735013961792  | Mean train accuracy: 0.8964008688926697\n",
            "Epoch: 4/5  | Epoch loss: 2.5771167278289795  | Mean train accuracy: 0.8964048624038696\n",
            "Epoch: 4/5  | Epoch loss: 2.5773353576660156  | Mean train accuracy: 0.896406888961792\n",
            "Epoch: 4/5  | Epoch loss: 2.5769004821777344  | Mean train accuracy: 0.8964087963104248\n",
            "Epoch: 4/5  | Epoch loss: 2.576270341873169  | Mean train accuracy: 0.8964119553565979\n",
            "Epoch: 4/5  | Epoch loss: 2.5758399963378906  | Mean train accuracy: 0.8964157700538635\n",
            "Epoch: 4/5  | Epoch loss: 2.576521873474121  | Mean train accuracy: 0.896418035030365\n",
            "Epoch: 4/5  | Epoch loss: 2.576590061187744  | Mean train accuracy: 0.8964225053787231\n",
            "Epoch: 4/5  | Epoch loss: 2.576766014099121  | Mean train accuracy: 0.8964265584945679\n",
            "Epoch: 4/5  | Epoch loss: 2.5771522521972656  | Mean train accuracy: 0.8964290022850037\n",
            "Epoch: 4/5  | Epoch loss: 2.577024221420288  | Mean train accuracy: 0.8964341282844543\n",
            "Epoch: 4/5  | Epoch loss: 2.5769011974334717  | Mean train accuracy: 0.8964377045631409\n",
            "Epoch: 4/5  | Epoch loss: 2.576878786087036  | Mean train accuracy: 0.8964426517486572\n",
            "Epoch: 4/5  | Epoch loss: 2.577166795730591  | Mean train accuracy: 0.8964454531669617\n",
            "Epoch: 4/5  | Epoch loss: 2.577878475189209  | Mean train accuracy: 0.8964492678642273\n",
            "Epoch: 4/5  | Epoch loss: 2.5778250694274902  | Mean train accuracy: 0.8964546322822571\n",
            "Epoch: 4/5  | Epoch loss: 2.5776350498199463  | Mean train accuracy: 0.8964571356773376\n",
            "Epoch: 4/5  | Epoch loss: 2.5775442123413086  | Mean train accuracy: 0.8964582085609436\n",
            "Epoch: 4/5  | Epoch loss: 2.5770463943481445  | Mean train accuracy: 0.8964604139328003\n",
            "Epoch: 4/5  | Epoch loss: 2.577230930328369  | Mean train accuracy: 0.8964630365371704\n",
            "Epoch: 4/5  | Epoch loss: 2.576848030090332  | Mean train accuracy: 0.8964669108390808\n",
            "Epoch: 4/5  | Epoch loss: 2.576874256134033  | Mean train accuracy: 0.8964694142341614\n",
            "Epoch: 4/5  | Epoch loss: 2.5771570205688477  | Mean train accuracy: 0.8964729309082031\n",
            "Epoch: 4/5  | Epoch loss: 2.5774741172790527  | Mean train accuracy: 0.8964763879776001\n",
            "Epoch: 4/5  | Epoch loss: 2.5774078369140625  | Mean train accuracy: 0.8964830040931702\n",
            "Epoch: 4/5  | Epoch loss: 2.5775530338287354  | Mean train accuracy: 0.8964883089065552\n",
            "Epoch: 4/5  | Epoch loss: 2.5772953033447266  | Mean train accuracy: 0.8964942097663879\n",
            "Epoch: 4/5  | Epoch loss: 2.577320098876953  | Mean train accuracy: 0.8965005874633789\n",
            "Epoch: 4/5  | Epoch loss: 2.5775792598724365  | Mean train accuracy: 0.8965083360671997\n",
            "Epoch: 4/5  | Epoch loss: 2.5780088901519775  | Mean train accuracy: 0.8965154886245728\n",
            "Epoch: 4/5  | Epoch loss: 2.5780043601989746  | Mean train accuracy: 0.8965216279029846\n",
            "Epoch: 4/5  | Epoch loss: 2.5781688690185547  | Mean train accuracy: 0.8965280652046204\n",
            "Epoch: 4/5  | Epoch loss: 2.577930450439453  | Mean train accuracy: 0.8965337872505188\n",
            "Epoch: 4/5  | Epoch loss: 2.578286647796631  | Mean train accuracy: 0.8965381979942322\n",
            "Epoch: 4/5  | Epoch loss: 2.578272819519043  | Mean train accuracy: 0.8965436220169067\n",
            "Epoch: 4/5  | Epoch loss: 2.578023672103882  | Mean train accuracy: 0.8965492248535156\n",
            "Epoch: 4/5  | Epoch loss: 2.5779097080230713  | Mean train accuracy: 0.8965553641319275\n",
            "Epoch: 4/5  | Epoch loss: 2.5779953002929688  | Mean train accuracy: 0.8965599536895752\n",
            "Epoch: 4/5  | Epoch loss: 2.5776243209838867  | Mean train accuracy: 0.8965641260147095\n",
            "Epoch: 4/5  | Epoch loss: 2.577418565750122  | Mean train accuracy: 0.8965682983398438\n",
            "Epoch: 4/5  | Epoch loss: 2.5770647525787354  | Mean train accuracy: 0.8965704441070557\n",
            "Epoch: 4/5  | Epoch loss: 2.5769894123077393  | Mean train accuracy: 0.8965745568275452\n",
            "Epoch: 4/5  | Epoch loss: 2.577119827270508  | Mean train accuracy: 0.8965779542922974\n",
            "Epoch: 4/5  | Epoch loss: 2.5776195526123047  | Mean train accuracy: 0.8965811729431152\n",
            "Epoch: 4/5  | Epoch loss: 2.5773537158966064  | Mean train accuracy: 0.8965850472450256\n",
            "Epoch: 4/5  | Epoch loss: 2.5770015716552734  | Mean train accuracy: 0.8965887427330017\n",
            "Epoch: 4/5  | Epoch loss: 2.5773651599884033  | Mean train accuracy: 0.8965919613838196\n",
            "Epoch: 4/5  | Epoch loss: 2.576773166656494  | Mean train accuracy: 0.8965969085693359\n",
            "Epoch: 4/5  | Epoch loss: 2.5765817165374756  | Mean train accuracy: 0.8966005444526672\n",
            "Epoch: 4/5  | Epoch loss: 2.5764753818511963  | Mean train accuracy: 0.896604061126709\n",
            "Epoch: 4/5  | Epoch loss: 2.576721429824829  | Mean train accuracy: 0.8966082334518433\n",
            "Epoch: 4/5  | Epoch loss: 2.5763533115386963  | Mean train accuracy: 0.8966126441955566\n",
            "Epoch: 4/5  | Epoch loss: 2.576294422149658  | Mean train accuracy: 0.8966168761253357\n",
            "Epoch: 4/5  | Epoch loss: 2.5762901306152344  | Mean train accuracy: 0.8966221213340759\n",
            "Epoch: 4/5  | Epoch loss: 2.5768468379974365  | Mean train accuracy: 0.8966271877288818\n",
            "Epoch: 4/5  | Epoch loss: 2.5772504806518555  | Mean train accuracy: 0.8966313600540161\n",
            "Epoch: 4/5  | Epoch loss: 2.576784372329712  | Mean train accuracy: 0.8966370224952698\n",
            "Epoch: 4/5  | Epoch loss: 2.5771706104278564  | Mean train accuracy: 0.8966405391693115\n",
            "Epoch: 4/5  | Epoch loss: 2.5770928859710693  | Mean train accuracy: 0.8966432213783264\n",
            "Epoch: 4/5  | Epoch loss: 2.577265739440918  | Mean train accuracy: 0.8966466784477234\n",
            "Epoch: 4/5  | Epoch loss: 2.5770676136016846  | Mean train accuracy: 0.8966507315635681\n",
            "Epoch: 4/5  | Epoch loss: 2.5774495601654053  | Mean train accuracy: 0.896654486656189\n",
            "Epoch: 4/5  | Epoch loss: 2.5774950981140137  | Mean train accuracy: 0.8966564536094666\n",
            "Epoch: 4/5  | Epoch loss: 2.577820301055908  | Mean train accuracy: 0.8966570496559143\n",
            "Epoch: 4/5  | Epoch loss: 2.5776500701904297  | Mean train accuracy: 0.8966578841209412\n",
            "Epoch: 4/5  | Epoch loss: 2.577860116958618  | Mean train accuracy: 0.8966580033302307\n",
            "Epoch: 4/5  | Epoch loss: 2.578092098236084  | Mean train accuracy: 0.8966578841209412\n",
            "Epoch: 4/5  | Epoch loss: 2.578792095184326  | Mean train accuracy: 0.8966566324234009\n",
            "Epoch: 4/5  | Epoch loss: 2.578909397125244  | Mean train accuracy: 0.896656334400177\n",
            "Epoch: 4/5  | Epoch loss: 2.578575372695923  | Mean train accuracy: 0.8966550230979919\n",
            "Epoch: 4/5  | Epoch loss: 2.5789403915405273  | Mean train accuracy: 0.896652340888977\n",
            "Epoch: 4/5  | Epoch loss: 2.5787405967712402  | Mean train accuracy: 0.8966495394706726\n",
            "Epoch: 4/5  | Epoch loss: 2.5788066387176514  | Mean train accuracy: 0.896647572517395\n",
            "Epoch: 4/5  | Epoch loss: 2.579179048538208  | Mean train accuracy: 0.896644651889801\n",
            "Epoch: 4/5  | Epoch loss: 2.579315423965454  | Mean train accuracy: 0.8966410160064697\n",
            "Epoch: 4/5  | Epoch loss: 2.5792222023010254  | Mean train accuracy: 0.8966386318206787\n",
            "Epoch: 4/5  | Epoch loss: 2.5790817737579346  | Mean train accuracy: 0.8966355323791504\n",
            "Epoch: 4/5  | Epoch loss: 2.57891583442688  | Mean train accuracy: 0.8966328501701355\n",
            "Epoch: 4/5  | Epoch loss: 2.5788943767547607  | Mean train accuracy: 0.8966294527053833\n",
            "Epoch: 4/5  | Epoch loss: 2.5789082050323486  | Mean train accuracy: 0.8966277837753296\n",
            "Epoch: 4/5  | Epoch loss: 2.579047918319702  | Mean train accuracy: 0.8966270685195923\n",
            "Epoch: 4/5  | Epoch loss: 2.5786497592926025  | Mean train accuracy: 0.8966261148452759\n",
            "Epoch: 4/5  | Epoch loss: 2.57861590385437  | Mean train accuracy: 0.8966253399848938\n",
            "Epoch: 4/5  | Epoch loss: 2.5785064697265625  | Mean train accuracy: 0.896625280380249\n",
            "Epoch: 4/5  | Epoch loss: 2.5781266689300537  | Mean train accuracy: 0.8966264128684998\n",
            "Epoch: 4/5  | Epoch loss: 2.578038454055786  | Mean train accuracy: 0.8966248631477356\n",
            "Epoch: 4/5  | Epoch loss: 2.5782158374786377  | Mean train accuracy: 0.8966254591941833\n",
            "Epoch: 4/5  | Epoch loss: 2.578010320663452  | Mean train accuracy: 0.8966277837753296\n",
            "Epoch: 4/5  | Epoch loss: 2.578120470046997  | Mean train accuracy: 0.8966293334960938\n",
            "Epoch: 4/5  | Epoch loss: 2.5786890983581543  | Mean train accuracy: 0.8966310620307922\n",
            "Epoch: 4/5  | Epoch loss: 2.5788204669952393  | Mean train accuracy: 0.8966332077980042\n",
            "Epoch: 4/5  | Epoch loss: 2.578508138656616  | Mean train accuracy: 0.8966357111930847\n",
            "Epoch: 4/5  | Epoch loss: 2.578400135040283  | Mean train accuracy: 0.8966394066810608\n",
            "Epoch: 4/5  | Epoch loss: 2.578474760055542  | Mean train accuracy: 0.8966425061225891\n",
            "Epoch: 4/5  | Epoch loss: 2.578352689743042  | Mean train accuracy: 0.8966472744941711\n",
            "Epoch: 4/5  | Epoch loss: 2.578443765640259  | Mean train accuracy: 0.8966519832611084\n",
            "Epoch: 4/5  | Epoch loss: 2.5787930488586426  | Mean train accuracy: 0.8966547846794128\n",
            "Epoch: 4/5  | Epoch loss: 2.5792455673217773  | Mean train accuracy: 0.8966596722602844\n",
            "Epoch: 4/5  | Epoch loss: 2.579557180404663  | Mean train accuracy: 0.89666348695755\n",
            "Epoch: 4/5  | Epoch loss: 2.579313039779663  | Mean train accuracy: 0.89666748046875\n",
            "Epoch: 4/5  | Epoch loss: 2.579510450363159  | Mean train accuracy: 0.8966702818870544\n",
            "Epoch: 4/5  | Epoch loss: 2.5795605182647705  | Mean train accuracy: 0.8966729640960693\n",
            "Epoch: 4/5  | Epoch loss: 2.5789294242858887  | Mean train accuracy: 0.8966776728630066\n",
            "Epoch: 4/5  | Epoch loss: 2.578993082046509  | Mean train accuracy: 0.8966805934906006\n",
            "Epoch: 4/5  | Epoch loss: 2.57912278175354  | Mean train accuracy: 0.8966831564903259\n",
            "Epoch: 4/5  | Epoch loss: 2.579211950302124  | Mean train accuracy: 0.8966872692108154\n",
            "Epoch: 4/5  | Epoch loss: 2.579087734222412  | Mean train accuracy: 0.8966910243034363\n",
            "Epoch: 4/5  | Epoch loss: 2.578476905822754  | Mean train accuracy: 0.8966947793960571\n",
            "Epoch: 4/5  | Epoch loss: 2.5784683227539062  | Mean train accuracy: 0.896699070930481\n",
            "Epoch: 4/5  | Epoch loss: 2.578352212905884  | Mean train accuracy: 0.8967045545578003\n",
            "Epoch: 4/5  | Epoch loss: 2.578472137451172  | Mean train accuracy: 0.8967094421386719\n",
            "Epoch: 4/5  | Epoch loss: 2.5784881114959717  | Mean train accuracy: 0.8967153429985046\n",
            "Epoch: 4/5  | Epoch loss: 2.5786023139953613  | Mean train accuracy: 0.8967214226722717\n",
            "Epoch: 4/5  | Epoch loss: 2.578624725341797  | Mean train accuracy: 0.8967270255088806\n",
            "Epoch: 4/5  | Epoch loss: 2.5787200927734375  | Mean train accuracy: 0.8967334032058716\n",
            "Epoch: 4/5  | Epoch loss: 2.5784080028533936  | Mean train accuracy: 0.8967382311820984\n",
            "Epoch: 4/5  | Epoch loss: 2.578295946121216  | Mean train accuracy: 0.8967428207397461\n",
            "Epoch: 4/5  | Epoch loss: 2.577927827835083  | Mean train accuracy: 0.8967495560646057\n",
            "Epoch: 4/5  | Epoch loss: 2.578287124633789  | Mean train accuracy: 0.8967545628547668\n",
            "Epoch: 4/5  | Epoch loss: 2.5781466960906982  | Mean train accuracy: 0.8967611193656921\n",
            "Epoch: 4/5  | Epoch loss: 2.5781326293945312  | Mean train accuracy: 0.8967661261558533\n",
            "Epoch: 4/5  | Epoch loss: 2.578512668609619  | Mean train accuracy: 0.8967702388763428\n",
            "Epoch: 4/5  | Epoch loss: 2.5786914825439453  | Mean train accuracy: 0.8967729210853577\n",
            "Epoch: 4/5  | Epoch loss: 2.579054594039917  | Mean train accuracy: 0.8967753648757935\n",
            "Epoch: 4/5  | Epoch loss: 2.5788815021514893  | Mean train accuracy: 0.8967772722244263\n",
            "Epoch: 4/5  | Epoch loss: 2.5789620876312256  | Mean train accuracy: 0.8967777490615845\n",
            "Epoch: 4/5  | Epoch loss: 2.579042673110962  | Mean train accuracy: 0.8967781662940979\n",
            "Epoch: 4/5  | Epoch loss: 2.5786678791046143  | Mean train accuracy: 0.8967783451080322\n",
            "Epoch: 4/5  | Epoch loss: 2.5784969329833984  | Mean train accuracy: 0.8967804312705994\n",
            "Epoch: 4/5  | Epoch loss: 2.5782546997070312  | Mean train accuracy: 0.896782636642456\n",
            "Epoch: 4/5  | Epoch loss: 2.57855486869812  | Mean train accuracy: 0.8967840671539307\n",
            "Epoch: 4/5  | Epoch loss: 2.5783042907714844  | Mean train accuracy: 0.8967857956886292\n",
            "Epoch: 4/5  | Epoch loss: 2.577699899673462  | Mean train accuracy: 0.8967873454093933\n",
            "Epoch: 4/5  | Epoch loss: 2.5771684646606445  | Mean train accuracy: 0.8967902660369873\n",
            "Epoch: 4/5  | Epoch loss: 2.5770316123962402  | Mean train accuracy: 0.8967938423156738\n",
            "Epoch: 4/5  | Epoch loss: 2.577237367630005  | Mean train accuracy: 0.8967964053153992\n",
            "Epoch: 4/5  | Epoch loss: 2.5771377086639404  | Mean train accuracy: 0.8967992067337036\n",
            "Epoch: 4/5  | Epoch loss: 2.577216625213623  | Mean train accuracy: 0.8968027234077454\n",
            "Epoch: 4/5  | Epoch loss: 2.5772085189819336  | Mean train accuracy: 0.8968068361282349\n",
            "Epoch: 4/5  | Epoch loss: 2.577474594116211  | Mean train accuracy: 0.896809995174408\n",
            "Epoch: 4/5  | Epoch loss: 2.577592134475708  | Mean train accuracy: 0.8968149423599243\n",
            "Epoch: 4/5  | Epoch loss: 2.5781424045562744  | Mean train accuracy: 0.8968189358711243\n",
            "Epoch: 4/5  | Epoch loss: 2.578127861022949  | Mean train accuracy: 0.8968244194984436\n",
            "Epoch: 4/5  | Epoch loss: 2.578406572341919  | Mean train accuracy: 0.896827757358551\n",
            "Epoch: 4/5  | Epoch loss: 2.5783870220184326  | Mean train accuracy: 0.896832287311554\n",
            "Epoch: 4/5  | Epoch loss: 2.578273296356201  | Mean train accuracy: 0.8968369960784912\n",
            "Epoch: 4/5  | Epoch loss: 2.5781989097595215  | Mean train accuracy: 0.8968397378921509\n",
            "Epoch: 4/5  | Epoch loss: 2.5782229900360107  | Mean train accuracy: 0.8968443274497986\n",
            "Epoch: 4/5  | Epoch loss: 2.5778274536132812  | Mean train accuracy: 0.8968479633331299\n",
            "Epoch: 4/5  | Epoch loss: 2.5779001712799072  | Mean train accuracy: 0.8968513011932373\n",
            "Epoch: 4/5  | Epoch loss: 2.5778067111968994  | Mean train accuracy: 0.8968549966812134\n",
            "Epoch: 4/5  | Epoch loss: 2.578057289123535  | Mean train accuracy: 0.8968591094017029\n",
            "Epoch: 4/5  | Epoch loss: 2.5780539512634277  | Mean train accuracy: 0.896862268447876\n",
            "Epoch: 4/5  | Epoch loss: 2.577713966369629  | Mean train accuracy: 0.8968662023544312\n",
            "Epoch: 4/5  | Epoch loss: 2.577702522277832  | Mean train accuracy: 0.8968683481216431\n",
            "Epoch: 4/5  | Epoch loss: 2.5778489112854004  | Mean train accuracy: 0.8968711495399475\n",
            "Epoch: 4/5  | Epoch loss: 2.57783579826355  | Mean train accuracy: 0.8968733549118042\n",
            "Epoch: 4/5  | Epoch loss: 2.5774261951446533  | Mean train accuracy: 0.8968753814697266\n",
            "Epoch: 4/5  | Epoch loss: 2.5770435333251953  | Mean train accuracy: 0.8968766927719116\n",
            "Epoch: 4/5  | Epoch loss: 2.5770912170410156  | Mean train accuracy: 0.8968785405158997\n",
            "Epoch: 4/5  | Epoch loss: 2.5772454738616943  | Mean train accuracy: 0.8968800902366638\n",
            "Epoch: 4/5  | Epoch loss: 2.5775654315948486  | Mean train accuracy: 0.8968798518180847\n",
            "Epoch: 4/5  | Epoch loss: 2.5777933597564697  | Mean train accuracy: 0.8968796730041504\n",
            "Epoch: 4/5  | Epoch loss: 2.5779449939727783  | Mean train accuracy: 0.8968788981437683\n",
            "Epoch: 4/5  | Epoch loss: 2.5774271488189697  | Mean train accuracy: 0.8968779444694519\n",
            "Epoch: 4/5  | Epoch loss: 2.5771090984344482  | Mean train accuracy: 0.8968772292137146\n",
            "Epoch: 4/5  | Epoch loss: 2.5767219066619873  | Mean train accuracy: 0.8968767523765564\n",
            "Epoch: 4/5  | Epoch loss: 2.576388359069824  | Mean train accuracy: 0.8968759775161743\n",
            "Epoch: 4/5  | Epoch loss: 2.5767719745635986  | Mean train accuracy: 0.896873950958252\n",
            "Epoch: 4/5  | Epoch loss: 2.5767717361450195  | Mean train accuracy: 0.896872341632843\n",
            "Epoch: 4/5  | Epoch loss: 2.576960802078247  | Mean train accuracy: 0.896870493888855\n",
            "Epoch: 4/5  | Epoch loss: 2.5772879123687744  | Mean train accuracy: 0.896868109703064\n",
            "Epoch: 4/5  | Epoch loss: 2.5771291255950928  | Mean train accuracy: 0.8968672752380371\n",
            "Epoch: 4/5  | Epoch loss: 2.5771751403808594  | Mean train accuracy: 0.8968659043312073\n",
            "Epoch: 4/5  | Epoch loss: 2.577108144760132  | Mean train accuracy: 0.8968653678894043\n",
            "Epoch: 4/5  | Epoch loss: 2.577256917953491  | Mean train accuracy: 0.8968653082847595\n",
            "Epoch: 4/5  | Epoch loss: 2.5771448612213135  | Mean train accuracy: 0.8968670964241028\n",
            "Epoch: 4/5  | Epoch loss: 2.5772039890289307  | Mean train accuracy: 0.8968686461448669\n",
            "Epoch: 4/5  | Epoch loss: 2.577345371246338  | Mean train accuracy: 0.8968719840049744\n",
            "Epoch: 4/5  | Epoch loss: 2.5774919986724854  | Mean train accuracy: 0.896875262260437\n",
            "Epoch: 4/5  | Epoch loss: 2.5773508548736572  | Mean train accuracy: 0.8968786001205444\n",
            "Epoch: 4/5  | Epoch loss: 2.577186107635498  | Mean train accuracy: 0.8968834280967712\n",
            "Epoch: 4/5  | Epoch loss: 2.5770626068115234  | Mean train accuracy: 0.8968886137008667\n",
            "Epoch: 4/5  | Epoch loss: 2.5768401622772217  | Mean train accuracy: 0.8968953490257263\n",
            "Epoch: 4/5  | Epoch loss: 2.57637357711792  | Mean train accuracy: 0.8969017863273621\n",
            "Epoch: 4/5  | Epoch loss: 2.5762596130371094  | Mean train accuracy: 0.8969088196754456\n",
            "Epoch: 4/5  | Epoch loss: 2.5762035846710205  | Mean train accuracy: 0.8969160914421082\n",
            "Epoch: 4/5  | Epoch loss: 2.5765740871429443  | Mean train accuracy: 0.8969220519065857\n",
            "Epoch: 4/5  | Epoch loss: 2.5765602588653564  | Mean train accuracy: 0.8969293832778931\n",
            "Epoch: 4/5  | Epoch loss: 2.5767335891723633  | Mean train accuracy: 0.8969369530677795\n",
            "Epoch: 4/5  | Epoch loss: 2.5766804218292236  | Mean train accuracy: 0.8969451189041138\n",
            "Epoch: 4/5  | Epoch loss: 2.576636552810669  | Mean train accuracy: 0.896952211856842\n",
            "Epoch: 4/5  | Epoch loss: 2.576948642730713  | Mean train accuracy: 0.8969601988792419\n",
            "Epoch: 4/5  | Epoch loss: 2.5767929553985596  | Mean train accuracy: 0.896967887878418\n",
            "Epoch: 4/5  | Epoch loss: 2.5770370960235596  | Mean train accuracy: 0.8969753384590149\n",
            "Epoch: 4/5  | Epoch loss: 2.5770788192749023  | Mean train accuracy: 0.896983802318573\n",
            "Epoch: 4/5  | Epoch loss: 2.5770022869110107  | Mean train accuracy: 0.8969916105270386\n",
            "Epoch: 4/5  | Epoch loss: 2.5772757530212402  | Mean train accuracy: 0.8969995379447937\n",
            "Epoch: 4/5  | Epoch loss: 2.577296018600464  | Mean train accuracy: 0.8970064520835876\n",
            "Epoch: 4/5  | Epoch loss: 2.576977252960205  | Mean train accuracy: 0.8970138430595398\n",
            "Epoch: 4/5  | Epoch loss: 2.5770761966705322  | Mean train accuracy: 0.8970198035240173\n",
            "Epoch: 4/5  | Epoch loss: 2.5769994258880615  | Mean train accuracy: 0.8970252871513367\n",
            "Epoch: 4/5  | Epoch loss: 2.577190399169922  | Mean train accuracy: 0.8970304727554321\n",
            "Epoch: 4/5  | Epoch loss: 2.5770070552825928  | Mean train accuracy: 0.8970353007316589\n",
            "Epoch: 4/5  | Epoch loss: 2.577434778213501  | Mean train accuracy: 0.8970406651496887\n",
            "Epoch: 4/5  | Epoch loss: 2.5773916244506836  | Mean train accuracy: 0.8970467448234558\n",
            "Epoch: 4/5  | Epoch loss: 2.5774972438812256  | Mean train accuracy: 0.8970538973808289\n",
            "Epoch: 4/5  | Epoch loss: 2.5773651599884033  | Mean train accuracy: 0.8970590829849243\n",
            "Epoch: 4/5  | Epoch loss: 2.577653169631958  | Mean train accuracy: 0.8970639705657959\n",
            "Epoch: 4/5  | Epoch loss: 2.577415943145752  | Mean train accuracy: 0.8970696926116943\n",
            "Epoch: 4/5  | Epoch loss: 2.5776896476745605  | Mean train accuracy: 0.8970757126808167\n",
            "Epoch: 4/5  | Epoch loss: 2.5776920318603516  | Mean train accuracy: 0.8970816731452942\n",
            "Epoch: 4/5  | Epoch loss: 2.5777206420898438  | Mean train accuracy: 0.8970882296562195\n",
            "Epoch: 4/5  | Epoch loss: 2.577911615371704  | Mean train accuracy: 0.8970937728881836\n",
            "Epoch: 4/5  | Epoch loss: 2.578125  | Mean train accuracy: 0.8970997929573059\n",
            "Epoch: 4/5  | Epoch loss: 2.578272581100464  | Mean train accuracy: 0.8971059918403625\n",
            "Epoch: 4/5  | Epoch loss: 2.578120231628418  | Mean train accuracy: 0.8971130847930908\n",
            "Epoch: 4/5  | Epoch loss: 2.578352689743042  | Mean train accuracy: 0.8971196413040161\n",
            "Epoch: 4/5  | Epoch loss: 2.5784313678741455  | Mean train accuracy: 0.8971270322799683\n",
            "Epoch: 4/5  | Epoch loss: 2.5784850120544434  | Mean train accuracy: 0.897133469581604\n",
            "Epoch: 4/5  | Epoch loss: 2.578427791595459  | Mean train accuracy: 0.8971400856971741\n",
            "Epoch: 4/5  | Epoch loss: 2.578338384628296  | Mean train accuracy: 0.8971468210220337\n",
            "Epoch: 4/5  | Epoch loss: 2.5779964923858643  | Mean train accuracy: 0.8971514701843262\n",
            "Epoch: 4/5  | Epoch loss: 2.578101634979248  | Mean train accuracy: 0.8971579074859619\n",
            "Epoch: 4/5  | Epoch loss: 2.577930450439453  | Mean train accuracy: 0.897163450717926\n",
            "Epoch: 4/5  | Epoch loss: 2.5779078006744385  | Mean train accuracy: 0.8971694707870483\n",
            "Epoch: 4/5  | Epoch loss: 2.5778908729553223  | Mean train accuracy: 0.8971753716468811\n",
            "Epoch: 4/5  | Epoch loss: 2.5781068801879883  | Mean train accuracy: 0.897180438041687\n",
            "Epoch: 4/5  | Epoch loss: 2.5783486366271973  | Mean train accuracy: 0.8971856832504272\n",
            "Epoch: 4/5  | Epoch loss: 2.5784339904785156  | Mean train accuracy: 0.8971908688545227\n",
            "Epoch: 4/5  | Epoch loss: 2.5784363746643066  | Mean train accuracy: 0.8971959948539734\n",
            "Epoch: 4/5  | Epoch loss: 2.5784497261047363  | Mean train accuracy: 0.8972009420394897\n",
            "Epoch: 4/5  | Epoch loss: 2.5782856941223145  | Mean train accuracy: 0.8972058296203613\n",
            "Epoch: 4/5  | Epoch loss: 2.5778720378875732  | Mean train accuracy: 0.8972117304801941\n",
            "Epoch: 4/5  | Epoch loss: 2.5777974128723145  | Mean train accuracy: 0.8972170948982239\n",
            "Epoch: 4/5  | Epoch loss: 2.577864170074463  | Mean train accuracy: 0.8972228765487671\n",
            "Epoch: 4/5  | Epoch loss: 2.5777180194854736  | Mean train accuracy: 0.8972270488739014\n",
            "Epoch: 4/5  | Epoch loss: 2.5776126384735107  | Mean train accuracy: 0.8972320556640625\n",
            "Epoch: 4/5  | Epoch loss: 2.5774385929107666  | Mean train accuracy: 0.8972368240356445\n",
            "Epoch: 4/5  | Epoch loss: 2.5776686668395996  | Mean train accuracy: 0.8972412347793579\n",
            "Epoch: 4/5  | Epoch loss: 2.5775654315948486  | Mean train accuracy: 0.8972464799880981\n",
            "Epoch: 4/5  | Epoch loss: 2.578002452850342  | Mean train accuracy: 0.8972506523132324\n",
            "Epoch: 4/5  | Epoch loss: 2.5777108669281006  | Mean train accuracy: 0.8972548842430115\n",
            "Epoch: 4/5  | Epoch loss: 2.5775070190429688  | Mean train accuracy: 0.8972592949867249\n",
            "Epoch: 4/5  | Epoch loss: 2.577382802963257  | Mean train accuracy: 0.8972636461257935\n",
            "Epoch: 4/5  | Epoch loss: 2.577363967895508  | Mean train accuracy: 0.8972684741020203\n",
            "Epoch: 4/5  | Epoch loss: 2.5775163173675537  | Mean train accuracy: 0.8972733020782471\n",
            "Epoch: 4/5  | Epoch loss: 2.577404260635376  | Mean train accuracy: 0.897278368473053\n",
            "Epoch: 4/5  | Epoch loss: 2.5773937702178955  | Mean train accuracy: 0.8972833752632141\n",
            "Epoch: 4/5  | Epoch loss: 2.57733154296875  | Mean train accuracy: 0.8972901105880737\n",
            "Epoch: 4/5  | Epoch loss: 2.577641725540161  | Mean train accuracy: 0.8972947001457214\n",
            "Epoch: 4/5  | Epoch loss: 2.5777220726013184  | Mean train accuracy: 0.8972997665405273\n",
            "Epoch: 4/5  | Epoch loss: 2.577544927597046  | Mean train accuracy: 0.8973045945167542\n",
            "Epoch: 4/5  | Epoch loss: 2.5774593353271484  | Mean train accuracy: 0.8973099589347839\n",
            "Epoch: 4/5  | Epoch loss: 2.5779688358306885  | Mean train accuracy: 0.8973159790039062\n",
            "Epoch: 4/5  | Epoch loss: 2.5783324241638184  | Mean train accuracy: 0.8973217010498047\n",
            "Epoch: 4/5  | Epoch loss: 2.578619956970215  | Mean train accuracy: 0.897326648235321\n",
            "Epoch: 4/5  | Epoch loss: 2.5787389278411865  | Mean train accuracy: 0.8973333239555359\n",
            "Epoch: 4/5  | Epoch loss: 2.5787901878356934  | Mean train accuracy: 0.8973392844200134\n",
            "Epoch: 4/5  | Epoch loss: 2.578873872756958  | Mean train accuracy: 0.8973446488380432\n",
            "Epoch: 4/5  | Epoch loss: 2.5789296627044678  | Mean train accuracy: 0.8973506689071655\n",
            "Epoch: 4/5  | Epoch loss: 2.57901930809021  | Mean train accuracy: 0.8973574042320251\n",
            "Epoch: 4/5  | Epoch loss: 2.579334020614624  | Mean train accuracy: 0.8973633646965027\n",
            "Epoch: 4/5  | Epoch loss: 2.579559087753296  | Mean train accuracy: 0.8973690867424011\n",
            "Epoch: 4/5  | Epoch loss: 2.5800464153289795  | Mean train accuracy: 0.8973743319511414\n",
            "Epoch: 4/5  | Epoch loss: 2.5800366401672363  | Mean train accuracy: 0.8973799347877502\n",
            "Epoch: 4/5  | Epoch loss: 2.5799448490142822  | Mean train accuracy: 0.8973840475082397\n",
            "Epoch: 4/5  | Epoch loss: 2.5802485942840576  | Mean train accuracy: 0.8973880410194397\n",
            "Epoch: 4/5  | Epoch loss: 2.580223798751831  | Mean train accuracy: 0.8973908424377441\n",
            "Epoch: 4/5  | Epoch loss: 2.5799560546875  | Mean train accuracy: 0.897392749786377\n",
            "Epoch: 4/5  | Epoch loss: 2.579833507537842  | Mean train accuracy: 0.8973942399024963\n",
            "Epoch: 4/5  | Epoch loss: 2.58011531829834  | Mean train accuracy: 0.8973953723907471\n",
            "Epoch: 4/5  | Epoch loss: 2.580198287963867  | Mean train accuracy: 0.8973963260650635\n",
            "Epoch: 4/5  | Epoch loss: 2.5801212787628174  | Mean train accuracy: 0.8973968029022217\n",
            "Epoch: 4/5  | Epoch loss: 2.580019950866699  | Mean train accuracy: 0.8973972201347351\n",
            "Epoch: 4/5  | Epoch loss: 2.5799026489257812  | Mean train accuracy: 0.8973969221115112\n",
            "Epoch: 4/5  | Epoch loss: 2.5797905921936035  | Mean train accuracy: 0.8973960876464844\n",
            "Epoch: 4/5  | Epoch loss: 2.579824447631836  | Mean train accuracy: 0.8973956108093262\n",
            "Epoch: 4/5  | Epoch loss: 2.580035924911499  | Mean train accuracy: 0.8973943591117859\n",
            "Epoch: 4/5  | Epoch loss: 2.5796172618865967  | Mean train accuracy: 0.897393524646759\n",
            "Epoch: 4/5  | Epoch loss: 2.579249620437622  | Mean train accuracy: 0.897392749786377\n",
            "Epoch: 4/5  | Epoch loss: 2.5791335105895996  | Mean train accuracy: 0.8973916172981262\n",
            "Epoch: 4/5  | Epoch loss: 2.5792479515075684  | Mean train accuracy: 0.8973907828330994\n",
            "Epoch: 4/5  | Epoch loss: 2.579237222671509  | Mean train accuracy: 0.8973902463912964\n",
            "Epoch: 4/5  | Epoch loss: 2.578904390335083  | Mean train accuracy: 0.897389829158783\n",
            "Epoch: 4/5  | Epoch loss: 2.5787353515625  | Mean train accuracy: 0.8973891139030457\n",
            "Epoch: 4/5  | Epoch loss: 2.5787241458892822  | Mean train accuracy: 0.8973888158798218\n",
            "Epoch: 4/5  | Epoch loss: 2.578756093978882  | Mean train accuracy: 0.8973886370658875\n",
            "Epoch: 4/5  | Epoch loss: 2.578538417816162  | Mean train accuracy: 0.8973897099494934\n",
            "Epoch: 4/5  | Epoch loss: 2.578709840774536  | Mean train accuracy: 0.8973909020423889\n",
            "Epoch: 4/5  | Epoch loss: 2.579040050506592  | Mean train accuracy: 0.8973931074142456\n",
            "Epoch: 4/5  | Epoch loss: 2.5788538455963135  | Mean train accuracy: 0.8973953127861023\n",
            "Epoch: 4/5  | Epoch loss: 2.578803539276123  | Mean train accuracy: 0.8973985314369202\n",
            "Epoch: 4/5  | Epoch loss: 2.5786852836608887  | Mean train accuracy: 0.8974011540412903\n",
            "Epoch: 4/5  | Epoch loss: 2.5785071849823  | Mean train accuracy: 0.8974043726921082\n",
            "Epoch: 4/5  | Epoch loss: 2.578618288040161  | Mean train accuracy: 0.8974080085754395\n",
            "Epoch: 4/5  | Epoch loss: 2.5784952640533447  | Mean train accuracy: 0.8974115252494812\n",
            "Epoch: 4/5  | Epoch loss: 2.578282356262207  | Mean train accuracy: 0.8974155187606812\n",
            "Epoch: 4/5  | Epoch loss: 2.5782382488250732  | Mean train accuracy: 0.897419810295105\n",
            "Epoch: 4/5  | Epoch loss: 2.5784788131713867  | Mean train accuracy: 0.8974238038063049\n",
            "Epoch: 4/5  | Epoch loss: 2.578359603881836  | Mean train accuracy: 0.8974292278289795\n",
            "Epoch: 4/5  | Epoch loss: 2.578249931335449  | Mean train accuracy: 0.8974348306655884\n",
            "Epoch: 4/5  | Epoch loss: 2.5781283378601074  | Mean train accuracy: 0.8974406123161316\n",
            "Epoch: 4/5  | Epoch loss: 2.5783610343933105  | Mean train accuracy: 0.8974460959434509\n",
            "Epoch: 4/5  | Epoch loss: 2.5781164169311523  | Mean train accuracy: 0.8974520564079285\n",
            "Epoch: 4/5  | Epoch loss: 2.578212022781372  | Mean train accuracy: 0.8974582552909851\n",
            "Epoch: 4/5  | Epoch loss: 2.5779290199279785  | Mean train accuracy: 0.8974646925926208\n",
            "Epoch: 4/5  | Epoch loss: 2.5776443481445312  | Mean train accuracy: 0.8974711298942566\n",
            "Epoch: 4/5  | Epoch loss: 2.577967882156372  | Mean train accuracy: 0.897477388381958\n",
            "Epoch: 4/5  | Epoch loss: 2.5779733657836914  | Mean train accuracy: 0.8974840641021729\n",
            "Epoch: 4/5  | Epoch loss: 2.5778777599334717  | Mean train accuracy: 0.8974905014038086\n",
            "Epoch: 4/5  | Epoch loss: 2.577871561050415  | Mean train accuracy: 0.897497296333313\n",
            "Epoch: 4/5  | Epoch loss: 2.5777323246002197  | Mean train accuracy: 0.8975045084953308\n",
            "Epoch: 4/5  | Epoch loss: 2.577807903289795  | Mean train accuracy: 0.8975113034248352\n",
            "Epoch: 4/5  | Epoch loss: 2.577913284301758  | Mean train accuracy: 0.8975186347961426\n",
            "Epoch: 4/5  | Epoch loss: 2.5778915882110596  | Mean train accuracy: 0.8975235819816589\n",
            "Epoch: 4/5  | Epoch loss: 2.578125476837158  | Mean train accuracy: 0.8975278735160828\n",
            "Epoch: 4/5  | Epoch loss: 2.577981948852539  | Mean train accuracy: 0.8975327610969543\n",
            "Epoch: 4/5  | Epoch loss: 2.5784010887145996  | Mean train accuracy: 0.8975372314453125\n",
            "Epoch: 4/5  | Epoch loss: 2.5781965255737305  | Mean train accuracy: 0.8975421190261841\n",
            "Epoch: 4/5  | Epoch loss: 2.5780797004699707  | Mean train accuracy: 0.8975469470024109\n",
            "Epoch: 4/5  | Epoch loss: 2.5781939029693604  | Mean train accuracy: 0.8975505828857422\n",
            "Epoch: 4/5  | Epoch loss: 2.578256607055664  | Mean train accuracy: 0.8975542187690735\n",
            "Epoch: 4/5  | Epoch loss: 2.5782957077026367  | Mean train accuracy: 0.8975580334663391\n",
            "Epoch: 4/5  | Epoch loss: 2.578239679336548  | Mean train accuracy: 0.8975600600242615\n",
            "Epoch: 4/5  | Epoch loss: 2.5785775184631348  | Mean train accuracy: 0.8975622653961182\n",
            "Epoch: 4/5  | Epoch loss: 2.578704357147217  | Mean train accuracy: 0.8975646495819092\n",
            "Epoch: 4/5  | Epoch loss: 2.5787925720214844  | Mean train accuracy: 0.8975662589073181\n",
            "Epoch: 4/5  | Epoch loss: 2.578810930252075  | Mean train accuracy: 0.897567868232727\n",
            "Epoch: 4/5  | Epoch loss: 2.5791139602661133  | Mean train accuracy: 0.8975676894187927\n",
            "Epoch: 4/5  | Epoch loss: 2.5792720317840576  | Mean train accuracy: 0.8975681066513062\n",
            "Epoch: 4/5  | Epoch loss: 2.5792617797851562  | Mean train accuracy: 0.8975692391395569\n",
            "Epoch: 4/5  | Epoch loss: 2.5789637565612793  | Mean train accuracy: 0.8975704312324524\n",
            "Epoch: 4/5  | Epoch loss: 2.5792012214660645  | Mean train accuracy: 0.8975722789764404\n",
            "Epoch: 4/5  | Epoch loss: 2.5789077281951904  | Mean train accuracy: 0.8975739479064941\n",
            "Epoch: 4/5  | Epoch loss: 2.5791075229644775  | Mean train accuracy: 0.8975768089294434\n",
            "Epoch: 4/5  | Epoch loss: 2.5789408683776855  | Mean train accuracy: 0.8975797295570374\n",
            "Epoch: 4/5  | Epoch loss: 2.5791337490081787  | Mean train accuracy: 0.897581934928894\n",
            "Epoch: 4/5  | Epoch loss: 2.5791656970977783  | Mean train accuracy: 0.8975843787193298\n",
            "Epoch: 4/5  | Epoch loss: 2.579091787338257  | Mean train accuracy: 0.8975869417190552\n",
            "Epoch: 4/5  | Epoch loss: 2.5789577960968018  | Mean train accuracy: 0.897590160369873\n",
            "Epoch: 4/5  | Epoch loss: 2.578996419906616  | Mean train accuracy: 0.8975930213928223\n",
            "Epoch: 4/5  | Epoch loss: 2.5790867805480957  | Mean train accuracy: 0.8975957632064819\n",
            "Epoch: 4/5  | Epoch loss: 2.579230546951294  | Mean train accuracy: 0.8975971937179565\n",
            "Epoch: 4/5  | Epoch loss: 2.5789904594421387  | Mean train accuracy: 0.897599995136261\n",
            "Epoch: 4/5  | Epoch loss: 2.5790534019470215  | Mean train accuracy: 0.8976023197174072\n",
            "Epoch: 4/5  | Epoch loss: 2.5791618824005127  | Mean train accuracy: 0.8976048827171326\n",
            "Epoch: 4/5  | Epoch loss: 2.5792388916015625  | Mean train accuracy: 0.8976064920425415\n",
            "Epoch: 4/5  | Epoch loss: 2.5794076919555664  | Mean train accuracy: 0.8976079821586609\n",
            "Epoch: 4/5  | Epoch loss: 2.5793838500976562  | Mean train accuracy: 0.8976088762283325\n",
            "Epoch: 4/5  | Epoch loss: 2.5791444778442383  | Mean train accuracy: 0.8976109027862549\n",
            "Epoch: 4/5  | Epoch loss: 2.5794074535369873  | Mean train accuracy: 0.8976120352745056\n",
            "Epoch: 4/5  | Epoch loss: 2.5795841217041016  | Mean train accuracy: 0.8976132869720459\n",
            "Epoch: 4/5  | Epoch loss: 2.5792553424835205  | Mean train accuracy: 0.8976153135299683\n",
            "Epoch: 4/5  | Epoch loss: 2.579345703125  | Mean train accuracy: 0.8976168036460876\n",
            "Epoch: 4/5  | Epoch loss: 2.579514980316162  | Mean train accuracy: 0.8976171016693115\n",
            "Epoch: 4/5  | Epoch loss: 2.5796377658843994  | Mean train accuracy: 0.8976178169250488\n",
            "Epoch: 4/5  | Epoch loss: 2.5800487995147705  | Mean train accuracy: 0.8976184725761414\n",
            "Epoch: 4/5  | Epoch loss: 2.580155372619629  | Mean train accuracy: 0.8976184725761414\n",
            "Epoch: 4/5  | Epoch loss: 2.5802416801452637  | Mean train accuracy: 0.8976191282272339\n",
            "Epoch: 4/5  | Epoch loss: 2.5802454948425293  | Mean train accuracy: 0.8976203799247742\n",
            "Epoch: 4/5  | Epoch loss: 2.5802502632141113  | Mean train accuracy: 0.8976207971572876\n",
            "Epoch: 4/5  | Epoch loss: 2.5803415775299072  | Mean train accuracy: 0.8976202011108398\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ae5036df4b74b06914015cf671f5a61",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=655), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/5  | Epoch loss: 2.5593345165252686  | Mean train accuracy: 0.8976897597312927\n",
            "Epoch: 5/5  | Epoch loss: 2.5350542068481445  | Mean train accuracy: 0.8977671265602112\n",
            "Epoch: 5/5  | Epoch loss: 2.568711519241333  | Mean train accuracy: 0.8975479602813721\n",
            "Epoch: 5/5  | Epoch loss: 2.5909786224365234  | Mean train accuracy: 0.8974931836128235\n",
            "Epoch: 5/5  | Epoch loss: 2.591726779937744  | Mean train accuracy: 0.8974499702453613\n",
            "Epoch: 5/5  | Epoch loss: 2.5861363410949707  | Mean train accuracy: 0.8973352313041687\n",
            "Epoch: 5/5  | Epoch loss: 2.590301513671875  | Mean train accuracy: 0.8973103165626526\n",
            "Epoch: 5/5  | Epoch loss: 2.5924808979034424  | Mean train accuracy: 0.8972369432449341\n",
            "Epoch: 5/5  | Epoch loss: 2.605217456817627  | Mean train accuracy: 0.8971655368804932\n",
            "Epoch: 5/5  | Epoch loss: 2.5831165313720703  | Mean train accuracy: 0.8971328735351562\n",
            "Epoch: 5/5  | Epoch loss: 2.594522476196289  | Mean train accuracy: 0.8970733284950256\n",
            "Epoch: 5/5  | Epoch loss: 2.59782338142395  | Mean train accuracy: 0.8970291018486023\n",
            "Epoch: 5/5  | Epoch loss: 2.6005961894989014  | Mean train accuracy: 0.8969727754592896\n",
            "Epoch: 5/5  | Epoch loss: 2.6081788539886475  | Mean train accuracy: 0.8969494104385376\n",
            "Epoch: 5/5  | Epoch loss: 2.5939393043518066  | Mean train accuracy: 0.8968973755836487\n",
            "Epoch: 5/5  | Epoch loss: 2.589704990386963  | Mean train accuracy: 0.896862268447876\n",
            "Epoch: 5/5  | Epoch loss: 2.5896098613739014  | Mean train accuracy: 0.8968199491500854\n",
            "Epoch: 5/5  | Epoch loss: 2.5819947719573975  | Mean train accuracy: 0.8967550992965698\n",
            "Epoch: 5/5  | Epoch loss: 2.595033645629883  | Mean train accuracy: 0.8966978192329407\n",
            "Epoch: 5/5  | Epoch loss: 2.5805349349975586  | Mean train accuracy: 0.8966571688652039\n",
            "Epoch: 5/5  | Epoch loss: 2.585371732711792  | Mean train accuracy: 0.8966209888458252\n",
            "Epoch: 5/5  | Epoch loss: 2.5895426273345947  | Mean train accuracy: 0.8965675830841064\n",
            "Epoch: 5/5  | Epoch loss: 2.5953962802886963  | Mean train accuracy: 0.8965054154396057\n",
            "Epoch: 5/5  | Epoch loss: 2.5952937602996826  | Mean train accuracy: 0.8964473605155945\n",
            "Epoch: 5/5  | Epoch loss: 2.5936760902404785  | Mean train accuracy: 0.8963671326637268\n",
            "Epoch: 5/5  | Epoch loss: 2.595224618911743  | Mean train accuracy: 0.8963267207145691\n",
            "Epoch: 5/5  | Epoch loss: 2.5983755588531494  | Mean train accuracy: 0.8962849974632263\n",
            "Epoch: 5/5  | Epoch loss: 2.602893590927124  | Mean train accuracy: 0.8962380290031433\n",
            "Epoch: 5/5  | Epoch loss: 2.6042613983154297  | Mean train accuracy: 0.8962076902389526\n",
            "Epoch: 5/5  | Epoch loss: 2.6073501110076904  | Mean train accuracy: 0.8961517810821533\n",
            "Epoch: 5/5  | Epoch loss: 2.6071653366088867  | Mean train accuracy: 0.8961090445518494\n",
            "Epoch: 5/5  | Epoch loss: 2.6060280799865723  | Mean train accuracy: 0.8960678577423096\n",
            "Epoch: 5/5  | Epoch loss: 2.607311487197876  | Mean train accuracy: 0.8960087895393372\n",
            "Epoch: 5/5  | Epoch loss: 2.6029491424560547  | Mean train accuracy: 0.895990788936615\n",
            "Epoch: 5/5  | Epoch loss: 2.6033542156219482  | Mean train accuracy: 0.8959788680076599\n",
            "Epoch: 5/5  | Epoch loss: 2.596853733062744  | Mean train accuracy: 0.8959651589393616\n",
            "Epoch: 5/5  | Epoch loss: 2.594933271408081  | Mean train accuracy: 0.8959716558456421\n",
            "Epoch: 5/5  | Epoch loss: 2.591313123703003  | Mean train accuracy: 0.8959843516349792\n",
            "Epoch: 5/5  | Epoch loss: 2.593924045562744  | Mean train accuracy: 0.8959913849830627\n",
            "Epoch: 5/5  | Epoch loss: 2.5962488651275635  | Mean train accuracy: 0.8960148096084595\n",
            "Epoch: 5/5  | Epoch loss: 2.5931787490844727  | Mean train accuracy: 0.8960399031639099\n",
            "Epoch: 5/5  | Epoch loss: 2.5921154022216797  | Mean train accuracy: 0.8960665464401245\n",
            "Epoch: 5/5  | Epoch loss: 2.5924527645111084  | Mean train accuracy: 0.896093487739563\n",
            "Epoch: 5/5  | Epoch loss: 2.5902585983276367  | Mean train accuracy: 0.8961130380630493\n",
            "Epoch: 5/5  | Epoch loss: 2.591042995452881  | Mean train accuracy: 0.8961139917373657\n",
            "Epoch: 5/5  | Epoch loss: 2.5944066047668457  | Mean train accuracy: 0.8961485028266907\n",
            "Epoch: 5/5  | Epoch loss: 2.5942986011505127  | Mean train accuracy: 0.8961681723594666\n",
            "Epoch: 5/5  | Epoch loss: 2.5965328216552734  | Mean train accuracy: 0.8961967825889587\n",
            "Epoch: 5/5  | Epoch loss: 2.595832586288452  | Mean train accuracy: 0.8962132930755615\n",
            "Epoch: 5/5  | Epoch loss: 2.5940756797790527  | Mean train accuracy: 0.8962309956550598\n",
            "Epoch: 5/5  | Epoch loss: 2.596900224685669  | Mean train accuracy: 0.8962434530258179\n",
            "Epoch: 5/5  | Epoch loss: 2.6011440753936768  | Mean train accuracy: 0.8962615728378296\n",
            "Epoch: 5/5  | Epoch loss: 2.6034111976623535  | Mean train accuracy: 0.8962763547897339\n",
            "Epoch: 5/5  | Epoch loss: 2.602872848510742  | Mean train accuracy: 0.8962989449501038\n",
            "Epoch: 5/5  | Epoch loss: 2.599501371383667  | Mean train accuracy: 0.8963077664375305\n",
            "Epoch: 5/5  | Epoch loss: 2.600032329559326  | Mean train accuracy: 0.8963128328323364\n",
            "Epoch: 5/5  | Epoch loss: 2.5974013805389404  | Mean train accuracy: 0.8963320255279541\n",
            "Epoch: 5/5  | Epoch loss: 2.5981993675231934  | Mean train accuracy: 0.8963488340377808\n",
            "Epoch: 5/5  | Epoch loss: 2.6000237464904785  | Mean train accuracy: 0.8963621854782104\n",
            "Epoch: 5/5  | Epoch loss: 2.597825765609741  | Mean train accuracy: 0.8963834047317505\n",
            "Epoch: 5/5  | Epoch loss: 2.5951478481292725  | Mean train accuracy: 0.8964022397994995\n",
            "Epoch: 5/5  | Epoch loss: 2.5928432941436768  | Mean train accuracy: 0.896431565284729\n",
            "Epoch: 5/5  | Epoch loss: 2.594163179397583  | Mean train accuracy: 0.8964549899101257\n",
            "Epoch: 5/5  | Epoch loss: 2.5945210456848145  | Mean train accuracy: 0.8964841961860657\n",
            "Epoch: 5/5  | Epoch loss: 2.595789909362793  | Mean train accuracy: 0.8965142369270325\n",
            "Epoch: 5/5  | Epoch loss: 2.5980446338653564  | Mean train accuracy: 0.8965535759925842\n",
            "Epoch: 5/5  | Epoch loss: 2.5959227085113525  | Mean train accuracy: 0.8965840339660645\n",
            "Epoch: 5/5  | Epoch loss: 2.5966997146606445  | Mean train accuracy: 0.8966197967529297\n",
            "Epoch: 5/5  | Epoch loss: 2.5953128337860107  | Mean train accuracy: 0.896659255027771\n",
            "Epoch: 5/5  | Epoch loss: 2.5948660373687744  | Mean train accuracy: 0.8966925740242004\n",
            "Epoch: 5/5  | Epoch loss: 2.595130443572998  | Mean train accuracy: 0.8967313170433044\n",
            "Epoch: 5/5  | Epoch loss: 2.59476375579834  | Mean train accuracy: 0.8967759609222412\n",
            "Epoch: 5/5  | Epoch loss: 2.5934338569641113  | Mean train accuracy: 0.89681077003479\n",
            "Epoch: 5/5  | Epoch loss: 2.5911312103271484  | Mean train accuracy: 0.8968496322631836\n",
            "Epoch: 5/5  | Epoch loss: 2.5889041423797607  | Mean train accuracy: 0.8968802690505981\n",
            "Epoch: 5/5  | Epoch loss: 2.587822675704956  | Mean train accuracy: 0.8969236612319946\n",
            "Epoch: 5/5  | Epoch loss: 2.590076446533203  | Mean train accuracy: 0.8969516754150391\n",
            "Epoch: 5/5  | Epoch loss: 2.589984893798828  | Mean train accuracy: 0.8969855904579163\n",
            "Epoch: 5/5  | Epoch loss: 2.5882248878479004  | Mean train accuracy: 0.8970274329185486\n",
            "Epoch: 5/5  | Epoch loss: 2.591187000274658  | Mean train accuracy: 0.8970574140548706\n",
            "Epoch: 5/5  | Epoch loss: 2.5917506217956543  | Mean train accuracy: 0.8970931172370911\n",
            "Epoch: 5/5  | Epoch loss: 2.591376304626465  | Mean train accuracy: 0.897128701210022\n",
            "Epoch: 5/5  | Epoch loss: 2.5916030406951904  | Mean train accuracy: 0.8971633911132812\n",
            "Epoch: 5/5  | Epoch loss: 2.5904223918914795  | Mean train accuracy: 0.8971995115280151\n",
            "Epoch: 5/5  | Epoch loss: 2.589906692504883  | Mean train accuracy: 0.897229015827179\n",
            "Epoch: 5/5  | Epoch loss: 2.5891754627227783  | Mean train accuracy: 0.8972613215446472\n",
            "Epoch: 5/5  | Epoch loss: 2.5898358821868896  | Mean train accuracy: 0.897288978099823\n",
            "Epoch: 5/5  | Epoch loss: 2.588374376296997  | Mean train accuracy: 0.8973251581192017\n",
            "Epoch: 5/5  | Epoch loss: 2.5893588066101074  | Mean train accuracy: 0.8973559141159058\n",
            "Epoch: 5/5  | Epoch loss: 2.589810609817505  | Mean train accuracy: 0.8973836898803711\n",
            "Epoch: 5/5  | Epoch loss: 2.5898327827453613  | Mean train accuracy: 0.897409200668335\n",
            "Epoch: 5/5  | Epoch loss: 2.589639663696289  | Mean train accuracy: 0.897426962852478\n",
            "Epoch: 5/5  | Epoch loss: 2.588597059249878  | Mean train accuracy: 0.8974483013153076\n",
            "Epoch: 5/5  | Epoch loss: 2.589719772338867  | Mean train accuracy: 0.8974608778953552\n",
            "Epoch: 5/5  | Epoch loss: 2.5923514366149902  | Mean train accuracy: 0.8974810242652893\n",
            "Epoch: 5/5  | Epoch loss: 2.593822717666626  | Mean train accuracy: 0.8974969983100891\n",
            "Epoch: 5/5  | Epoch loss: 2.593858480453491  | Mean train accuracy: 0.8975103497505188\n",
            "Epoch: 5/5  | Epoch loss: 2.5938498973846436  | Mean train accuracy: 0.8975158333778381\n",
            "Epoch: 5/5  | Epoch loss: 2.59383225440979  | Mean train accuracy: 0.8975228071212769\n",
            "Epoch: 5/5  | Epoch loss: 2.5917768478393555  | Mean train accuracy: 0.8975219130516052\n",
            "Epoch: 5/5  | Epoch loss: 2.5936334133148193  | Mean train accuracy: 0.8975206017494202\n",
            "Epoch: 5/5  | Epoch loss: 2.595057964324951  | Mean train accuracy: 0.8975138664245605\n",
            "Epoch: 5/5  | Epoch loss: 2.5948452949523926  | Mean train accuracy: 0.8975101113319397\n",
            "Epoch: 5/5  | Epoch loss: 2.5958962440490723  | Mean train accuracy: 0.8975018858909607\n",
            "Epoch: 5/5  | Epoch loss: 2.595158576965332  | Mean train accuracy: 0.8974887132644653\n",
            "Epoch: 5/5  | Epoch loss: 2.592752695083618  | Mean train accuracy: 0.8974757790565491\n",
            "Epoch: 5/5  | Epoch loss: 2.592034339904785  | Mean train accuracy: 0.8974575400352478\n",
            "Epoch: 5/5  | Epoch loss: 2.5908071994781494  | Mean train accuracy: 0.8974415063858032\n",
            "Epoch: 5/5  | Epoch loss: 2.589810371398926  | Mean train accuracy: 0.8974290490150452\n",
            "Epoch: 5/5  | Epoch loss: 2.589782953262329  | Mean train accuracy: 0.8974177837371826\n",
            "Epoch: 5/5  | Epoch loss: 2.5888254642486572  | Mean train accuracy: 0.8974111676216125\n",
            "Epoch: 5/5  | Epoch loss: 2.587772846221924  | Mean train accuracy: 0.8974041938781738\n",
            "Epoch: 5/5  | Epoch loss: 2.589099168777466  | Mean train accuracy: 0.8973995447158813\n",
            "Epoch: 5/5  | Epoch loss: 2.5901989936828613  | Mean train accuracy: 0.897394597530365\n",
            "Epoch: 5/5  | Epoch loss: 2.589069128036499  | Mean train accuracy: 0.8973991274833679\n",
            "Epoch: 5/5  | Epoch loss: 2.5892579555511475  | Mean train accuracy: 0.8974044919013977\n",
            "Epoch: 5/5  | Epoch loss: 2.5893185138702393  | Mean train accuracy: 0.8974067568778992\n",
            "Epoch: 5/5  | Epoch loss: 2.589974880218506  | Mean train accuracy: 0.8974133133888245\n",
            "Epoch: 5/5  | Epoch loss: 2.589022159576416  | Mean train accuracy: 0.897420346736908\n",
            "Epoch: 5/5  | Epoch loss: 2.589507818222046  | Mean train accuracy: 0.8974221348762512\n",
            "Epoch: 5/5  | Epoch loss: 2.589095115661621  | Mean train accuracy: 0.8974270224571228\n",
            "Epoch: 5/5  | Epoch loss: 2.588550329208374  | Mean train accuracy: 0.8974360227584839\n",
            "Epoch: 5/5  | Epoch loss: 2.5883352756500244  | Mean train accuracy: 0.8974440097808838\n",
            "Epoch: 5/5  | Epoch loss: 2.5881752967834473  | Mean train accuracy: 0.8974559307098389\n",
            "Epoch: 5/5  | Epoch loss: 2.5893502235412598  | Mean train accuracy: 0.8974635004997253\n",
            "Epoch: 5/5  | Epoch loss: 2.5884206295013428  | Mean train accuracy: 0.8974736928939819\n",
            "Epoch: 5/5  | Epoch loss: 2.5884013175964355  | Mean train accuracy: 0.8974831104278564\n",
            "Epoch: 5/5  | Epoch loss: 2.5874171257019043  | Mean train accuracy: 0.8974925875663757\n",
            "Epoch: 5/5  | Epoch loss: 2.5879433155059814  | Mean train accuracy: 0.8975074887275696\n",
            "Epoch: 5/5  | Epoch loss: 2.58817720413208  | Mean train accuracy: 0.8975222110748291\n",
            "Epoch: 5/5  | Epoch loss: 2.5874626636505127  | Mean train accuracy: 0.8975374698638916\n",
            "Epoch: 5/5  | Epoch loss: 2.587285280227661  | Mean train accuracy: 0.8975492715835571\n",
            "Epoch: 5/5  | Epoch loss: 2.587005376815796  | Mean train accuracy: 0.8975695967674255\n",
            "Epoch: 5/5  | Epoch loss: 2.586716413497925  | Mean train accuracy: 0.8975880146026611\n",
            "Epoch: 5/5  | Epoch loss: 2.586371898651123  | Mean train accuracy: 0.8976078629493713\n",
            "Epoch: 5/5  | Epoch loss: 2.585911989212036  | Mean train accuracy: 0.8976295590400696\n",
            "Epoch: 5/5  | Epoch loss: 2.5861599445343018  | Mean train accuracy: 0.8976525068283081\n",
            "Epoch: 5/5  | Epoch loss: 2.5859782695770264  | Mean train accuracy: 0.8976721167564392\n",
            "Epoch: 5/5  | Epoch loss: 2.5859858989715576  | Mean train accuracy: 0.8976867198944092\n",
            "Epoch: 5/5  | Epoch loss: 2.5857982635498047  | Mean train accuracy: 0.8976974487304688\n",
            "Epoch: 5/5  | Epoch loss: 2.5853207111358643  | Mean train accuracy: 0.8977075815200806\n",
            "Epoch: 5/5  | Epoch loss: 2.585261344909668  | Mean train accuracy: 0.8977177739143372\n",
            "Epoch: 5/5  | Epoch loss: 2.584768056869507  | Mean train accuracy: 0.8977277278900146\n",
            "Epoch: 5/5  | Epoch loss: 2.584653377532959  | Mean train accuracy: 0.8977401256561279\n",
            "Epoch: 5/5  | Epoch loss: 2.584044933319092  | Mean train accuracy: 0.8977535963058472\n",
            "Epoch: 5/5  | Epoch loss: 2.583907127380371  | Mean train accuracy: 0.8977652788162231\n",
            "Epoch: 5/5  | Epoch loss: 2.584165096282959  | Mean train accuracy: 0.8977712988853455\n",
            "Epoch: 5/5  | Epoch loss: 2.584418535232544  | Mean train accuracy: 0.8977799415588379\n",
            "Epoch: 5/5  | Epoch loss: 2.5851480960845947  | Mean train accuracy: 0.8977883458137512\n",
            "Epoch: 5/5  | Epoch loss: 2.584458351135254  | Mean train accuracy: 0.897796094417572\n",
            "Epoch: 5/5  | Epoch loss: 2.585007905960083  | Mean train accuracy: 0.8978031277656555\n",
            "Epoch: 5/5  | Epoch loss: 2.5850393772125244  | Mean train accuracy: 0.8978078961372375\n",
            "Epoch: 5/5  | Epoch loss: 2.5847320556640625  | Mean train accuracy: 0.8978111743927002\n",
            "Epoch: 5/5  | Epoch loss: 2.584113359451294  | Mean train accuracy: 0.8978108763694763\n",
            "Epoch: 5/5  | Epoch loss: 2.5834567546844482  | Mean train accuracy: 0.8978108167648315\n",
            "Epoch: 5/5  | Epoch loss: 2.5823163986206055  | Mean train accuracy: 0.8978060483932495\n",
            "Epoch: 5/5  | Epoch loss: 2.58160400390625  | Mean train accuracy: 0.8978030681610107\n",
            "Epoch: 5/5  | Epoch loss: 2.5813937187194824  | Mean train accuracy: 0.897796630859375\n",
            "Epoch: 5/5  | Epoch loss: 2.582003593444824  | Mean train accuracy: 0.8977921009063721\n",
            "Epoch: 5/5  | Epoch loss: 2.5813937187194824  | Mean train accuracy: 0.8977838754653931\n",
            "Epoch: 5/5  | Epoch loss: 2.5816471576690674  | Mean train accuracy: 0.897774338722229\n",
            "Epoch: 5/5  | Epoch loss: 2.5821051597595215  | Mean train accuracy: 0.8977680206298828\n",
            "Epoch: 5/5  | Epoch loss: 2.581817150115967  | Mean train accuracy: 0.8977605104446411\n",
            "Epoch: 5/5  | Epoch loss: 2.581413745880127  | Mean train accuracy: 0.8977539539337158\n",
            "Epoch: 5/5  | Epoch loss: 2.581549882888794  | Mean train accuracy: 0.8977437615394592\n",
            "Epoch: 5/5  | Epoch loss: 2.5818440914154053  | Mean train accuracy: 0.8977357745170593\n",
            "Epoch: 5/5  | Epoch loss: 2.5813586711883545  | Mean train accuracy: 0.8977299332618713\n",
            "Epoch: 5/5  | Epoch loss: 2.5808422565460205  | Mean train accuracy: 0.8977227210998535\n",
            "Epoch: 5/5  | Epoch loss: 2.5804829597473145  | Mean train accuracy: 0.897720456123352\n",
            "Epoch: 5/5  | Epoch loss: 2.5805552005767822  | Mean train accuracy: 0.8977173566818237\n",
            "Epoch: 5/5  | Epoch loss: 2.579937219619751  | Mean train accuracy: 0.8977163434028625\n",
            "Epoch: 5/5  | Epoch loss: 2.5788369178771973  | Mean train accuracy: 0.8977159261703491\n",
            "Epoch: 5/5  | Epoch loss: 2.578695774078369  | Mean train accuracy: 0.897718071937561\n",
            "Epoch: 5/5  | Epoch loss: 2.578770160675049  | Mean train accuracy: 0.8977187871932983\n",
            "Epoch: 5/5  | Epoch loss: 2.578240156173706  | Mean train accuracy: 0.897723376750946\n",
            "Epoch: 5/5  | Epoch loss: 2.5771851539611816  | Mean train accuracy: 0.8977240920066833\n",
            "Epoch: 5/5  | Epoch loss: 2.5771403312683105  | Mean train accuracy: 0.8977264761924744\n",
            "Epoch: 5/5  | Epoch loss: 2.5760202407836914  | Mean train accuracy: 0.8977289199829102\n",
            "Epoch: 5/5  | Epoch loss: 2.575664520263672  | Mean train accuracy: 0.8977318406105042\n",
            "Epoch: 5/5  | Epoch loss: 2.5741162300109863  | Mean train accuracy: 0.8977380394935608\n",
            "Epoch: 5/5  | Epoch loss: 2.5744106769561768  | Mean train accuracy: 0.8977433443069458\n",
            "Epoch: 5/5  | Epoch loss: 2.5745413303375244  | Mean train accuracy: 0.897750198841095\n",
            "Epoch: 5/5  | Epoch loss: 2.5745742321014404  | Mean train accuracy: 0.8977577090263367\n",
            "Epoch: 5/5  | Epoch loss: 2.575615167617798  | Mean train accuracy: 0.8977649807929993\n",
            "Epoch: 5/5  | Epoch loss: 2.575310230255127  | Mean train accuracy: 0.897774338722229\n",
            "Epoch: 5/5  | Epoch loss: 2.574949026107788  | Mean train accuracy: 0.8977851271629333\n",
            "Epoch: 5/5  | Epoch loss: 2.576568841934204  | Mean train accuracy: 0.8977963328361511\n",
            "Epoch: 5/5  | Epoch loss: 2.575474262237549  | Mean train accuracy: 0.8978092670440674\n",
            "Epoch: 5/5  | Epoch loss: 2.5743565559387207  | Mean train accuracy: 0.8978219628334045\n",
            "Epoch: 5/5  | Epoch loss: 2.5746660232543945  | Mean train accuracy: 0.8978366255760193\n",
            "Epoch: 5/5  | Epoch loss: 2.575396776199341  | Mean train accuracy: 0.8978508114814758\n",
            "Epoch: 5/5  | Epoch loss: 2.5756208896636963  | Mean train accuracy: 0.8978669047355652\n",
            "Epoch: 5/5  | Epoch loss: 2.5757503509521484  | Mean train accuracy: 0.8978815674781799\n",
            "Epoch: 5/5  | Epoch loss: 2.576192855834961  | Mean train accuracy: 0.8978977799415588\n",
            "Epoch: 5/5  | Epoch loss: 2.575728416442871  | Mean train accuracy: 0.8979136943817139\n",
            "Epoch: 5/5  | Epoch loss: 2.5761349201202393  | Mean train accuracy: 0.8979278206825256\n",
            "Epoch: 5/5  | Epoch loss: 2.576317071914673  | Mean train accuracy: 0.8979425430297852\n",
            "Epoch: 5/5  | Epoch loss: 2.5767219066619873  | Mean train accuracy: 0.897953987121582\n",
            "Epoch: 5/5  | Epoch loss: 2.576401948928833  | Mean train accuracy: 0.8979673385620117\n",
            "Epoch: 5/5  | Epoch loss: 2.576448917388916  | Mean train accuracy: 0.8979789614677429\n",
            "Epoch: 5/5  | Epoch loss: 2.576908588409424  | Mean train accuracy: 0.8979901075363159\n",
            "Epoch: 5/5  | Epoch loss: 2.577153205871582  | Mean train accuracy: 0.8980031609535217\n",
            "Epoch: 5/5  | Epoch loss: 2.5770137310028076  | Mean train accuracy: 0.8980146646499634\n",
            "Epoch: 5/5  | Epoch loss: 2.5769829750061035  | Mean train accuracy: 0.8980205655097961\n",
            "Epoch: 5/5  | Epoch loss: 2.577522039413452  | Mean train accuracy: 0.8980290293693542\n",
            "Epoch: 5/5  | Epoch loss: 2.576955795288086  | Mean train accuracy: 0.8980394601821899\n",
            "Epoch: 5/5  | Epoch loss: 2.5765504837036133  | Mean train accuracy: 0.89805006980896\n",
            "Epoch: 5/5  | Epoch loss: 2.5772297382354736  | Mean train accuracy: 0.8980613350868225\n",
            "Epoch: 5/5  | Epoch loss: 2.5768849849700928  | Mean train accuracy: 0.8980752229690552\n",
            "Epoch: 5/5  | Epoch loss: 2.576047420501709  | Mean train accuracy: 0.8980904221534729\n",
            "Epoch: 5/5  | Epoch loss: 2.5754895210266113  | Mean train accuracy: 0.898105800151825\n",
            "Epoch: 5/5  | Epoch loss: 2.5756587982177734  | Mean train accuracy: 0.8981242179870605\n",
            "Epoch: 5/5  | Epoch loss: 2.575695514678955  | Mean train accuracy: 0.8981426358222961\n",
            "Epoch: 5/5  | Epoch loss: 2.57535719871521  | Mean train accuracy: 0.8981624245643616\n",
            "Epoch: 5/5  | Epoch loss: 2.5752930641174316  | Mean train accuracy: 0.8981794714927673\n",
            "Epoch: 5/5  | Epoch loss: 2.574921131134033  | Mean train accuracy: 0.8981959819793701\n",
            "Epoch: 5/5  | Epoch loss: 2.5753183364868164  | Mean train accuracy: 0.8982133269309998\n",
            "Epoch: 5/5  | Epoch loss: 2.5747740268707275  | Mean train accuracy: 0.8982335925102234\n",
            "Epoch: 5/5  | Epoch loss: 2.5746428966522217  | Mean train accuracy: 0.8982563614845276\n",
            "Epoch: 5/5  | Epoch loss: 2.575164794921875  | Mean train accuracy: 0.8982769846916199\n",
            "Epoch: 5/5  | Epoch loss: 2.5749902725219727  | Mean train accuracy: 0.8983005881309509\n",
            "Epoch: 5/5  | Epoch loss: 2.574441909790039  | Mean train accuracy: 0.8983202576637268\n",
            "Epoch: 5/5  | Epoch loss: 2.575408458709717  | Mean train accuracy: 0.898338258266449\n",
            "Epoch: 5/5  | Epoch loss: 2.5763626098632812  | Mean train accuracy: 0.898357093334198\n",
            "Epoch: 5/5  | Epoch loss: 2.5758543014526367  | Mean train accuracy: 0.8983720541000366\n",
            "Epoch: 5/5  | Epoch loss: 2.57559871673584  | Mean train accuracy: 0.8983882069587708\n",
            "Epoch: 5/5  | Epoch loss: 2.5756072998046875  | Mean train accuracy: 0.898398756980896\n",
            "Epoch: 5/5  | Epoch loss: 2.576082229614258  | Mean train accuracy: 0.8984118103981018\n",
            "Epoch: 5/5  | Epoch loss: 2.576508045196533  | Mean train accuracy: 0.8984209895133972\n",
            "Epoch: 5/5  | Epoch loss: 2.5766398906707764  | Mean train accuracy: 0.8984302282333374\n",
            "Epoch: 5/5  | Epoch loss: 2.57655930519104  | Mean train accuracy: 0.898438572883606\n",
            "Epoch: 5/5  | Epoch loss: 2.5759642124176025  | Mean train accuracy: 0.8984452486038208\n",
            "Epoch: 5/5  | Epoch loss: 2.5767478942871094  | Mean train accuracy: 0.8984508514404297\n",
            "Epoch: 5/5  | Epoch loss: 2.5771098136901855  | Mean train accuracy: 0.8984556198120117\n",
            "Epoch: 5/5  | Epoch loss: 2.5774216651916504  | Mean train accuracy: 0.8984586596488953\n",
            "Epoch: 5/5  | Epoch loss: 2.5768473148345947  | Mean train accuracy: 0.8984642624855042\n",
            "Epoch: 5/5  | Epoch loss: 2.5766186714172363  | Mean train accuracy: 0.8984680771827698\n",
            "Epoch: 5/5  | Epoch loss: 2.5772085189819336  | Mean train accuracy: 0.8984705805778503\n",
            "Epoch: 5/5  | Epoch loss: 2.5766985416412354  | Mean train accuracy: 0.8984723687171936\n",
            "Epoch: 5/5  | Epoch loss: 2.576341152191162  | Mean train accuracy: 0.8984747529029846\n",
            "Epoch: 5/5  | Epoch loss: 2.5767831802368164  | Mean train accuracy: 0.8984822034835815\n",
            "Epoch: 5/5  | Epoch loss: 2.5769009590148926  | Mean train accuracy: 0.8984875679016113\n",
            "Epoch: 5/5  | Epoch loss: 2.5767691135406494  | Mean train accuracy: 0.8984906673431396\n",
            "Epoch: 5/5  | Epoch loss: 2.577136516571045  | Mean train accuracy: 0.8984952569007874\n",
            "Epoch: 5/5  | Epoch loss: 2.5767366886138916  | Mean train accuracy: 0.8984994292259216\n",
            "Epoch: 5/5  | Epoch loss: 2.5772323608398438  | Mean train accuracy: 0.8985058665275574\n",
            "Epoch: 5/5  | Epoch loss: 2.5769765377044678  | Mean train accuracy: 0.8985128998756409\n",
            "Epoch: 5/5  | Epoch loss: 2.5770723819732666  | Mean train accuracy: 0.8985216021537781\n",
            "Epoch: 5/5  | Epoch loss: 2.5771262645721436  | Mean train accuracy: 0.8985296487808228\n",
            "Epoch: 5/5  | Epoch loss: 2.577564001083374  | Mean train accuracy: 0.8985381126403809\n",
            "Epoch: 5/5  | Epoch loss: 2.5772883892059326  | Mean train accuracy: 0.8985474109649658\n",
            "Epoch: 5/5  | Epoch loss: 2.578155517578125  | Mean train accuracy: 0.898557186126709\n",
            "Epoch: 5/5  | Epoch loss: 2.578263998031616  | Mean train accuracy: 0.8985667824745178\n",
            "Epoch: 5/5  | Epoch loss: 2.5782809257507324  | Mean train accuracy: 0.8985720276832581\n",
            "Epoch: 5/5  | Epoch loss: 2.578429698944092  | Mean train accuracy: 0.8985781669616699\n",
            "Epoch: 5/5  | Epoch loss: 2.578204870223999  | Mean train accuracy: 0.8985817432403564\n",
            "Epoch: 5/5  | Epoch loss: 2.578213930130005  | Mean train accuracy: 0.8985841274261475\n",
            "Epoch: 5/5  | Epoch loss: 2.5787696838378906  | Mean train accuracy: 0.8985872864723206\n",
            "Epoch: 5/5  | Epoch loss: 2.5791966915130615  | Mean train accuracy: 0.8985888957977295\n",
            "Epoch: 5/5  | Epoch loss: 2.579334259033203  | Mean train accuracy: 0.8985904455184937\n",
            "Epoch: 5/5  | Epoch loss: 2.579437732696533  | Mean train accuracy: 0.8985904455184937\n",
            "Epoch: 5/5  | Epoch loss: 2.5799622535705566  | Mean train accuracy: 0.8985905647277832\n",
            "Epoch: 5/5  | Epoch loss: 2.5799717903137207  | Mean train accuracy: 0.8985891342163086\n",
            "Epoch: 5/5  | Epoch loss: 2.5796518325805664  | Mean train accuracy: 0.8985871076583862\n",
            "Epoch: 5/5  | Epoch loss: 2.579730987548828  | Mean train accuracy: 0.8985875248908997\n",
            "Epoch: 5/5  | Epoch loss: 2.579467296600342  | Mean train accuracy: 0.8985857963562012\n",
            "Epoch: 5/5  | Epoch loss: 2.5800764560699463  | Mean train accuracy: 0.8985840082168579\n",
            "Epoch: 5/5  | Epoch loss: 2.580095052719116  | Mean train accuracy: 0.8985835909843445\n",
            "Epoch: 5/5  | Epoch loss: 2.579932451248169  | Mean train accuracy: 0.8985825777053833\n",
            "Epoch: 5/5  | Epoch loss: 2.5802950859069824  | Mean train accuracy: 0.8985846042633057\n",
            "Epoch: 5/5  | Epoch loss: 2.580700635910034  | Mean train accuracy: 0.8985856771469116\n",
            "Epoch: 5/5  | Epoch loss: 2.581049919128418  | Mean train accuracy: 0.8985875248908997\n",
            "Epoch: 5/5  | Epoch loss: 2.5810816287994385  | Mean train accuracy: 0.8985893726348877\n",
            "Epoch: 5/5  | Epoch loss: 2.5806643962860107  | Mean train accuracy: 0.8985925316810608\n",
            "Epoch: 5/5  | Epoch loss: 2.580211639404297  | Mean train accuracy: 0.8985939621925354\n",
            "Epoch: 5/5  | Epoch loss: 2.58025860786438  | Mean train accuracy: 0.8985957503318787\n",
            "Epoch: 5/5  | Epoch loss: 2.579893112182617  | Mean train accuracy: 0.8985984325408936\n",
            "Epoch: 5/5  | Epoch loss: 2.580000400543213  | Mean train accuracy: 0.8986019492149353\n",
            "Epoch: 5/5  | Epoch loss: 2.5801098346710205  | Mean train accuracy: 0.8986048698425293\n",
            "Epoch: 5/5  | Epoch loss: 2.579568386077881  | Mean train accuracy: 0.8986085653305054\n",
            "Epoch: 5/5  | Epoch loss: 2.579869031906128  | Mean train accuracy: 0.8986101150512695\n",
            "Epoch: 5/5  | Epoch loss: 2.5795934200286865  | Mean train accuracy: 0.8986132740974426\n",
            "Epoch: 5/5  | Epoch loss: 2.579620838165283  | Mean train accuracy: 0.8986155986785889\n",
            "Epoch: 5/5  | Epoch loss: 2.5793817043304443  | Mean train accuracy: 0.8986176252365112\n",
            "Epoch: 5/5  | Epoch loss: 2.578822612762451  | Mean train accuracy: 0.8986234068870544\n",
            "Epoch: 5/5  | Epoch loss: 2.578914165496826  | Mean train accuracy: 0.8986274600028992\n",
            "Epoch: 5/5  | Epoch loss: 2.5791730880737305  | Mean train accuracy: 0.8986325263977051\n",
            "Epoch: 5/5  | Epoch loss: 2.578826904296875  | Mean train accuracy: 0.8986384272575378\n",
            "Epoch: 5/5  | Epoch loss: 2.5790326595306396  | Mean train accuracy: 0.8986438512802124\n",
            "Epoch: 5/5  | Epoch loss: 2.5787112712860107  | Mean train accuracy: 0.8986487984657288\n",
            "Epoch: 5/5  | Epoch loss: 2.5788991451263428  | Mean train accuracy: 0.8986542820930481\n",
            "Epoch: 5/5  | Epoch loss: 2.578084945678711  | Mean train accuracy: 0.898658037185669\n",
            "Epoch: 5/5  | Epoch loss: 2.5779857635498047  | Mean train accuracy: 0.8986601829528809\n",
            "Epoch: 5/5  | Epoch loss: 2.577744722366333  | Mean train accuracy: 0.898663341999054\n",
            "Epoch: 5/5  | Epoch loss: 2.5779573917388916  | Mean train accuracy: 0.8986642360687256\n",
            "Epoch: 5/5  | Epoch loss: 2.5775129795074463  | Mean train accuracy: 0.8986657857894897\n",
            "Epoch: 5/5  | Epoch loss: 2.5769264698028564  | Mean train accuracy: 0.8986680507659912\n",
            "Epoch: 5/5  | Epoch loss: 2.576544761657715  | Mean train accuracy: 0.8986691236495972\n",
            "Epoch: 5/5  | Epoch loss: 2.5773885250091553  | Mean train accuracy: 0.8986697793006897\n",
            "Epoch: 5/5  | Epoch loss: 2.5773983001708984  | Mean train accuracy: 0.8986737132072449\n",
            "Epoch: 5/5  | Epoch loss: 2.577484607696533  | Mean train accuracy: 0.8986775875091553\n",
            "Epoch: 5/5  | Epoch loss: 2.5778536796569824  | Mean train accuracy: 0.8986812233924866\n",
            "Epoch: 5/5  | Epoch loss: 2.577725410461426  | Mean train accuracy: 0.8986861705780029\n",
            "Epoch: 5/5  | Epoch loss: 2.577622175216675  | Mean train accuracy: 0.8986896872520447\n",
            "Epoch: 5/5  | Epoch loss: 2.5775930881500244  | Mean train accuracy: 0.8986928462982178\n",
            "Epoch: 5/5  | Epoch loss: 2.577895164489746  | Mean train accuracy: 0.8986985087394714\n",
            "Epoch: 5/5  | Epoch loss: 2.578580617904663  | Mean train accuracy: 0.8987016081809998\n",
            "Epoch: 5/5  | Epoch loss: 2.5786259174346924  | Mean train accuracy: 0.8987058401107788\n",
            "Epoch: 5/5  | Epoch loss: 2.5784859657287598  | Mean train accuracy: 0.8987084031105042\n",
            "Epoch: 5/5  | Epoch loss: 2.57840633392334  | Mean train accuracy: 0.8987106084823608\n",
            "Epoch: 5/5  | Epoch loss: 2.5778801441192627  | Mean train accuracy: 0.8987112045288086\n",
            "Epoch: 5/5  | Epoch loss: 2.578117609024048  | Mean train accuracy: 0.8987120389938354\n",
            "Epoch: 5/5  | Epoch loss: 2.5777111053466797  | Mean train accuracy: 0.8987152576446533\n",
            "Epoch: 5/5  | Epoch loss: 2.5777432918548584  | Mean train accuracy: 0.8987185955047607\n",
            "Epoch: 5/5  | Epoch loss: 2.5780704021453857  | Mean train accuracy: 0.8987215757369995\n",
            "Epoch: 5/5  | Epoch loss: 2.5784695148468018  | Mean train accuracy: 0.8987262845039368\n",
            "Epoch: 5/5  | Epoch loss: 2.578484296798706  | Mean train accuracy: 0.8987327814102173\n",
            "Epoch: 5/5  | Epoch loss: 2.5786755084991455  | Mean train accuracy: 0.8987390398979187\n",
            "Epoch: 5/5  | Epoch loss: 2.578449010848999  | Mean train accuracy: 0.898748517036438\n",
            "Epoch: 5/5  | Epoch loss: 2.5783755779266357  | Mean train accuracy: 0.8987566828727722\n",
            "Epoch: 5/5  | Epoch loss: 2.5786523818969727  | Mean train accuracy: 0.89876389503479\n",
            "Epoch: 5/5  | Epoch loss: 2.579012870788574  | Mean train accuracy: 0.8987712860107422\n",
            "Epoch: 5/5  | Epoch loss: 2.5790419578552246  | Mean train accuracy: 0.8987768888473511\n",
            "Epoch: 5/5  | Epoch loss: 2.5791618824005127  | Mean train accuracy: 0.8987829685211182\n",
            "Epoch: 5/5  | Epoch loss: 2.5789294242858887  | Mean train accuracy: 0.8987898230552673\n",
            "Epoch: 5/5  | Epoch loss: 2.579249382019043  | Mean train accuracy: 0.8987935185432434\n",
            "Epoch: 5/5  | Epoch loss: 2.5792462825775146  | Mean train accuracy: 0.8987998962402344\n",
            "Epoch: 5/5  | Epoch loss: 2.5789897441864014  | Mean train accuracy: 0.8988057374954224\n",
            "Epoch: 5/5  | Epoch loss: 2.578881025314331  | Mean train accuracy: 0.8988102078437805\n",
            "Epoch: 5/5  | Epoch loss: 2.578994035720825  | Mean train accuracy: 0.8988141417503357\n",
            "Epoch: 5/5  | Epoch loss: 2.5785465240478516  | Mean train accuracy: 0.8988171815872192\n",
            "Epoch: 5/5  | Epoch loss: 2.5782957077026367  | Mean train accuracy: 0.8988206386566162\n",
            "Epoch: 5/5  | Epoch loss: 2.5778965950012207  | Mean train accuracy: 0.8988242149353027\n",
            "Epoch: 5/5  | Epoch loss: 2.577709674835205  | Mean train accuracy: 0.8988277316093445\n",
            "Epoch: 5/5  | Epoch loss: 2.577848196029663  | Mean train accuracy: 0.8988302946090698\n",
            "Epoch: 5/5  | Epoch loss: 2.5783488750457764  | Mean train accuracy: 0.8988333940505981\n",
            "Epoch: 5/5  | Epoch loss: 2.578080177307129  | Mean train accuracy: 0.8988389372825623\n",
            "Epoch: 5/5  | Epoch loss: 2.5776915550231934  | Mean train accuracy: 0.898842453956604\n",
            "Epoch: 5/5  | Epoch loss: 2.5781033039093018  | Mean train accuracy: 0.8988449573516846\n",
            "Epoch: 5/5  | Epoch loss: 2.5775492191314697  | Mean train accuracy: 0.898849368095398\n",
            "Epoch: 5/5  | Epoch loss: 2.577343463897705  | Mean train accuracy: 0.8988538980484009\n",
            "Epoch: 5/5  | Epoch loss: 2.577162742614746  | Mean train accuracy: 0.8988577723503113\n",
            "Epoch: 5/5  | Epoch loss: 2.5773825645446777  | Mean train accuracy: 0.8988634347915649\n",
            "Epoch: 5/5  | Epoch loss: 2.577028751373291  | Mean train accuracy: 0.8988698720932007\n",
            "Epoch: 5/5  | Epoch loss: 2.5770046710968018  | Mean train accuracy: 0.8988742232322693\n",
            "Epoch: 5/5  | Epoch loss: 2.577021360397339  | Mean train accuracy: 0.8988800644874573\n",
            "Epoch: 5/5  | Epoch loss: 2.5776376724243164  | Mean train accuracy: 0.8988864421844482\n",
            "Epoch: 5/5  | Epoch loss: 2.5779738426208496  | Mean train accuracy: 0.8988920450210571\n",
            "Epoch: 5/5  | Epoch loss: 2.5775506496429443  | Mean train accuracy: 0.8988974690437317\n",
            "Epoch: 5/5  | Epoch loss: 2.5779480934143066  | Mean train accuracy: 0.8989005088806152\n",
            "Epoch: 5/5  | Epoch loss: 2.5778605937957764  | Mean train accuracy: 0.8989048004150391\n",
            "Epoch: 5/5  | Epoch loss: 2.5779778957366943  | Mean train accuracy: 0.8989097476005554\n",
            "Epoch: 5/5  | Epoch loss: 2.577843427658081  | Mean train accuracy: 0.8989166021347046\n",
            "Epoch: 5/5  | Epoch loss: 2.578145980834961  | Mean train accuracy: 0.8989217877388\n",
            "Epoch: 5/5  | Epoch loss: 2.5782294273376465  | Mean train accuracy: 0.898926317691803\n",
            "Epoch: 5/5  | Epoch loss: 2.578455924987793  | Mean train accuracy: 0.8989302515983582\n",
            "Epoch: 5/5  | Epoch loss: 2.578249216079712  | Mean train accuracy: 0.8989330530166626\n",
            "Epoch: 5/5  | Epoch loss: 2.5784800052642822  | Mean train accuracy: 0.8989349603652954\n",
            "Epoch: 5/5  | Epoch loss: 2.578761100769043  | Mean train accuracy: 0.8989334106445312\n",
            "Epoch: 5/5  | Epoch loss: 2.5794951915740967  | Mean train accuracy: 0.8989322185516357\n",
            "Epoch: 5/5  | Epoch loss: 2.579641819000244  | Mean train accuracy: 0.8989308476448059\n",
            "Epoch: 5/5  | Epoch loss: 2.579291343688965  | Mean train accuracy: 0.8989296555519104\n",
            "Epoch: 5/5  | Epoch loss: 2.5796658992767334  | Mean train accuracy: 0.8989272713661194\n",
            "Epoch: 5/5  | Epoch loss: 2.579425096511841  | Mean train accuracy: 0.8989264369010925\n",
            "Epoch: 5/5  | Epoch loss: 2.5794689655303955  | Mean train accuracy: 0.8989244699478149\n",
            "Epoch: 5/5  | Epoch loss: 2.5798099040985107  | Mean train accuracy: 0.8989217877388\n",
            "Epoch: 5/5  | Epoch loss: 2.5798678398132324  | Mean train accuracy: 0.8989192843437195\n",
            "Epoch: 5/5  | Epoch loss: 2.5798466205596924  | Mean train accuracy: 0.8989162445068359\n",
            "Epoch: 5/5  | Epoch loss: 2.5797111988067627  | Mean train accuracy: 0.8989136815071106\n",
            "Epoch: 5/5  | Epoch loss: 2.579582691192627  | Mean train accuracy: 0.8989104628562927\n",
            "Epoch: 5/5  | Epoch loss: 2.5795412063598633  | Mean train accuracy: 0.8989060521125793\n",
            "Epoch: 5/5  | Epoch loss: 2.5795085430145264  | Mean train accuracy: 0.8989035487174988\n",
            "Epoch: 5/5  | Epoch loss: 2.5796732902526855  | Mean train accuracy: 0.8989014625549316\n",
            "Epoch: 5/5  | Epoch loss: 2.5793449878692627  | Mean train accuracy: 0.8988999724388123\n",
            "Epoch: 5/5  | Epoch loss: 2.5792903900146484  | Mean train accuracy: 0.8988983035087585\n",
            "Epoch: 5/5  | Epoch loss: 2.5791218280792236  | Mean train accuracy: 0.8988980054855347\n",
            "Epoch: 5/5  | Epoch loss: 2.578767776489258  | Mean train accuracy: 0.898897111415863\n",
            "Epoch: 5/5  | Epoch loss: 2.5786495208740234  | Mean train accuracy: 0.8988961577415466\n",
            "Epoch: 5/5  | Epoch loss: 2.578829765319824  | Mean train accuracy: 0.8988960981369019\n",
            "Epoch: 5/5  | Epoch loss: 2.57863187789917  | Mean train accuracy: 0.8988969326019287\n",
            "Epoch: 5/5  | Epoch loss: 2.578723907470703  | Mean train accuracy: 0.8988975882530212\n",
            "Epoch: 5/5  | Epoch loss: 2.5793042182922363  | Mean train accuracy: 0.8988987803459167\n",
            "Epoch: 5/5  | Epoch loss: 2.5794517993927  | Mean train accuracy: 0.8988994359970093\n",
            "Epoch: 5/5  | Epoch loss: 2.5791022777557373  | Mean train accuracy: 0.8989008069038391\n",
            "Epoch: 5/5  | Epoch loss: 2.578984498977661  | Mean train accuracy: 0.8989026546478271\n",
            "Epoch: 5/5  | Epoch loss: 2.5791070461273193  | Mean train accuracy: 0.8989039063453674\n",
            "Epoch: 5/5  | Epoch loss: 2.5789730548858643  | Mean train accuracy: 0.8989062309265137\n",
            "Epoch: 5/5  | Epoch loss: 2.579042673110962  | Mean train accuracy: 0.8989090323448181\n",
            "Epoch: 5/5  | Epoch loss: 2.5793261528015137  | Mean train accuracy: 0.8989121317863464\n",
            "Epoch: 5/5  | Epoch loss: 2.579770088195801  | Mean train accuracy: 0.8989134430885315\n",
            "Epoch: 5/5  | Epoch loss: 2.580085515975952  | Mean train accuracy: 0.8989144563674927\n",
            "Epoch: 5/5  | Epoch loss: 2.579862594604492  | Mean train accuracy: 0.8989157676696777\n",
            "Epoch: 5/5  | Epoch loss: 2.5799546241760254  | Mean train accuracy: 0.898915708065033\n",
            "Epoch: 5/5  | Epoch loss: 2.579974412918091  | Mean train accuracy: 0.8989165425300598\n",
            "Epoch: 5/5  | Epoch loss: 2.5793328285217285  | Mean train accuracy: 0.8989180326461792\n",
            "Epoch: 5/5  | Epoch loss: 2.5794200897216797  | Mean train accuracy: 0.898918867111206\n",
            "Epoch: 5/5  | Epoch loss: 2.5795836448669434  | Mean train accuracy: 0.8989208340644836\n",
            "Epoch: 5/5  | Epoch loss: 2.5796849727630615  | Mean train accuracy: 0.8989236354827881\n",
            "Epoch: 5/5  | Epoch loss: 2.579561710357666  | Mean train accuracy: 0.8989265561103821\n",
            "Epoch: 5/5  | Epoch loss: 2.5789384841918945  | Mean train accuracy: 0.8989291191101074\n",
            "Epoch: 5/5  | Epoch loss: 2.578921318054199  | Mean train accuracy: 0.8989327549934387\n",
            "Epoch: 5/5  | Epoch loss: 2.5787620544433594  | Mean train accuracy: 0.8989351987838745\n",
            "Epoch: 5/5  | Epoch loss: 2.5789380073547363  | Mean train accuracy: 0.8989397287368774\n",
            "Epoch: 5/5  | Epoch loss: 2.5788869857788086  | Mean train accuracy: 0.8989444375038147\n",
            "Epoch: 5/5  | Epoch loss: 2.5789337158203125  | Mean train accuracy: 0.898949146270752\n",
            "Epoch: 5/5  | Epoch loss: 2.5789265632629395  | Mean train accuracy: 0.8989542722702026\n",
            "Epoch: 5/5  | Epoch loss: 2.5789918899536133  | Mean train accuracy: 0.8989601731300354\n",
            "Epoch: 5/5  | Epoch loss: 2.5786662101745605  | Mean train accuracy: 0.8989648222923279\n",
            "Epoch: 5/5  | Epoch loss: 2.578545570373535  | Mean train accuracy: 0.8989689350128174\n",
            "Epoch: 5/5  | Epoch loss: 2.578129768371582  | Mean train accuracy: 0.8989740610122681\n",
            "Epoch: 5/5  | Epoch loss: 2.5785491466522217  | Mean train accuracy: 0.8989805579185486\n",
            "Epoch: 5/5  | Epoch loss: 2.5783846378326416  | Mean train accuracy: 0.8989850282669067\n",
            "Epoch: 5/5  | Epoch loss: 2.5783731937408447  | Mean train accuracy: 0.8989892601966858\n",
            "Epoch: 5/5  | Epoch loss: 2.578662633895874  | Mean train accuracy: 0.8989934325218201\n",
            "Epoch: 5/5  | Epoch loss: 2.578855276107788  | Mean train accuracy: 0.8989948034286499\n",
            "Epoch: 5/5  | Epoch loss: 2.57916259765625  | Mean train accuracy: 0.8989980220794678\n",
            "Epoch: 5/5  | Epoch loss: 2.5789644718170166  | Mean train accuracy: 0.8990005850791931\n",
            "Epoch: 5/5  | Epoch loss: 2.5790019035339355  | Mean train accuracy: 0.8990040421485901\n",
            "Epoch: 5/5  | Epoch loss: 2.579090118408203  | Mean train accuracy: 0.8990054726600647\n",
            "Epoch: 5/5  | Epoch loss: 2.5787220001220703  | Mean train accuracy: 0.8990083336830139\n",
            "Epoch: 5/5  | Epoch loss: 2.5785036087036133  | Mean train accuracy: 0.8990094065666199\n",
            "Epoch: 5/5  | Epoch loss: 2.578232526779175  | Mean train accuracy: 0.899012565612793\n",
            "Epoch: 5/5  | Epoch loss: 2.578554391860962  | Mean train accuracy: 0.899015486240387\n",
            "Epoch: 5/5  | Epoch loss: 2.5783729553222656  | Mean train accuracy: 0.8990183472633362\n",
            "Epoch: 5/5  | Epoch loss: 2.5777883529663086  | Mean train accuracy: 0.899021327495575\n",
            "Epoch: 5/5  | Epoch loss: 2.5772743225097656  | Mean train accuracy: 0.8990253806114197\n",
            "Epoch: 5/5  | Epoch loss: 2.577134132385254  | Mean train accuracy: 0.8990290760993958\n",
            "Epoch: 5/5  | Epoch loss: 2.57730770111084  | Mean train accuracy: 0.8990322947502136\n",
            "Epoch: 5/5  | Epoch loss: 2.577221393585205  | Mean train accuracy: 0.8990365266799927\n",
            "Epoch: 5/5  | Epoch loss: 2.5773000717163086  | Mean train accuracy: 0.8990408182144165\n",
            "Epoch: 5/5  | Epoch loss: 2.577331304550171  | Mean train accuracy: 0.8990449905395508\n",
            "Epoch: 5/5  | Epoch loss: 2.5775980949401855  | Mean train accuracy: 0.8990499973297119\n",
            "Epoch: 5/5  | Epoch loss: 2.5776638984680176  | Mean train accuracy: 0.8990536332130432\n",
            "Epoch: 5/5  | Epoch loss: 2.5781829357147217  | Mean train accuracy: 0.8990585803985596\n",
            "Epoch: 5/5  | Epoch loss: 2.5781750679016113  | Mean train accuracy: 0.8990631699562073\n",
            "Epoch: 5/5  | Epoch loss: 2.5783963203430176  | Mean train accuracy: 0.899067759513855\n",
            "Epoch: 5/5  | Epoch loss: 2.5783143043518066  | Mean train accuracy: 0.8990719318389893\n",
            "Epoch: 5/5  | Epoch loss: 2.5781238079071045  | Mean train accuracy: 0.8990762829780579\n",
            "Epoch: 5/5  | Epoch loss: 2.5780270099639893  | Mean train accuracy: 0.8990814685821533\n",
            "Epoch: 5/5  | Epoch loss: 2.5780677795410156  | Mean train accuracy: 0.8990840315818787\n",
            "Epoch: 5/5  | Epoch loss: 2.577665328979492  | Mean train accuracy: 0.8990879654884338\n",
            "Epoch: 5/5  | Epoch loss: 2.5777018070220947  | Mean train accuracy: 0.8990916013717651\n",
            "Epoch: 5/5  | Epoch loss: 2.5775842666625977  | Mean train accuracy: 0.8990951180458069\n",
            "Epoch: 5/5  | Epoch loss: 2.5778024196624756  | Mean train accuracy: 0.8990981578826904\n",
            "Epoch: 5/5  | Epoch loss: 2.577793836593628  | Mean train accuracy: 0.8991007804870605\n",
            "Epoch: 5/5  | Epoch loss: 2.5774424076080322  | Mean train accuracy: 0.899104654788971\n",
            "Epoch: 5/5  | Epoch loss: 2.5774388313293457  | Mean train accuracy: 0.8991101384162903\n",
            "Epoch: 5/5  | Epoch loss: 2.577669143676758  | Mean train accuracy: 0.899113655090332\n",
            "Epoch: 5/5  | Epoch loss: 2.5776937007904053  | Mean train accuracy: 0.8991177678108215\n",
            "Epoch: 5/5  | Epoch loss: 2.5772545337677  | Mean train accuracy: 0.8991209864616394\n",
            "Epoch: 5/5  | Epoch loss: 2.5768253803253174  | Mean train accuracy: 0.89912348985672\n",
            "Epoch: 5/5  | Epoch loss: 2.576820135116577  | Mean train accuracy: 0.8991261124610901\n",
            "Epoch: 5/5  | Epoch loss: 2.5768625736236572  | Mean train accuracy: 0.8991280198097229\n",
            "Epoch: 5/5  | Epoch loss: 2.577122449874878  | Mean train accuracy: 0.8991299867630005\n",
            "Epoch: 5/5  | Epoch loss: 2.5773532390594482  | Mean train accuracy: 0.8991320729255676\n",
            "Epoch: 5/5  | Epoch loss: 2.577523946762085  | Mean train accuracy: 0.8991334438323975\n",
            "Epoch: 5/5  | Epoch loss: 2.5769357681274414  | Mean train accuracy: 0.8991340398788452\n",
            "Epoch: 5/5  | Epoch loss: 2.5766353607177734  | Mean train accuracy: 0.8991352915763855\n",
            "Epoch: 5/5  | Epoch loss: 2.576197624206543  | Mean train accuracy: 0.8991374373435974\n",
            "Epoch: 5/5  | Epoch loss: 2.575833320617676  | Mean train accuracy: 0.8991382122039795\n",
            "Epoch: 5/5  | Epoch loss: 2.5762274265289307  | Mean train accuracy: 0.8991395831108093\n",
            "Epoch: 5/5  | Epoch loss: 2.5761947631835938  | Mean train accuracy: 0.8991419672966003\n",
            "Epoch: 5/5  | Epoch loss: 2.5763347148895264  | Mean train accuracy: 0.8991427421569824\n",
            "Epoch: 5/5  | Epoch loss: 2.5766732692718506  | Mean train accuracy: 0.8991461992263794\n",
            "Epoch: 5/5  | Epoch loss: 2.5765557289123535  | Mean train accuracy: 0.8991490602493286\n",
            "Epoch: 5/5  | Epoch loss: 2.576604127883911  | Mean train accuracy: 0.8991522789001465\n",
            "Epoch: 5/5  | Epoch loss: 2.5764975547790527  | Mean train accuracy: 0.8991537690162659\n",
            "Epoch: 5/5  | Epoch loss: 2.5766732692718506  | Mean train accuracy: 0.8991567492485046\n",
            "Epoch: 5/5  | Epoch loss: 2.5765514373779297  | Mean train accuracy: 0.8991590142250061\n",
            "Epoch: 5/5  | Epoch loss: 2.576613426208496  | Mean train accuracy: 0.8991642594337463\n",
            "Epoch: 5/5  | Epoch loss: 2.576747179031372  | Mean train accuracy: 0.8991690874099731\n",
            "Epoch: 5/5  | Epoch loss: 2.576849937438965  | Mean train accuracy: 0.8991748094558716\n",
            "Epoch: 5/5  | Epoch loss: 2.576724052429199  | Mean train accuracy: 0.8991826772689819\n",
            "Epoch: 5/5  | Epoch loss: 2.5765340328216553  | Mean train accuracy: 0.8991907835006714\n",
            "Epoch: 5/5  | Epoch loss: 2.576443672180176  | Mean train accuracy: 0.8991994857788086\n",
            "Epoch: 5/5  | Epoch loss: 2.5762298107147217  | Mean train accuracy: 0.8992084264755249\n",
            "Epoch: 5/5  | Epoch loss: 2.5757415294647217  | Mean train accuracy: 0.8992167711257935\n",
            "Epoch: 5/5  | Epoch loss: 2.5756213665008545  | Mean train accuracy: 0.8992266058921814\n",
            "Epoch: 5/5  | Epoch loss: 2.5755667686462402  | Mean train accuracy: 0.8992360830307007\n",
            "Epoch: 5/5  | Epoch loss: 2.5759708881378174  | Mean train accuracy: 0.8992440700531006\n",
            "Epoch: 5/5  | Epoch loss: 2.5759663581848145  | Mean train accuracy: 0.8992520570755005\n",
            "Epoch: 5/5  | Epoch loss: 2.5761547088623047  | Mean train accuracy: 0.8992606997489929\n",
            "Epoch: 5/5  | Epoch loss: 2.576122760772705  | Mean train accuracy: 0.8992689847946167\n",
            "Epoch: 5/5  | Epoch loss: 2.576035737991333  | Mean train accuracy: 0.899275541305542\n",
            "Epoch: 5/5  | Epoch loss: 2.5762834548950195  | Mean train accuracy: 0.8992828726768494\n",
            "Epoch: 5/5  | Epoch loss: 2.576076030731201  | Mean train accuracy: 0.8992915153503418\n",
            "Epoch: 5/5  | Epoch loss: 2.5762505531311035  | Mean train accuracy: 0.8992984294891357\n",
            "Epoch: 5/5  | Epoch loss: 2.5763185024261475  | Mean train accuracy: 0.8993049263954163\n",
            "Epoch: 5/5  | Epoch loss: 2.576213836669922  | Mean train accuracy: 0.8993114233016968\n",
            "Epoch: 5/5  | Epoch loss: 2.5764753818511963  | Mean train accuracy: 0.8993192315101624\n",
            "Epoch: 5/5  | Epoch loss: 2.5765249729156494  | Mean train accuracy: 0.8993263244628906\n",
            "Epoch: 5/5  | Epoch loss: 2.5762672424316406  | Mean train accuracy: 0.8993321061134338\n",
            "Epoch: 5/5  | Epoch loss: 2.576310157775879  | Mean train accuracy: 0.8993383049964905\n",
            "Epoch: 5/5  | Epoch loss: 2.576300621032715  | Mean train accuracy: 0.8993430137634277\n",
            "Epoch: 5/5  | Epoch loss: 2.576457977294922  | Mean train accuracy: 0.8993483185768127\n",
            "Epoch: 5/5  | Epoch loss: 2.576249599456787  | Mean train accuracy: 0.8993520140647888\n",
            "Epoch: 5/5  | Epoch loss: 2.5767464637756348  | Mean train accuracy: 0.8993563652038574\n",
            "Epoch: 5/5  | Epoch loss: 2.5766992568969727  | Mean train accuracy: 0.8993614315986633\n",
            "Epoch: 5/5  | Epoch loss: 2.5767717361450195  | Mean train accuracy: 0.8993662595748901\n",
            "Epoch: 5/5  | Epoch loss: 2.5766687393188477  | Mean train accuracy: 0.8993708491325378\n",
            "Epoch: 5/5  | Epoch loss: 2.577005624771118  | Mean train accuracy: 0.8993753790855408\n",
            "Epoch: 5/5  | Epoch loss: 2.576725721359253  | Mean train accuracy: 0.8993808627128601\n",
            "Epoch: 5/5  | Epoch loss: 2.577014446258545  | Mean train accuracy: 0.8993845582008362\n",
            "Epoch: 5/5  | Epoch loss: 2.5770461559295654  | Mean train accuracy: 0.8993898630142212\n",
            "Epoch: 5/5  | Epoch loss: 2.5770416259765625  | Mean train accuracy: 0.8993957042694092\n",
            "Epoch: 5/5  | Epoch loss: 2.577193260192871  | Mean train accuracy: 0.8994015455245972\n",
            "Epoch: 5/5  | Epoch loss: 2.5773820877075195  | Mean train accuracy: 0.8994067311286926\n",
            "Epoch: 5/5  | Epoch loss: 2.5774314403533936  | Mean train accuracy: 0.8994125127792358\n",
            "Epoch: 5/5  | Epoch loss: 2.577253580093384  | Mean train accuracy: 0.899419367313385\n",
            "Epoch: 5/5  | Epoch loss: 2.577481985092163  | Mean train accuracy: 0.8994255661964417\n",
            "Epoch: 5/5  | Epoch loss: 2.5774734020233154  | Mean train accuracy: 0.8994325995445251\n",
            "Epoch: 5/5  | Epoch loss: 2.5775318145751953  | Mean train accuracy: 0.8994389772415161\n",
            "Epoch: 5/5  | Epoch loss: 2.5775139331817627  | Mean train accuracy: 0.8994464874267578\n",
            "Epoch: 5/5  | Epoch loss: 2.5773704051971436  | Mean train accuracy: 0.8994539380073547\n",
            "Epoch: 5/5  | Epoch loss: 2.577033281326294  | Mean train accuracy: 0.8994607925415039\n",
            "Epoch: 5/5  | Epoch loss: 2.577160596847534  | Mean train accuracy: 0.8994677662849426\n",
            "Epoch: 5/5  | Epoch loss: 2.5769972801208496  | Mean train accuracy: 0.8994749784469604\n",
            "Epoch: 5/5  | Epoch loss: 2.5769755840301514  | Mean train accuracy: 0.8994835615158081\n",
            "Epoch: 5/5  | Epoch loss: 2.5769424438476562  | Mean train accuracy: 0.8994902968406677\n",
            "Epoch: 5/5  | Epoch loss: 2.5771851539611816  | Mean train accuracy: 0.8994967937469482\n",
            "Epoch: 5/5  | Epoch loss: 2.5773346424102783  | Mean train accuracy: 0.8995037078857422\n",
            "Epoch: 5/5  | Epoch loss: 2.577375650405884  | Mean train accuracy: 0.8995105028152466\n",
            "Epoch: 5/5  | Epoch loss: 2.5773701667785645  | Mean train accuracy: 0.8995155692100525\n",
            "Epoch: 5/5  | Epoch loss: 2.5773744583129883  | Mean train accuracy: 0.899520754814148\n",
            "Epoch: 5/5  | Epoch loss: 2.5772252082824707  | Mean train accuracy: 0.8995262980461121\n",
            "Epoch: 5/5  | Epoch loss: 2.576813220977783  | Mean train accuracy: 0.899531364440918\n",
            "Epoch: 5/5  | Epoch loss: 2.5767338275909424  | Mean train accuracy: 0.8995367288589478\n",
            "Epoch: 5/5  | Epoch loss: 2.5767998695373535  | Mean train accuracy: 0.8995413184165955\n",
            "Epoch: 5/5  | Epoch loss: 2.5766537189483643  | Mean train accuracy: 0.8995462656021118\n",
            "Epoch: 5/5  | Epoch loss: 2.5765621662139893  | Mean train accuracy: 0.8995503187179565\n",
            "Epoch: 5/5  | Epoch loss: 2.5763959884643555  | Mean train accuracy: 0.8995552659034729\n",
            "Epoch: 5/5  | Epoch loss: 2.576552152633667  | Mean train accuracy: 0.8995599150657654\n",
            "Epoch: 5/5  | Epoch loss: 2.5764689445495605  | Mean train accuracy: 0.8995645046234131\n",
            "Epoch: 5/5  | Epoch loss: 2.5768301486968994  | Mean train accuracy: 0.8995692133903503\n",
            "Epoch: 5/5  | Epoch loss: 2.576521635055542  | Mean train accuracy: 0.8995730876922607\n",
            "Epoch: 5/5  | Epoch loss: 2.5763320922851562  | Mean train accuracy: 0.8995782732963562\n",
            "Epoch: 5/5  | Epoch loss: 2.576235771179199  | Mean train accuracy: 0.8995829224586487\n",
            "Epoch: 5/5  | Epoch loss: 2.5762462615966797  | Mean train accuracy: 0.899587094783783\n",
            "Epoch: 5/5  | Epoch loss: 2.5763776302337646  | Mean train accuracy: 0.8995921015739441\n",
            "Epoch: 5/5  | Epoch loss: 2.576298952102661  | Mean train accuracy: 0.8995976448059082\n",
            "Epoch: 5/5  | Epoch loss: 2.576317071914673  | Mean train accuracy: 0.8996029496192932\n",
            "Epoch: 5/5  | Epoch loss: 2.576301336288452  | Mean train accuracy: 0.8996084928512573\n",
            "Epoch: 5/5  | Epoch loss: 2.5766139030456543  | Mean train accuracy: 0.8996137976646423\n",
            "Epoch: 5/5  | Epoch loss: 2.5766849517822266  | Mean train accuracy: 0.8996193408966064\n",
            "Epoch: 5/5  | Epoch loss: 2.576470136642456  | Mean train accuracy: 0.8996242880821228\n",
            "Epoch: 5/5  | Epoch loss: 2.576352596282959  | Mean train accuracy: 0.8996293544769287\n",
            "Epoch: 5/5  | Epoch loss: 2.576842784881592  | Mean train accuracy: 0.8996334671974182\n",
            "Epoch: 5/5  | Epoch loss: 2.577190399169922  | Mean train accuracy: 0.8996389508247375\n",
            "Epoch: 5/5  | Epoch loss: 2.577491283416748  | Mean train accuracy: 0.8996433019638062\n",
            "Epoch: 5/5  | Epoch loss: 2.577603340148926  | Mean train accuracy: 0.8996483087539673\n",
            "Epoch: 5/5  | Epoch loss: 2.5776638984680176  | Mean train accuracy: 0.8996542692184448\n",
            "Epoch: 5/5  | Epoch loss: 2.5777573585510254  | Mean train accuracy: 0.8996596336364746\n",
            "Epoch: 5/5  | Epoch loss: 2.5778164863586426  | Mean train accuracy: 0.8996652960777283\n",
            "Epoch: 5/5  | Epoch loss: 2.5779306888580322  | Mean train accuracy: 0.8996708989143372\n",
            "Epoch: 5/5  | Epoch loss: 2.5782790184020996  | Mean train accuracy: 0.8996763229370117\n",
            "Epoch: 5/5  | Epoch loss: 2.5785088539123535  | Mean train accuracy: 0.8996821641921997\n",
            "Epoch: 5/5  | Epoch loss: 2.579007387161255  | Mean train accuracy: 0.8996858596801758\n",
            "Epoch: 5/5  | Epoch loss: 2.5790019035339355  | Mean train accuracy: 0.8996900320053101\n",
            "Epoch: 5/5  | Epoch loss: 2.5789239406585693  | Mean train accuracy: 0.8996950387954712\n",
            "Epoch: 5/5  | Epoch loss: 2.5791985988616943  | Mean train accuracy: 0.899699866771698\n",
            "Epoch: 5/5  | Epoch loss: 2.5791051387786865  | Mean train accuracy: 0.8997031450271606\n",
            "Epoch: 5/5  | Epoch loss: 2.578850507736206  | Mean train accuracy: 0.8997061252593994\n",
            "Epoch: 5/5  | Epoch loss: 2.5787241458892822  | Mean train accuracy: 0.899708092212677\n",
            "Epoch: 5/5  | Epoch loss: 2.5789592266082764  | Mean train accuracy: 0.8997095823287964\n",
            "Epoch: 5/5  | Epoch loss: 2.5790209770202637  | Mean train accuracy: 0.8997116684913635\n",
            "Epoch: 5/5  | Epoch loss: 2.578990936279297  | Mean train accuracy: 0.8997137546539307\n",
            "Epoch: 5/5  | Epoch loss: 2.5788934230804443  | Mean train accuracy: 0.8997159004211426\n",
            "Epoch: 5/5  | Epoch loss: 2.578728437423706  | Mean train accuracy: 0.8997176289558411\n",
            "Epoch: 5/5  | Epoch loss: 2.5786454677581787  | Mean train accuracy: 0.8997194766998291\n",
            "Epoch: 5/5  | Epoch loss: 2.5786538124084473  | Mean train accuracy: 0.899721086025238\n",
            "Epoch: 5/5  | Epoch loss: 2.5788843631744385  | Mean train accuracy: 0.8997218012809753\n",
            "Epoch: 5/5  | Epoch loss: 2.578528881072998  | Mean train accuracy: 0.8997223377227783\n",
            "Epoch: 5/5  | Epoch loss: 2.5781919956207275  | Mean train accuracy: 0.8997228741645813\n",
            "Epoch: 5/5  | Epoch loss: 2.5780608654022217  | Mean train accuracy: 0.8997225165367126\n",
            "Epoch: 5/5  | Epoch loss: 2.5781474113464355  | Mean train accuracy: 0.8997220993041992\n",
            "Epoch: 5/5  | Epoch loss: 2.578132390975952  | Mean train accuracy: 0.8997220993041992\n",
            "Epoch: 5/5  | Epoch loss: 2.5778136253356934  | Mean train accuracy: 0.8997212648391724\n",
            "Epoch: 5/5  | Epoch loss: 2.5776376724243164  | Mean train accuracy: 0.8997211456298828\n",
            "Epoch: 5/5  | Epoch loss: 2.577669382095337  | Mean train accuracy: 0.8997220993041992\n",
            "Epoch: 5/5  | Epoch loss: 2.5777032375335693  | Mean train accuracy: 0.8997228145599365\n",
            "Epoch: 5/5  | Epoch loss: 2.577504873275757  | Mean train accuracy: 0.8997238874435425\n",
            "Epoch: 5/5  | Epoch loss: 2.577746868133545  | Mean train accuracy: 0.8997254371643066\n",
            "Epoch: 5/5  | Epoch loss: 2.5780935287475586  | Mean train accuracy: 0.8997269868850708\n",
            "Epoch: 5/5  | Epoch loss: 2.5779104232788086  | Mean train accuracy: 0.8997292518615723\n",
            "Epoch: 5/5  | Epoch loss: 2.5778799057006836  | Mean train accuracy: 0.8997318744659424\n",
            "Epoch: 5/5  | Epoch loss: 2.5776798725128174  | Mean train accuracy: 0.8997334241867065\n",
            "Epoch: 5/5  | Epoch loss: 2.57751727104187  | Mean train accuracy: 0.8997354507446289\n",
            "Epoch: 5/5  | Epoch loss: 2.577615261077881  | Mean train accuracy: 0.8997388482093811\n",
            "Epoch: 5/5  | Epoch loss: 2.577500820159912  | Mean train accuracy: 0.8997420072555542\n",
            "Epoch: 5/5  | Epoch loss: 2.5773091316223145  | Mean train accuracy: 0.8997452855110168\n",
            "Epoch: 5/5  | Epoch loss: 2.5772435665130615  | Mean train accuracy: 0.8997495174407959\n",
            "Epoch: 5/5  | Epoch loss: 2.577462673187256  | Mean train accuracy: 0.8997536897659302\n",
            "Epoch: 5/5  | Epoch loss: 2.5773839950561523  | Mean train accuracy: 0.8997588753700256\n",
            "Epoch: 5/5  | Epoch loss: 2.5772995948791504  | Mean train accuracy: 0.8997641205787659\n",
            "Epoch: 5/5  | Epoch loss: 2.5772571563720703  | Mean train accuracy: 0.8997697830200195\n",
            "Epoch: 5/5  | Epoch loss: 2.5774905681610107  | Mean train accuracy: 0.8997761607170105\n",
            "Epoch: 5/5  | Epoch loss: 2.577258825302124  | Mean train accuracy: 0.8997822999954224\n",
            "Epoch: 5/5  | Epoch loss: 2.5773744583129883  | Mean train accuracy: 0.899787425994873\n",
            "Epoch: 5/5  | Epoch loss: 2.5770812034606934  | Mean train accuracy: 0.8997941017150879\n",
            "Epoch: 5/5  | Epoch loss: 2.57682204246521  | Mean train accuracy: 0.8998002409934998\n",
            "Epoch: 5/5  | Epoch loss: 2.577125072479248  | Mean train accuracy: 0.8998058438301086\n",
            "Epoch: 5/5  | Epoch loss: 2.577096939086914  | Mean train accuracy: 0.8998125195503235\n",
            "Epoch: 5/5  | Epoch loss: 2.5770254135131836  | Mean train accuracy: 0.8998188376426697\n",
            "Epoch: 5/5  | Epoch loss: 2.5770535469055176  | Mean train accuracy: 0.8998249173164368\n",
            "Epoch: 5/5  | Epoch loss: 2.576937437057495  | Mean train accuracy: 0.89983069896698\n",
            "Epoch: 5/5  | Epoch loss: 2.5769941806793213  | Mean train accuracy: 0.8998363018035889\n",
            "Epoch: 5/5  | Epoch loss: 2.5771148204803467  | Mean train accuracy: 0.8998422026634216\n",
            "Epoch: 5/5  | Epoch loss: 2.577078104019165  | Mean train accuracy: 0.8998473286628723\n",
            "Epoch: 5/5  | Epoch loss: 2.5773284435272217  | Mean train accuracy: 0.8998510241508484\n",
            "Epoch: 5/5  | Epoch loss: 2.5772228240966797  | Mean train accuracy: 0.8998563289642334\n",
            "Epoch: 5/5  | Epoch loss: 2.577633857727051  | Mean train accuracy: 0.8998597860336304\n",
            "Epoch: 5/5  | Epoch loss: 2.577420473098755  | Mean train accuracy: 0.8998642563819885\n",
            "Epoch: 5/5  | Epoch loss: 2.5772953033447266  | Mean train accuracy: 0.8998671174049377\n",
            "Epoch: 5/5  | Epoch loss: 2.577390193939209  | Mean train accuracy: 0.8998700380325317\n",
            "Epoch: 5/5  | Epoch loss: 2.577486753463745  | Mean train accuracy: 0.8998727798461914\n",
            "Epoch: 5/5  | Epoch loss: 2.5775249004364014  | Mean train accuracy: 0.8998755812644958\n",
            "Epoch: 5/5  | Epoch loss: 2.5774872303009033  | Mean train accuracy: 0.8998770117759705\n",
            "Epoch: 5/5  | Epoch loss: 2.5777833461761475  | Mean train accuracy: 0.8998790383338928\n",
            "Epoch: 5/5  | Epoch loss: 2.577895164489746  | Mean train accuracy: 0.8998809456825256\n",
            "Epoch: 5/5  | Epoch loss: 2.5779836177825928  | Mean train accuracy: 0.8998813033103943\n",
            "Epoch: 5/5  | Epoch loss: 2.577953577041626  | Mean train accuracy: 0.8998820781707764\n",
            "Epoch: 5/5  | Epoch loss: 2.578199863433838  | Mean train accuracy: 0.899882435798645\n",
            "Epoch: 5/5  | Epoch loss: 2.5783209800720215  | Mean train accuracy: 0.899883508682251\n",
            "Epoch: 5/5  | Epoch loss: 2.578327178955078  | Mean train accuracy: 0.8998843431472778\n",
            "Epoch: 5/5  | Epoch loss: 2.578057050704956  | Mean train accuracy: 0.8998854756355286\n",
            "Epoch: 5/5  | Epoch loss: 2.57828950881958  | Mean train accuracy: 0.8998865485191345\n",
            "Epoch: 5/5  | Epoch loss: 2.5779953002929688  | Mean train accuracy: 0.8998897075653076\n",
            "Epoch: 5/5  | Epoch loss: 2.5782124996185303  | Mean train accuracy: 0.8998926281929016\n",
            "Epoch: 5/5  | Epoch loss: 2.578045129776001  | Mean train accuracy: 0.8998967409133911\n",
            "Epoch: 5/5  | Epoch loss: 2.5781970024108887  | Mean train accuracy: 0.8998998999595642\n",
            "Epoch: 5/5  | Epoch loss: 2.57820463180542  | Mean train accuracy: 0.8999032378196716\n",
            "Epoch: 5/5  | Epoch loss: 2.578181028366089  | Mean train accuracy: 0.899906575679779\n",
            "Epoch: 5/5  | Epoch loss: 2.578113317489624  | Mean train accuracy: 0.8999104499816895\n",
            "Epoch: 5/5  | Epoch loss: 2.578155040740967  | Mean train accuracy: 0.8999137282371521\n",
            "Epoch: 5/5  | Epoch loss: 2.5782630443573  | Mean train accuracy: 0.8999173641204834\n",
            "Epoch: 5/5  | Epoch loss: 2.5783746242523193  | Mean train accuracy: 0.8999207615852356\n",
            "Epoch: 5/5  | Epoch loss: 2.5781280994415283  | Mean train accuracy: 0.8999255299568176\n",
            "Epoch: 5/5  | Epoch loss: 2.5782008171081543  | Mean train accuracy: 0.8999300599098206\n",
            "Epoch: 5/5  | Epoch loss: 2.5782835483551025  | Mean train accuracy: 0.8999344110488892\n",
            "Epoch: 5/5  | Epoch loss: 2.578342914581299  | Mean train accuracy: 0.8999379873275757\n",
            "Epoch: 5/5  | Epoch loss: 2.5785562992095947  | Mean train accuracy: 0.8999415636062622\n",
            "Epoch: 5/5  | Epoch loss: 2.578552484512329  | Mean train accuracy: 0.8999443650245667\n",
            "Epoch: 5/5  | Epoch loss: 2.578338384628296  | Mean train accuracy: 0.8999477028846741\n",
            "Epoch: 5/5  | Epoch loss: 2.578650951385498  | Mean train accuracy: 0.8999499082565308\n",
            "Epoch: 5/5  | Epoch loss: 2.578798294067383  | Mean train accuracy: 0.8999532461166382\n",
            "Epoch: 5/5  | Epoch loss: 2.5784525871276855  | Mean train accuracy: 0.8999569416046143\n",
            "Epoch: 5/5  | Epoch loss: 2.5785508155822754  | Mean train accuracy: 0.8999589681625366\n",
            "Epoch: 5/5  | Epoch loss: 2.5786471366882324  | Mean train accuracy: 0.8999617099761963\n",
            "Epoch: 5/5  | Epoch loss: 2.5787582397460938  | Mean train accuracy: 0.8999646306037903\n",
            "Epoch: 5/5  | Epoch loss: 2.579151153564453  | Mean train accuracy: 0.8999663591384888\n",
            "Epoch: 5/5  | Epoch loss: 2.5792548656463623  | Mean train accuracy: 0.8999679684638977\n",
            "Epoch: 5/5  | Epoch loss: 2.5793044567108154  | Mean train accuracy: 0.899970293045044\n",
            "Epoch: 5/5  | Epoch loss: 2.5793375968933105  | Mean train accuracy: 0.8999717235565186\n",
            "Epoch: 5/5  | Epoch loss: 2.5793557167053223  | Mean train accuracy: 0.89997398853302\n",
            "Epoch: 5/5  | Epoch loss: 2.579457998275757  | Mean train accuracy: 0.899975597858429\n",
            "\n",
            "Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kkYfhabhfkV1",
        "colab_type": "code",
        "outputId": "9e52530d-a753-46b6-ebf1-46296a03645e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_batch.shape)\n",
        "print(len(X_train))\n",
        "#x_tr=X_train[:,-1,:]\n",
        "print(len(X_train) % batch_size)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 250)\n",
            "167704\n",
            "24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OiX8GC46z6US",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "test_accuracy = []\n",
        "\n",
        "ii = 0\n",
        "while ii + batch_size <= len(X_test):\n",
        "    X_batch = test_sentences_X[ii:ii+batch_size]\n",
        "    y_batch = y_test[ii:ii+batch_size]\n",
        "\n",
        "    a = session.run([model.accuracy], feed_dict={model.inputs:X_batch, \n",
        "                                                 model.targets:y_batch, \n",
        "                                                 model.keep_probs:1.0})\n",
        "    \n",
        "    test_accuracy.append(a)\n",
        "    ii += batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ACULYp9M6yHf",
        "colab_type": "code",
        "outputId": "63cee76b-9fbe-41ff-9539-54199bd6cd81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Test accuracy: {}\".format(np.mean(test_accuracy)))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8974605798721313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oHZ3V4rJ7BkE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKMmrfuisKGJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compared to the baseline using BiLSTM for utterance classification, the second method effectively leverage context information and achieve better performance. Report your overall accuracy. Did context help disambiguate and better predict the minority classes ('br' and 'bf')? What are frequent errors? Show one positive example where adding context changed the prediction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gUZt48JgrE34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Advanced: Creating End-To-End Dialogue System"
      ]
    },
    {
      "metadata": {
        "id": "zE63Q5guuPdA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the last section we want to create end-to-end dialogue system, following on from the seq2seq MT labs you've \n",
        "just done. This is an advanced part of the assignment and worth 10 marks (20%) in total. In end-to-end dialogue systems, the encoder represents each utterance with a vector. The utterance vector is the hidden state after the last token of the utterance has been processed. The context LSTM keeps track of past utterances. The hidden state can be explained as the state of the dialogue system. The next utterance prediction is performed by a decoder LSTM, which takes the hidden state of the last LSTM and produces a probability distribution over the tokens in the next utterance. You can take the DA LSTM state of last section as input to a decoder which tries to generate the next utterance. You can add attention and monitor the performance. Instead of evaluating by an automatic evaluation method, you can show us some of the interesting predictions. \n"
      ]
    },
    {
      "metadata": {
        "id": "TSDY0nzuO1jm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "55a268e2-c338-4996-d284-79f56dc298c6"
      },
      "cell_type": "code",
      "source": [
        "# Try to use Serban et al 2015 (broadly) as a basis \n",
        "# https://arxiv.org/pdf/1507.04808.pdf\n",
        "\n",
        "# Need to set up Seq2Swq LSTM Model\n",
        "# As in Lab 8 Machine Translation, this will be set up\n",
        "# as an encoder and decoder within a class - attempting to follow that\n",
        "# input - output model\n",
        "# Essentially the structure is that of Machine Translation, but across \n",
        "# adjacency pairs using Switchboard data\n",
        "#\n",
        "# Will need to be able to pass in switchboard vector as input to decoder\n",
        "\n",
        "class DiaModel():   #Add params\n",
        "  \n",
        "  #Need Initialiser\n",
        "  def __init__(self, source_vocab_size, target_vocab_size, buckets, size,\n",
        "               num_layers, max_gradient_norm, batch_size, learning_rate,\n",
        "               learning_rate_decay_factor, use_lstm=False,\n",
        "               num_samples=512, forward_only=False):\n",
        "    \n",
        "    \n",
        "    self.source_vocab_size = source_vocab_size\n",
        "    self.target_vocab_size = target_vocab_size\n",
        "    self.buckets = buckets     # See article on seq2seq dialogue systems\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
        "    self.learning_rate_decay_op = self.learning_rate.assign(\n",
        "        self.learning_rate * learning_rate_decay_factor)\n",
        "    self.global_step = tf.Variable(0, trainable=False)\n",
        "    self.hidden_size = 128          # Match size coming from Switchboard data\n",
        "                                    # to pass to LSTM layer\n",
        "    \n",
        "  \n",
        "  def encoder(self,embeddings, sent_lens, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"encoder\"):\n",
        "\n",
        "      # Create an LSTM as the basis of the first layer of the encoder\n",
        "      cell1 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      \n",
        "      # Add Dropout (implemented as a Wrapper around the LSTM Cell in Tensorflow)\n",
        "      # Dropout is basically a regularisation technique to prevent overfitting\n",
        "      drop = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = hidden_keep_prob)\n",
        "      \n",
        "      # Add the second layer, which has the same structure as the first layer above\n",
        "      cell2 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = hidden_keep_prob)\n",
        "      \n",
        "      # Use MultiRNNCell to stack the two cells above so that they behave as a \n",
        "      # single 2-layer cell called 'encoder_cell'\n",
        "      encoder_cell = tf.nn.rnn_cell.MultiRNNCell([drop, drop2])\n",
        "      \n",
        "      # Use dynamic_rnn to take the encoder_cell (above) and the embeddings as \n",
        "      # input and produce the tuple of the encoder output and final state\n",
        "      encoder_outputs, encoder_final_state = tf.nn.dynamic_rnn(\n",
        "      encoder_cell, embeddings,\n",
        "      dtype=tf.float32 \n",
        "       )\n",
        "      \n",
        "    # The encoder function returns the encoder output and final state\n",
        "    return encoder_outputs, encoder_final_state\n",
        "   \n",
        "\n",
        "    \n",
        "    \n",
        "  def decoder(self,step_embeddings,encoder_outputs, pre_states, hidden_keep_prob=1.0):\n",
        "    with tf.variable_scope(\"decoder\",reuse=tf.AUTO_REUSE):\n",
        "\n",
        "      # Create an LSTM as the basis of the first layer of the decoder\n",
        "      cell1 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      \n",
        "      # Add Dropout (implemented as a Wrapper around the LSTM Cell in Tensorflow)\n",
        "      # Dropout is basically a regularisation technique to prevent overfitting\n",
        "      drop = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = hidden_keep_prob, dtype=tf.float32)\n",
        "      \n",
        "      # Add the second layer, which has the same structure as the first layer above\n",
        "      cell2 = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = hidden_keep_prob, dtype=tf.float32)\n",
        "      \n",
        "      # Use MultiRNNCell to stack the two cells above so that they behave as a \n",
        "      # single 2-layer cell called 'decoder_cell'            \n",
        "      decoder_cell = tf.nn.rnn_cell.MultiRNNCell([drop, drop2])\n",
        "\n",
        "      # Directly run the decoder_cell using the step embeddings and pre_states, which\n",
        "      # are both passed in to the decoder function      \n",
        "      step_decoder_output, curr_states = decoder_cell(step_embeddings,pre_states)\n",
        "   \n",
        "      \n",
        "      # Add attention to improve results:\n",
        "      if not self.use_attention:\n",
        "          w = tf.get_variable(\"weights\", shape=[shape(step_decoder_output,1),self.vocab_target_size],\n",
        "                    initializer=tf.glorot_uniform_initializer())\n",
        "          b = tf.get_variable(\"bias\", shape= self.vocab_target_size,\n",
        "                    initializer=tf.constant_initializer(0.1))\n",
        "          logits = tf.nn.xw_plus_b(step_decoder_output,w,b)\n",
        "      else:\n",
        "          # The part of the code which will actually run:\n",
        "          \n",
        "          # Encoder_outputs is a tensor with shape [batch_size, max_step, emb].\n",
        "          # Step_decoder_output is a matrix with shape [batch_size, emb]\n",
        "          # Use tf.expand_dims on step_decoder to enable the use of tf.matmul\n",
        "          # for matrix multiplication\n",
        "          \n",
        "          reshape_step = tf.expand_dims(step_decoder_output,2)\n",
        "          \n",
        "          # Order important: encoder_outputs first to give raw score required\n",
        "          # shape of [batch_size, max_step, 1]\n",
        "          \n",
        "          raw_score = tf.matmul(encoder_outputs,reshape_step)\n",
        "          \n",
        "          #Add Softmax to give multiple probability scores\n",
        "          soft = tf.nn.softmax(raw_score, axis= 1)  \n",
        "          \n",
        "          # Weighted score for encoder_outputs: multiply by softmax scores\n",
        "          encoder_vector = tf.reduce_sum(soft * encoder_outputs,1)\n",
        "          \n",
        "          # As per lecture slides on attention: concatenate the step_encoder_output\n",
        "          # and the encoder_vector\n",
        "          concat = tf.concat([step_decoder_output,encoder_vector], axis=1)\n",
        "            \n",
        "          # Final Layer: \n",
        "          # Feed Forward Network to output the logits from the step decoder output\n",
        "          w = tf.get_variable(\"weights\", shape=[shape(concat,1),self.vocab_target_size],\n",
        "                        initializer=tf.glorot_uniform_initializer())\n",
        "          b = tf.get_variable(\"bias\", shape=self.vocab_target_size,\n",
        "                        initializer=tf.constant_initializer(0.1))\n",
        "          logits = tf.nn.xw_plus_b(concat,w,b)\n",
        "          \n",
        "\n",
        "      return logits, curr_states\n",
        "\n",
        "  \n",
        "  #Input and output a bit mixed below - maybe rename functions\n",
        "  def input_feed(batch_size, encoder_size, encoder_inputs, decoder_size, \n",
        "                            decoder_inputs, target_weights):\n",
        "    \n",
        "    input_feed = {}\n",
        "\n",
        "    for l in xrange(encoder_size):\n",
        "      input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
        "\n",
        "    for l in xrange(decoder_size):\n",
        "      input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
        "      input_feed[self.target_weights[l].name] = target_weights[l]\n",
        "    \n",
        "    # Targets are decoder inputs shifted by one.\n",
        "    last_target = decoder_inputs[decoder_size].name\n",
        "    input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
        "    return input_feed\n",
        "\n",
        "  def output_feed(decoder_size):\n",
        "    # Output feed: depends on whether we do a backward step or not.\n",
        "\n",
        "    if not forward_only:\n",
        "      output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
        "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
        "                     self.losses[bucket_id]]  # Loss for this batch.\n",
        "    else:\n",
        "      output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
        "      for l in xrange(decoder_size):  # Output logits.\n",
        "        output_feed.append(self.outputs[bucket_id][l])\n",
        "    return output_feed\n",
        "    \n",
        "    \n",
        "    \n",
        "'''\n",
        "# Tensorflow version - not necessary - can borrow code from lab 8 and can \n",
        "# process ok\n",
        "\n",
        "    def encoder():      #Add params\n",
        "      keep_prob = 0.5      # Vary\n",
        "      \n",
        "      cell1 = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
        "      # Add dropout: prevent overfitting\n",
        "      drop1 = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = keep_prob)\n",
        "      \n",
        "      cell2 = tf.nn.rnn_cell.BasicLSTMCell(size)\n",
        "      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = keep_prob)\n",
        "      \n",
        "      # Make bidirectional\n",
        "      bilstm = outputs, states = tf.nn.bidirectional_dynamic_rnn(drop1, \n",
        "                                                              drop2, \n",
        "                                                              output,\n",
        "                                                              dtype=tf.float32)\n",
        "      bi_lstm_output = tf.concat(outputs,2)\n",
        "      \n",
        "      return bi_lstm_output\n",
        "'''"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Tensorflow version - not necessary - can borrow code from lab 8 and can \\n# process ok\\n\\n    def encoder():      #Add params\\n      keep_prob = 0.5      # Vary\\n      \\n      cell1 = tf.nn.rnn_cell.BasicLSTMCell(size)\\n      # Add dropout: prevent overfitting\\n      drop1 = tf.nn.rnn_cell.DropoutWrapper(cell1, state_keep_prob = keep_prob)\\n      \\n      cell2 = tf.nn.rnn_cell.BasicLSTMCell(size)\\n      drop2 = tf.nn.rnn_cell.DropoutWrapper(cell2, state_keep_prob = keep_prob)\\n      \\n      # Make bidirectional\\n      bilstm = outputs, states = tf.nn.bidirectional_dynamic_rnn(drop1, \\n                                                              drop2, \\n                                                              output,\\n                                                              dtype=tf.float32)\\n      bi_lstm_output = tf.concat(outputs,2)\\n      \\n      return bi_lstm_output\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}